{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Trace:\n",
    "\n",
    "    def __init__(self, filename: str, benchmarks=None, ignore=None, parse_progress=True):\n",
    "        self.baseline = set(benchmarks) if benchmarks else set()\n",
    "        self.scores, self.expressions, generations_by_task, self.baseline, self.progdf = parse_log(filename, baseline=self.baseline,ignore=ignore,parse_progress=parse_progress)\n",
    "        self.comparison, self.d_scores = comparisons(self.scores)\n",
    "        self.in_comparison, self.in_d_scores = comparisons(self.scores, sample=\"in-sample\")\n",
    "        self.generations_by_task = pd.Series(generations_by_task, name=\"generations\")\n",
    "\n",
    "    @property\n",
    "    def most_frequent_solutions_by_length(self):\n",
    "        for length, expressions in sorted(self.expressions.items()):\n",
    "            m = max(set(expressions), key=expressions.count)\n",
    "            yield m, expressions.count(m)\n",
    "            # print(f\" Found {len(expressions):3d} expressions of length {length}. Most frequent: {m} ({expressions.count(m)} times)\")\n",
    "\n",
    "\n",
    "def parse_log(file, with_prefix=False, baseline=None, ignore=None, parse_progress=True):\n",
    "    with open(file) as fh:\n",
    "        lines = fh.readlines()\n",
    "        \n",
    "    # ignore lines with an expression that is asked to be ignored\n",
    "    lines = [line for line in lines if not any([i in line for i in ignore])]\n",
    "\n",
    "    p = 'INFO:root:' if with_prefix else ''\n",
    "\n",
    "    definitions = [line for line in lines if ':=' in line]\n",
    "    baseline = baseline if baseline else set()\n",
    "    \n",
    "    print(\"The predefined defaults are (may show a repeat):\")\n",
    "    for line in definitions:\n",
    "        if ':=' in line and [i in line for i in ignore]:\n",
    "            print(f\" * {line[len(p):-1]}\")\n",
    "            baseline.add(line[len(p):].split(' :=')[0])\n",
    "\n",
    "    task_starts = [i for i, line in enumerate(lines) if \"START_TASK:\" in line]\n",
    "    in_sample_starts = [i for i, line in enumerate(lines) if \"Evaluating in sample:\" in line]\n",
    "    out_sample_starts = [i for i, line in enumerate(lines) if \"Evaluating out-of-sample:\" in line]\n",
    "\n",
    "    def parse_evaluation_line(line) -> Tuple[str, int, float]:\n",
    "        \"\"\" Parse an evaluation line, returning the expression or name, its 'length' and the score.\n",
    "\n",
    "        e.g. INFO:root:[make_tuple(p, mkd)|0.8893]\\n -> 'make_tuple(p, mkd)', 1, 0.8893\n",
    "        Length is 0 for benchmark problems.\n",
    "        \"\"\"\n",
    "        if line.count('|') == 1:\n",
    "            start, pipe, end = line.find('['), line.find('|'), line.find(']')\n",
    "            expression = line[start+1: pipe]\n",
    "        else:\n",
    "            start, end = line.find('|') + 1, line.find(']')\n",
    "            pipe = line.find('|', start)  \n",
    "            expression = line[start: pipe]          \n",
    "            \n",
    "        if ':' in expression:  # For the baseline expressions, record them by name\n",
    "            expression = expression[:expression.find(':')]\n",
    "        expression_length = expression.count('(')\n",
    "        return expression, expression_length, float(line[pipe + 1: end])\n",
    "    \n",
    "    def parse_progess_lines(task, lines) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Parse the log files progress over generations \n",
    "        Lines that evaluate progress start with 'GEN_' followed by Fitness and Size values\n",
    "        \"\"\"\n",
    "        progdf = pd.DataFrame(columns = ['task','gen', 'min', 'avg', 'max'])\n",
    "        for i, line in enumerate(lines):\n",
    "            progdf.loc[len(progdf)] =  [task, i] + [float(x) for x in line[line.find(\"FIT_\")+4:line.find(\"_SIZE\")].split(\"_\")]\n",
    "        return progdf\n",
    "\n",
    "\n",
    "    tasks = [int(line[:-1].split(\": \")[-1]) for line in lines if \"START_TASK:\" in line]\n",
    "    idx = pd.MultiIndex.from_product([tasks, [\"in-sample\", \"out-sample\"]], names=['task', 'sample-type'])\n",
    "    df = pd.DataFrame(index=idx, columns=[\"length-1\", \"length-2\", \"length-3\", \"final\", *baseline], dtype=float)\n",
    "\n",
    "    expressions_by_length = defaultdict(list)\n",
    "    generations_by_task = {}\n",
    "    progdf = pd.DataFrame(columns = ['task','gen', 'min', 'avg', 'max'])\n",
    "\n",
    "    for task_start, next_task in zip(task_starts, task_starts[1:] + [-(len(baseline)*2 + 1)]):\n",
    "        # start line looks like: INFO:root:START_TASK: 29\\n\n",
    "        task = int(lines[task_start][:-1].split(\": \")[-1])\n",
    "        generations_by_task[task] = 0\n",
    "        gen_lines = []\n",
    "        \n",
    "        # Following the \"INFO:root:Evaluating in sample:\" message, symbolic default performance are printed\n",
    "        # They are formatted as \"INFO:root:[make_tuple(p, mkd)|0.8893]\"\n",
    "        # First is any number of best solutions from the pareto front. The last four are benchmark solutions.\n",
    "        # It is possible that two equally good solutions are printed (i.e. same length and performance).\n",
    "        expr_in_task = set()\n",
    "        max_length = 0\n",
    "        last_insample = []\n",
    "        last_outsample = []\n",
    "        \n",
    "        in_sample = True\n",
    "        for line in lines[task_start:next_task]:\n",
    "            if line.startswith('GEN_'):\n",
    "                generations_by_task[task] += 1\n",
    "                gen_lines.append(line)\n",
    "            if \"Evaluating in sample:\" in line:\n",
    "                in_sample = True\n",
    "                if 'BENCHMARK' not in line:\n",
    "                    last_insample = []\n",
    "                    last_outsample = []\n",
    "                    expr_in_task = set()            \n",
    "                    expressions_by_length = defaultdict(list)\n",
    "            if \"Evaluating out-of-sample:\" in line:\n",
    "                in_sample = False\n",
    "            if line.startswith('['):                \n",
    "                expr, length, score = parse_evaluation_line(line)\n",
    "                if expr not in expr_in_task: # and ':' not in expr:\n",
    "                    expressions_by_length[length].append(expr)\n",
    "                    expr_in_task.add(expr)\n",
    "                if in_sample:\n",
    "                    last_insample.append((expr, length, score))\n",
    "                else:\n",
    "                    last_outsample.append((expr, length, score))            \n",
    "        \n",
    "        # Parse optimization trace (all lines starting with 'GEN_')\n",
    "        if parse_progress:\n",
    "            progdf = progdf.append(parse_progess_lines(task, gen_lines), ignore_index=True)\n",
    "\n",
    "        for expr, length, score in last_insample:\n",
    "            if length != 0:\n",
    "                if length < 4:\n",
    "                    # Only report one out-of-sample solution for each length (and all benchmarks), so overwrite is OK.\n",
    "                    df.loc[task, \"in-sample\"][f\"length-{length}\"] = score\n",
    "\n",
    "                # Update best so far score and maximum length\n",
    "                df.loc[task, \"in-sample\"][f\"final\"] = np.nanmax(\n",
    "                    [score, df.loc[task, \"in-sample\"][f\"final\"]])\n",
    "                max_length = max(max_length, length)\n",
    "            else:\n",
    "                df.loc[task, \"in-sample\"][expr] = score\n",
    "\n",
    "            if length > max_length:\n",
    "                max_length = length  # To know for which length \"best\" should score out of sample\n",
    "\n",
    "        # Because two equal solutions can be in the Pareto front,\n",
    "        # we note the average out of sample performance if multiple solutions were found.\n",
    "        # Naturally, the solutions with the best in-sample score were those with the highest length in the Pareto front.\n",
    "\n",
    "        scores_by_length = defaultdict(list)\n",
    "        for expr, length, score in last_outsample:\n",
    "            if length != 0:\n",
    "                scores_by_length[length].append(score)\n",
    "            else:\n",
    "                df.loc[task, \"out-sample\"][expr] = score\n",
    "\n",
    "        for length, scores in scores_by_length.items():\n",
    "            if length < 4:\n",
    "                df.loc[task, \"out-sample\"][f\"length-{length}\"] = np.mean(scores)\n",
    "            if length == max_length:\n",
    "                df.loc[task, \"out-sample\"][f\"final\"] = np.mean(scores)\n",
    "            if np.mean(scores) == float(\"nan\"):\n",
    "                print('hi')\n",
    "\n",
    "    return df, expressions_by_length, generations_by_task, baseline, progdf\n",
    "\n",
    "\n",
    "def comparisons(df, sample=\"out-sample\"):\n",
    "    out_sample = df.index.map(lambda idx: idx[1] == sample)\n",
    "\n",
    "    alone = {k: 0 for k in df.iloc[0].index.values}\n",
    "    shared = {k: 0 for k in df.iloc[0].index.values}\n",
    "\n",
    "    for _, out in df.loc[out_sample].iterrows():\n",
    "        best = out[out == out.max()].index.values\n",
    "        if len(best) == 1:\n",
    "            alone[best[0]] += 1\n",
    "        else:\n",
    "            for winner in best:\n",
    "                shared[winner] += 1\n",
    "\n",
    "    alone = {k: alone[k] for k in sorted(alone)}\n",
    "    shared = {k: shared[k] for k in sorted(shared)}\n",
    "    either = {k: shared[k] + alone[k] for k in sorted({**alone, **shared})}\n",
    "    comparison = pd.DataFrame([alone, shared, either], index=['alone', 'shared', 'either'])\n",
    "\n",
    "    df_out = df.loc[out_sample].copy()\n",
    "    df_out['max'] = df_out.max(axis=1)\n",
    "    for col in df_out:\n",
    "        df_out['d_' + col] = df_out['max'] - df_out[col]\n",
    "    d_cols = [c for c in df_out.columns if c.startswith('d_')]\n",
    "    df_out[d_cols].mean()\n",
    "    df_out[d_cols].median()\n",
    "\n",
    "    in_sample = df.index.map(lambda idx: idx[1] == \"in-sample\")\n",
    "    df.loc[in_sample].idxmax(axis=1).value_counts()\n",
    "    df.loc[in_sample][reversed(df.columns)].idxmax(axis=1).value_counts()\n",
    "    return comparison, df_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolic Defaults by 'complexity' of expression\n",
    "In this notebook we take a look at the results of running the script at its default settings, this means:\n",
    " - evaluation across all tasks\n",
    " - recording the pareto front of symbolic defaults after each search\n",
    " - evaluating in-sample and out-of-sample performance of those dynamic defaults, as well as some pre-defined ones\n",
    " \n",
    "**note:** The console cut off results for the first few tasks, so I am rerunning those now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each task we will extract:\n",
    " - the number of generations optimization ran for (max=200)\n",
    " - max length expression\n",
    " - in and out of sample performance for length 1, 2 and 3 expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each task save the benchmark results. We also save results for length 1, 2 and 3 solutions as well as the best one found (that may be longer). Specifically we record:\n",
    " - best in_sample performance at length 1, 2, 3\n",
    " - best in_sample performance for any length\n",
    " - average out_sample performance by length for length 1, 2, 3\n",
    " - average out_sample performance for the longest (i.e. best in-sample score) solution(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We have experiment data for a set of algorithms and meta-data for the datasets on which the experiments took place.\n",
    "We use symbolic regression to find an expression for symbolic default values that give good performance across tasks.\n",
    "Symbolic regression is performed with leave-one-task-out, which means for each algorithm we have multiple searches for a symbolic default, and their performance is recorded for both in-sample (the optimization surface of all-but-one tasks) and out-of-sample (the left out task) performance. Performance here is solely based on surrogate model predictions, no additional experiments have been performed (yet).\n",
    "\n",
    "In our search, we use NSGA-II selection to perform multi-objective optimization: find the expression with the best performance, while using the fewest number of operators (e.g. `divide`, `multiply`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms\n",
    "**Length** of an expression denotes the number of operators in it. A symbolic value is *not* considered an operation.\n",
    "Consider the following SVM defaults for cost and gamma:\n",
    " - `make_tuple`(m, mkd) is length 1.\n",
    " - `make_tuple`(m, `truediv`(mkd, xvar)) is length 2.\n",
    " - `make_tuple`(16., `truediv`(mkd, xvar)) is length 2.\n",
    "\n",
    "The **final** solution refers to the symbolic default with the highest in-sample score for a task (regardless of its length). This means for each task there is *at least* one final solution, but there may be more and they are not of a specific length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **baseline** solutions are typically the default hyperparameter settings of mlr, scikit-learn, or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read all the logs, because some logs are incomplete we have to explicitly give the name of the baselines (this will be fixed for future runs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlr_svm_lisa.log\n",
      "The predefined defaults are (may show a repeat):\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      "mlr_svm_lisa_gauss24.log\n",
      "The predefined defaults are (may show a repeat):\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      "mlr_svm_lisa_gauss24_1eph.log\n",
      "The predefined defaults are (may show a repeat):\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      "mlr_svm_lisa_gauss24_1eph_d1.log\n",
      "The predefined defaults are (may show a repeat):\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      "mlr_svm_lisa_gauss24_1eph_d1_2.log\n",
      "The predefined defaults are (may show a repeat):\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      "mlr_svm_lisa_gauss24_1eph_d1_2i.log\n",
      "The predefined defaults are (may show a repeat):\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "O:\\Anaconda\\envs\\symbdef\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1116: RuntimeWarning: All-NaN slice encountered\n",
      "  overwrite_input=overwrite_input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlr_svm_lisa_gauss24_1eph_d1_3.log\n",
      "The predefined defaults are (may show a repeat):\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      "mlr_svm_lisa_gauss24_1eph_d1_3i.log\n",
      "The predefined defaults are (may show a repeat):\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      "mlr_svm_lisa_gauss24_1eph_d1_4.log\n",
      "The predefined defaults are (may show a repeat):\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * sklearn_default := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n"
     ]
    }
   ],
   "source": [
    "alg = 'mlr_svm_lisa'\n",
    "run_one = f\"{alg}_gauss24\"\n",
    "run_two = f\"{alg}_cst\"\n",
    "\n",
    "import os\n",
    "baselines = dict(\n",
    "    glmnet=[\"mlr_default\", \"sklearn_default\"],\n",
    "    kerasff=[\"initial_values\"],\n",
    "    knn=[\"mlr_default\"],\n",
    "    rf=[\"mlr_default\"],\n",
    "    rpart=[\"mlr_default\"],\n",
    "    asvm=[\"sklearn_scale\", \"symbolic_best\", \"symbolic_v2\" , \"const\"],\n",
    ")\n",
    "traces = {}\n",
    "for file in os.listdir('runs'):\n",
    "    if file.endswith('.log') and alg in file:\n",
    "        print(file)\n",
    "        baseline = []\n",
    "        for method, bls in baselines.items():\n",
    "            if method in file:\n",
    "                baseline = bls\n",
    "        traces[file[:-4]] = Trace(os.path.join('runs', file), benchmarks=baseline, ignore=[\"const\", \"symbolic_v2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "As described before, for each problem we find a symbolic default leaving one task out.\n",
    "We are interested to see how fast the symbolic regression converges across tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Median number of generations across tasks by problem:\")\n",
    "for log, trace in traces.items():\n",
    "    print(f\"{log: <15} {trace.generations_by_task.median().astype(int):3d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "fig, axes = plt.subplots(math.ceil(len(traces) / 4), 4, sharey=True, figsize=(16, 9))\n",
    "for ax, (log, trace) in zip(axes.flatten(), traces.items()):\n",
    "    traces[log].generations_by_task.hist(bins=20, ax=ax)\n",
    "    ax.set_title(f\"{log} ({len(trace.generations_by_task)} tasks)\")\n",
    "    ax.set_ylabel('count')\n",
    "    ax.set_xlabel('generations')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows a histogram counting the number of generations until stopping. These results were obtained with default setting of early stopping if no improvement was made after 20 generations, with a 200 generation maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing optimization traces\n",
    "The traces contain the full optimization traces inside the trace's **progdf** trace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.lineplot(x='gen', y='max', units='task', estimator=None, data=traces[f'{alg}_gauss24_1eph_d1_2'].progdf)\n",
    "sns.lineplot(x='gen', y='max', units='task', estimator=None, data=traces[f'{alg}_gauss24_1eph_d1_2i'].progdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Expressions\n",
    "For a given problem, we have a Pareto front of solutions for search (=each left out task).\n",
    "This Pareto front may contain \"twins\", multiple solutions which performance equally well and have the same length.\n",
    "Given that the response surface does not differ *that* much when leaving any particular task out, we hope that the symbolic expressions we find are reasonably consistent across searches.\n",
    "To have some indication of how consistent the results are, for each problem we find the most frequent solutions of length 1, 2 and 3. We also note the number of hyperparameters for which we aim to find a symbolic default, as we expect this to be correlated to how consistent the solutions are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_count = pd.DataFrame(np.zeros((5, len(traces))), columns=list(traces), index=[1, 2, 3, \"#tasks\", \"params\"])\n",
    "for log, trace in traces.items():  \n",
    "    for length, expressions in sorted(trace.expressions.items()):\n",
    "        if 0 < length < 4:\n",
    "            m = max(set(expressions), key=expressions.count)\n",
    "            expr_count.loc[length][log] = expressions.count(m)\n",
    "            # print(f\" Found {len(expressions):3d} expressions of length {length}. Most frequent: {m} ({expressions.count(m)} times)\")\n",
    "            if length == 1:\n",
    "                expr_count.loc[\"#tasks\"][log] = len(trace.scores) / 2\n",
    "                expr_count.loc[\"params\"][log] = m.count(',') + 1\n",
    "expr_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the found expressions per problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = f'{alg}_gauss24_1eph_d1_2i' # run_one #f\"mlr_knn_lisa_gaussian\" # run_one\n",
    "for log, trace in traces.items():\n",
    "    print(log)\n",
    "    for length, expressions in sorted(trace.expressions.items()):\n",
    "        if 0 < length < 4:\n",
    "            m = max(set(expressions), key=expressions.count)\n",
    "            expr_count.loc[length][log] = expressions.count(m)\n",
    "            # print(f\" Found {len(expressions):3d} expressions of length {length}. Most frequent: {m} ({expressions.count(m)} times)\")\n",
    "            print(f\"Most frequent length {length} solution in Pareto front ({expressions.count(m)} times in {len(traces[a].scores) // 2} tasks):\\n     {m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expression Quality\n",
    "The expressions we find also need to be good.\n",
    "Here we compare the following 'strategies':\n",
    " - length-*n*: always pick the best expression of length *n*\n",
    " - *final*: always pick the best expression, regardless of length\n",
    " - *baseline(s)*: compare it to baselines we defined\n",
    " \n",
    "We want to know (all based on out-of-sample performance):\n",
    " - which strategy gives the best solution most often?\n",
    " - which strategy experiences the least mean regret?\n",
    " - which strategy experiences the least median regret?\n",
    " \n",
    "As mentioned before, there can be \"twins\" in the Pareto front, which means multiple solutions with equal length have equal in-sample performance.\n",
    "In this case we average the out-of-sample score of those twins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number of wins:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table records the number of times a strategy led to the symbolic expression with the best out-of-sample performance (multiple strategies can be the best each task):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_comparisons = pd.DataFrame()\n",
    "for log, trace in traces.items():\n",
    "    out_comparisons = out_comparisons.append(trace.comparison.loc['either'].rename(log))\n",
    "out_comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### median regret:\n",
    "The following table records the median regret for a specific strategy compared to picking the best in hindsight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = pd.DataFrame()\n",
    "for log, trace in traces.items():\n",
    "    m = trace.d_scores.median().rename(log)\n",
    "    medians = medians.append(m)\n",
    "medians[[c for c in medians.columns if 'd_' in c and c != 'd_max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean regret:\n",
    "The following table records the mean regret for a specific strategy compared to picking the best in hindsight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = pd.DataFrame([])\n",
    "for log, trace in traces.items():\n",
    "    m = trace.d_scores.mean().rename(log)\n",
    "    means = means.append(m)\n",
    "means[[c for c in medians.columns if 'd_' in c and c != 'd_max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checks\n",
    "Sometimes out-of-sample performance of a baseline may still be better than that of our solution.\n",
    "However, in-sample performance of our own solutions should always be better than any baseline.\n",
    "If that is not the case, this would indicate our search does not explore the space well enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sample_comparisons = pd.DataFrame()\n",
    "for log, trace in traces.items():\n",
    "    in_sample_comparisons = in_sample_comparisons.append(trace.in_comparison.loc['either'].rename(log))\n",
    "in_sample_comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to random search \n",
    "The following provides an overview over scores for different iterations of random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"mlr_svm\"\n",
    "rsdf = pd.read_csv(\"data/\"+name+\"_baselines.csv\", index_col=0)\n",
    "rsdf.iloc[:,1:].apply(np.mean,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "End of notebook - just sketchpad below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace.scores.isna().any(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = \"svm\"\n",
    "run_one = \"mlr_svm_lisa\"\n",
    "minimum = dict(knn=100, svm=100, glmnet=100, rpart=60)\n",
    "final_scores = pd.DataFrame()\n",
    "for log, trace in traces.items():\n",
    "    # Filter out runs with >100 tasks completed:\n",
    "    if len(trace.scores) / 2 > minimum[alg]:\n",
    "        out_sample = trace.scores.index.map(lambda idx: idx[1] == \"out-sample\")\n",
    "        log_oos = trace.scores.loc[out_sample].final.rename(log)\n",
    "        final_scores = final_scores.append(log_oos)\n",
    "        if log == run_one:\n",
    "            # contains benchmark scores\n",
    "            for b in trace.baseline:\n",
    "                baseline_score = trace.scores.loc[out_sample][b].rename(b)\n",
    "                final_scores = final_scores.append(baseline_score)\n",
    "final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out incomplete tasks:\n",
    "final = final_scores.loc[:, ~final_scores.isna().any()]\n",
    "df = final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_one=\"mlr_svm_lisa\"\n",
    "run_two=\"svm_warm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[[run_one, run_two]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alone = {k: 0 for k in df.index.values}\n",
    "shared = {k: 0 for k in df.index.values}\n",
    "\n",
    "for _, out in df.T.iterrows():\n",
    "    best = out[out == out.max()].index.values\n",
    "    if len(best) == 1:\n",
    "        alone[best[0]] += 1\n",
    "    else:\n",
    "        for winner in best:\n",
    "            shared[winner] += 1\n",
    "\n",
    "alone = {k: alone[k] for k in sorted(alone)}\n",
    "shared = {k: shared[k] for k in sorted(shared)}\n",
    "either = {k: shared[k] + alone[k] for k in sorted({**alone, **shared})}\n",
    "comparison = pd.DataFrame([alone, shared, either], index=['alone', 'shared', 'either'])\n",
    "comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df.T.copy()\n",
    "df_out['max'] = df_out.max(axis=1)\n",
    "for col in df_out:\n",
    "    df_out['d_' + col] = df_out['max'] - df_out[col]\n",
    "d_cols = [c for c in df_out.columns if c.startswith('d_') and 'max' not in c]\n",
    "df_out[d_cols].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out[d_cols].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (df.loc[run_one] - df.loc[run_two]).hist(bins=[(f / 40 - 1) for f in range(81)])\n",
    "ax.set_title(f\"Symbolic - Constant | median: {(df.loc[run_one] - df.loc[run_two]).median():.3f}, mean: {(df.loc[run_one] - df.loc[run_two]).mean():.3f}, {df.shape[1]} tasks\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_xlabel(\"Difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((df.loc[\"mlr_svm_lisa\"] - df.loc[\"svm_cst\"]) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.loc[\"mlr_svm_lisa\"] - df.loc[\"svm_cst\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsdf.columns = [(round(float(x)), 'out-sample')  for x in rsdf.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rsdf.append(final_scores)\n",
    "df.iloc[:,1:].apply(np.mean,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.transpose().melt(var_name=\"method\", value_name=\"performance\")\n",
    "ax = sns.boxplot(x='method', y='performance', data = pdf)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
