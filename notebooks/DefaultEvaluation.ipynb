{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of default configurations\n",
    "\n",
    "\n",
    "We have two answer to questions:\n",
    " 1. By which method can we find good Symbolic Defaults? \n",
    " 2. **Can we find good (i.e. better than currently known) symbolic defaults?**\n",
    " \n",
    "This notebook addresses the second question.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "general remarks:\n",
    " - does not currently factor in runtime, but this may be especially important for default values. I didn't explicitly measure runtime, but the 'symbolic defaults' in adaboost take much longer (because it significantly increases `n_estimators` and `max_depth`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from visualization.output_parser import generate_comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1a. SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After determining good symbolic defaults, we ought to see how they compare to current (scikit-learn) defaults. To this end, we compare three different default configurations (in bold is the name by which they will be referenced henceforth):\n",
    "\n",
    " - The **symbolic** defaults we found from evolutionary optimization, specifically: `C=16, gamma=mkd/xvar`.\n",
    "     This symbolic function uses metafeatures as calculated on the dataset *after* it has been preprocessed.\n",
    " - The scikit-learn **0.20** defaults, specifically: `C=1., gamma=(1 / n_features)`\n",
    " - The scikit-learn >= **0.22** defaults, specifically: `C=1., gamma=(1 / (n_features * X.var()))`\n",
    " \n",
    "Note that actually all of these defaults are symbolic.\n",
    "\n",
    "A second important detail to note is that these settings are not tried by themselves.\n",
    "A (fairly standard) preprocessing pipeline is applied:\n",
    " - **Imputation**: using the mean for numeric features, and the most frequent value for categorical features.\n",
    " - **Transformation**: numeric features are scaled to N(0, 1), categorical features are one-hot encoded.\n",
    " - **Feature Selection**: all constant features are removed.\n",
    " \n",
    "After these steps, the SVC is invoked on the preprocessed data with the given values for `C` and `gamma`.\n",
    "\n",
    "Note that I had previously ran experiments that used meta-features computed on the data before preprocessing, and they resulted in different defaults: `C=128, gamma=(mkd / 4)`. I performed an evaluation with those defaults too, and they were better than both new and old scikit-learn defaults, but worse than the newly found symbolic defaults computed on the metafeatures after transformation. Those results however were still stored in an older format and will require some additional processing before I can compare them again to this newer format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Results\n",
    "We compare results by number of times one's average cross-validation performance is better (first three columns) and by their loss as compared to the best found result in the original set of experiments ('loss' column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbolic-post outperformed best on task 14 by -0.0015000000000000568\n",
      "symbolic-post outperformed best on task 12 by -1.1102230246251565e-16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbolic-post</th>\n",
       "      <th>0.20</th>\n",
       "      <th>0.22</th>\n",
       "      <th>loss</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>symbolic-post</th>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1.660083</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.20</th>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.107663</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.22</th>\n",
       "      <td>19.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.493432</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               symbolic-post  0.20  0.22      loss     N\n",
       "symbolic-post            0.0  70.0  68.0  1.660083  92.0\n",
       "0.20                    18.0   0.0   3.0  4.107663  92.0\n",
       "0.22                    19.0  26.0   0.0  3.493432  92.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_comparisons('svc_rbf',\n",
    "                    [\"data/results/svc_rbf_c16_gmkdxvar.txt\",\n",
    "                     \"data/results/ppp_svc_rbf_default.txt\",\n",
    "                     \"data/results/ppp_svc_rbf_default_Gscale.txt\"\n",
    "                    ],\n",
    "                    ['symbolic-post','0.20','0.22'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reads as *Symbolic-post* won over the *0.20* default 68 times, while the *0.20* default was better than *Symbolic* on 18 tasks. *Symbolic* obtained a loss of 1.66 over the best known result of each task (or slightly under 0.02 accuracy, on average).\n",
    "\n",
    "We see that *Symbolic* (i.e. `C=16, gamma=mkd/xvar`) as default outperforms either of the two scikit-learn ones, both in terms of tasks where it achieves higher predictive accuracy, and the loss in accuracy it occurs across tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1b SVC Poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b.1 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The grid search result from Jan.\n",
    "svc_results = load_random_search_results('svc_poly')\n",
    "\n",
    "# results currently still stored in log. should be aggregated to single file..\n",
    "# \"data/results/pipeline_c128mkd4.txt\"\n",
    "symb_default_performances = get_performance_from_csv(\"data/results/ppp_svc_poly_D1_C6_G7.txt\")\n",
    "old_default_performances = get_performance_from_csv(\"data/results/ppp_svc_poly_default.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b.2 Comparing Results\n",
    "We compare results by number of times one's average cross-validation performance is better (first three columns) and by their loss as compared to the best found result in the original set of experiments ('loss' column).\n",
    "\n",
    "**note:** The 'loss' column is calculated over all executed tasks with the default, because of work-in-progress/earlier cut-off with time constaints, currently the amount of tasks evaluated per method differs, so you find the amount of tasks for which the defaults have been evaluated in the 'N' column. The total loss is *not* normalized for the amount of completed tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "methods = ['Symbolic', '0.20']\n",
    "df = pd.DataFrame(np.zeros(shape=(len(methods), len(methods)+2)), columns = methods + ['loss', 'N'])\n",
    "df.index = methods\n",
    "\n",
    "# Calculate 'wins'\n",
    "performances = list(zip(methods, [symb_default_performances, old_default_performances]))\n",
    "for (method, performance) in performances:\n",
    "    for (method2, performance2) in performances:\n",
    "        one_over_two = (performance.avg - performance2.avg) > 0\n",
    "        df.loc[method][method2] = sum(one_over_two)\n",
    "\n",
    "# Calculate loss        \n",
    "for (method, performance) in performances:\n",
    "    loss_sum = 0\n",
    "    for i, row in performance.iterrows():\n",
    "        best_score = svc_results[svc_results.task_id == row.name].predictive_accuracy.max()\n",
    "        loss = best_score - row.avg\n",
    "        if loss < 0:\n",
    "            print('{} outperformed best on task {} by {}'.format(method, row.name, loss))\n",
    "        loss_sum += loss\n",
    "    df.loc[method]['loss'] = loss_sum\n",
    "    df.loc[method]['N'] = len(performance)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the evolutionary optimization, we found that often recommended symbolic defaults:\n",
    " - **learning rate**: 0.75..1.0\n",
    " - **n_estimators**: n\n",
    " - **max_depth**: p\n",
    " \n",
    "However, as also noted due to the values of hyperparameters in the original experiments, it might as well read `n_estimators=500` and `max_depth=10`, the bounds of the original experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not aware of any proposed changes to default values for this algorithm, so the only comparison is to scikit-learn defaults as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The grid search result from Jan.\n",
    "adaboost_results = load_random_search_results('adaboost')\n",
    "\n",
    "# results currently still stored in log. should be aggregated to single file..\n",
    "# \"data/results/pipeline_c128mkd4.txt\"\n",
    "symb_default_performances = get_performance_from_console_output(\"data/results/pipeline_ada_75_500_10.txt\")\n",
    "sklearn_default_performances = get_performance_from_console_output(\"data/results/pipeline_ada_default.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Comparing Results\n",
    "We compare results by number of times one's average cross-validation performance is better (first three columns) and by their loss as compared to the best found result in the original set of experiments (last column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "methods = ['Symbolic', 'sklearn']\n",
    "df = pd.DataFrame(np.zeros(shape=(len(methods), len(methods)+2)), columns = methods + ['loss', 'N'])\n",
    "df.index = methods\n",
    "\n",
    "# Calculate 'wins'\n",
    "performances = list(zip(methods, [symb_default_performances, sklearn_default_performances]))\n",
    "for (method, performance) in performances:\n",
    "    for (method2, performance2) in performances:\n",
    "        one_over_two = (performance.avg - performance2.avg) > 0\n",
    "        df.loc[method][method2] = sum(one_over_two)\n",
    "\n",
    "# Calculate loss        \n",
    "for (method, performance) in performances:\n",
    "    loss_sum = 0\n",
    "    for i, row in performance.iterrows():\n",
    "        best_score = adaboost_results[adaboost_results.task_id == row.name].predictive_accuracy.max()\n",
    "        loss = best_score - row.avg\n",
    "        if loss < 0:\n",
    "            print('{} outperformed best on task {} by {}'.format(method, row.name, loss))\n",
    "        loss_sum += loss\n",
    "    df.loc[method]['loss'] = loss_sum\n",
    "    df.loc[method]['N'] = len(performance)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Symbolic defaults take far longer to train, and I could not reasonably evaluate all datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Loading Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The grid search result from Jan.\n",
    "rfc_results = load_random_search_results('randomforest')\n",
    "\n",
    "# results currently still stored in log. should be aggregated to single file..\n",
    "# \"data/results/pipeline_c128mkd4.txt\"\n",
    "symb_default_performances = get_performance_from_csv(\"data/results/ppp_rf_p027.txt\")\n",
    "sklearn_default_performances = get_performance_from_csv(\"data/results/ppp_rfc_default.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Comparing Results\n",
    "We compare results by number of times one's average cross-validation performance is better (first three columns) and by their loss as compared to the best found result in the original set of experiments (last column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "methods = ['Symbolic', 'sklearn']\n",
    "df = pd.DataFrame(np.zeros(shape=(len(methods), len(methods)+2)), columns = methods + ['loss', 'N'])\n",
    "df.index = methods\n",
    "\n",
    "# Calculate 'wins'\n",
    "performances = list(zip(methods, [symb_default_performances, sklearn_default_performances]))\n",
    "for (method, performance) in performances:\n",
    "    for (method2, performance2) in performances:\n",
    "        one_over_two = (performance.avg - performance2.avg) > 0\n",
    "        df.loc[method][method2] = sum(one_over_two)\n",
    "\n",
    "# Calculate loss        \n",
    "for (method, performance) in performances:\n",
    "    loss_sum = 0\n",
    "    for i, row in performance.iterrows():\n",
    "        best_score = rfc_results[rfc_results.task_id == row.name].predictive_accuracy.max()\n",
    "        if np.isnan(best_score):\n",
    "            continue\n",
    "        loss = best_score - row.avg\n",
    "        if loss < 0:\n",
    "            print('{} outperformed best on task {} by {}'.format(method, row.name, loss))\n",
    "        print(method, row.name, loss, best_score, row.avg)\n",
    "        loss_sum += loss\n",
    "    df.loc[method]['loss'] = loss_sum\n",
    "    df.loc[method]['N'] = len(performance)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The default of `max_features=0.271` did outperform the default of `sqrt(p)` in terms of loss. However, it is not a clear better default. The default of `sqrt(p)` is still better more often and scales much better with the amount of features in a dataset in terms of runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from visualization.output_parser import generate_comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic outperformed best on task 3493 by -2.4371584699256488e-06\n",
      "Default outperformed best on task 3493 by -2.4371584699256488e-06\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbolic</th>\n",
       "      <th>Default</th>\n",
       "      <th>loss</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Symbolic</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.665586</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Default</th>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.105805</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Symbolic  Default      loss     N\n",
       "Symbolic       0.0      2.0  5.665586  82.0\n",
       "Default       65.0      0.0  3.105805  95.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_comparisons('xgb_linear',\n",
    "                     [\"data/results/xgblinear_symbolic_eval\",\n",
    "                      \"data/results/xgblinear_default_eval\"],\n",
    "                     ['Symbolic', 'Default'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that above loss is only calculated for ~37 tasks (and possibly a different amount of tasks for symbolic and default, though if this is the case those tasks for symbolic are a subset of default, meaning that default has less loss either way)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**note**: Everything below is scratchpad and should be ignored\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
