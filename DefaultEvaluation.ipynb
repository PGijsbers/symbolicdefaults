{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of default configurations\n",
    "\n",
    "\n",
    "We have two answer to questions:\n",
    " 1. By which method can we find good Symbolic Defaults? \n",
    " 2. **Can we find good (i.e. better than currently known) symbolic defaults?**\n",
    " \n",
    "This notebook addresses the second question.\n",
    "\n",
    "----\n",
    "\n",
    "After determining good symbolic defaults, we ought to see how they compare to current (scikit-learn) defaults.\n",
    "To this end, we compare three different default configurations (in bold is the name by which they will be referenced henceforth):\n",
    "\n",
    " - The **symbolic** defaults we found from evolutionary optimization, specifically: `C=128, gamma=(mkd / 4)`\n",
    " - The scikit-learn **0.20** defaults, specifically: `C=1., gamma=(1 / n_features)`\n",
    " - The scikit-learn >= **0.22** defaults, specifically: `C=1., gamma=(1 / (n_features * X.var()))`\n",
    " \n",
    "Note that actually all of these defaults are symbolic.\n",
    "\n",
    "A second important detail to note is that these settings are not tried by themselves.\n",
    "A (fairly standard) preprocessing pipeline is applied:\n",
    " - **Imputation**: using the mean for numeric features, and the most frequent value for categorical features.\n",
    " - **Transformation**: numeric features are scaled to N(0, 1), categorical features are one-hot encoded.\n",
    " - **Feature Selection**: all constant features are removed.\n",
    " \n",
    "After these steps, the SVC is invoked on the preprocessed data with the given values for `C` and `gamma`.\n",
    "\n",
    "Note: for the scikit-learn defaults, currently the metafeatures of the preprocessed data are used (e.g. `n_features` is determined after one-hot encoding, for instance). For the *symbolic* method `mkd` is determined on the original, (largely) unprocessed dataset (samples with NaN values are ignored)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1  Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data from results\n",
    "from persistence import load_problem, load_results_for_problem\n",
    "p = load_problem('problems.json', 'svc')\n",
    "svc_results = load_results_for_problem(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A couple methods that help parse data from the output logs.\n",
    "import operator\n",
    "def is_performance_line(line):\n",
    "    return line.count(' ') == 2\n",
    "\n",
    "def parse_performance_line(line):\n",
    "    task, avg, std = line[:-1].split(' ')\n",
    "    return int(task), float(avg), float(std)\n",
    "\n",
    "def get_performance_from_console_output(file):\n",
    "    with open(file, 'r') as fh:\n",
    "        lines = fh.readlines()\n",
    "    return [parse_performance_line(line) for line in lines\n",
    "            if is_performance_line(line)]\n",
    "\n",
    "def compare(list1, list2, idx, operator):\n",
    "    \"\"\" Compares two lists of tuples by their i-th element according to operator. \"\"\"\n",
    "    return list([operator(a[idx], b[idx]) for (a, b) in zip(list1, list2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load output logs\n",
    "symb_default_file = \"data/results/pipeline_c128mkd4.txt\"\n",
    "old_default_file = \"data/results/pipeline_default.txt\"\n",
    "new_default_file = \"data/results/pipeline_scale.txt\"\n",
    "\n",
    "symb_default_performances = get_performance_from_console_output(symb_default_file)\n",
    "old_default_performances = get_performance_from_console_output(old_default_file)\n",
    "new_default_performances = get_performance_from_console_output(new_default_file)\n",
    "\n",
    "# Make sure that we always compare the same tasks (idx 0 is task)\n",
    "assert sum(compare(symb_default_performances, old_default_performances, 0, operator.ne)) == 0\n",
    "assert sum(compare(symb_default_performances, new_default_performances, 0, operator.ne)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Comparing Results\n",
    "We compare results by number of times one's average cross-validation performance is better (first three columns) and by their loss as compared to the best found result in the original set of experiments (last column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic outperformed best on task 3543 by -1.1102230246251565e-16\n",
      "Symbolic outperformed best on task 3561 by -0.00732265496049167\n",
      "Symbolic outperformed best on task 34538 by -0.005555074074073962\n",
      "Symbolic outperformed best on task 9956 by -0.0012622452830188813\n",
      "0.20 outperformed best on task 3543 by -1.1102230246251565e-16\n",
      "0.20 outperformed best on task 34538 by -0.0027772962962961945\n",
      "0.20 outperformed best on task 23 by -0.006135132744989891\n",
      "0.22 outperformed best on task 3543 by -1.1102230246251565e-16\n",
      "0.22 outperformed best on task 34538 by -0.0027772962962961945\n",
      "0.22 outperformed best on task 23 by -0.005468649935649883\n",
      "0.22 outperformed best on task 20 by -0.0024999999999999467\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbolic</th>\n",
       "      <th>0.20</th>\n",
       "      <th>0.22</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Symbolic</th>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.639799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.20</th>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.893993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.22</th>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.359051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Symbolic  0.20  0.22      loss\n",
       "Symbolic       0.0  23.0  21.0  0.639799\n",
       "0.20          16.0   0.0   3.0  2.893993\n",
       "0.22          16.0  17.0   0.0  2.359051"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "methods = ['Symbolic', '0.20', '0.22']\n",
    "df = pd.DataFrame(np.zeros(shape=(len(methods), len(methods)+1)), columns = methods + ['loss'])\n",
    "df.index = methods\n",
    "\n",
    "performances = list(zip(methods, [symb_default_performances, old_default_performances, new_default_performances]))\n",
    "for (method, performance) in performances:\n",
    "    for (method2, performance2) in performances:\n",
    "        one_over_two = compare(performance, performance2, 1, operator.gt)\n",
    "        df.loc[method][method2] = sum(one_over_two)\n",
    "\n",
    "for (method, performance) in performances:\n",
    "    loss_sum = 0\n",
    "    for (task, score, _) in performance:\n",
    "        best_score = svc_results[svc_results.task_id == task].predictive_accuracy.max()\n",
    "        loss = best_score - score\n",
    "        if loss < 0:\n",
    "            print('{} outperformed best on task {} by {}'.format(method, task, loss))\n",
    "        loss_sum += loss\n",
    "    df.loc[method]['loss'] = loss_sum\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
