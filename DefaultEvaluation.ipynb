{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of default configurations\n",
    "\n",
    "\n",
    "We have two answer to questions:\n",
    " 1. By which method can we find good Symbolic Defaults? \n",
    " 2. **Can we find good (i.e. better than currently known) symbolic defaults?**\n",
    " \n",
    "This notebook addresses the second question.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After determining good symbolic defaults, we ought to see how they compare to current (scikit-learn) defaults. To this end, we compare three different default configurations (in bold is the name by which they will be referenced henceforth):\n",
    "\n",
    " - The **symbolic_pre** defaults we found from evolutionary optimization, specifically: `C=128, gamma=(mkd / 4)`. \n",
    " This symbolic function uses metafeatures as calculated on the dataset *before* it is preprocessed.\n",
    " - The **symbolic_post** defaults we found from evolutionary optimization, specifically: `C=64, gamma=mkd`.\n",
    "     This symbolic function uses metafeatures as calculated on the dataset *after* it has been preprocessed.\n",
    " - The scikit-learn **0.20** defaults, specifically: `C=1., gamma=(1 / n_features)`\n",
    " - The scikit-learn >= **0.22** defaults, specifically: `C=1., gamma=(1 / (n_features * X.var()))`\n",
    " \n",
    "Note that actually all of these defaults are symbolic.\n",
    "\n",
    "A second important detail to note is that these settings are not tried by themselves.\n",
    "A (fairly standard) preprocessing pipeline is applied:\n",
    " - **Imputation**: using the mean for numeric features, and the most frequent value for categorical features.\n",
    " - **Transformation**: numeric features are scaled to N(0, 1), categorical features are one-hot encoded.\n",
    " - **Feature Selection**: all constant features are removed.\n",
    " \n",
    "After these steps, the SVC is invoked on the preprocessed data with the given values for `C` and `gamma`.\n",
    "\n",
    "Note: for the scikit-learn defaults, currently the metafeatures of the preprocessed data are used (e.g. `n_features` is determined after one-hot encoding, for instance). For the *symbolic* method `mkd` is determined on the original, (largely) unprocessed dataset (samples with NaN values are ignored)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1  Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from persistence import load_problem, load_results_for_problem\n",
    "from visualization.output_parser import get_performance_from_console_output\n",
    "\n",
    "def load_random_search_results(problem_name):\n",
    "    p = load_problem('problems.json', problem_name)\n",
    "    return load_results_for_problem(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The grid search result from Jan.\n",
    "svc_results = load_random_search_results('svc')\n",
    "\n",
    "# results currently still stored in log. should be aggregated to single file..\n",
    "# \"data/results/pipeline_c128mkd4.txt\"\n",
    "symb_default_performances = get_performance_from_console_output(\"data/results/pipeline_c128mkd4.txt\")\n",
    "old_default_performances = get_performance_from_console_output(\"data/results/pipeline_default.txt\")\n",
    "new_default_performances = get_performance_from_console_output(\"data/results/pipeline_scale.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Comparing Results\n",
    "We compare results by number of times one's average cross-validation performance is better (first three columns) and by their loss as compared to the best found result in the original set of experiments (last column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic outperformed best on task 3543 by -1.1102230246251565e-16\n",
      "Symbolic outperformed best on task 3561 by -0.00732265496049167\n",
      "Symbolic outperformed best on task 34538 by -0.005555074074073962\n",
      "Symbolic outperformed best on task 9956 by -0.0012622452830188813\n",
      "Symbolic outperformed best on task 20 by -0.0025000000000000577\n",
      "Symbolic outperformed best on task 3 by -0.0012510815047022117\n",
      "Symbolic outperformed best on task 125922 by -0.00036372727272715455\n",
      "Symbolic outperformed best on task 34539 by -0.0030215009216191246\n",
      "Symbolic outperformed best on task 14965 by -0.00022149201108878636\n",
      "0.20 outperformed best on task 3543 by -1.1102230246251565e-16\n",
      "0.20 outperformed best on task 34538 by -0.0027772962962961945\n",
      "0.20 outperformed best on task 23 by -0.006135132744989891\n",
      "0.22 outperformed best on task 3543 by -1.1102230246251565e-16\n",
      "0.22 outperformed best on task 34538 by -0.0027772962962961945\n",
      "0.22 outperformed best on task 23 by -0.005468649935649883\n",
      "0.22 outperformed best on task 20 by -0.0024999999999999467\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbolic</th>\n",
       "      <th>0.20</th>\n",
       "      <th>0.22</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Symbolic</th>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.612471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.20</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.893993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.22</th>\n",
       "      <td>27.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.359051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Symbolic  0.20  0.22      loss\n",
       "Symbolic       0.0  35.0  40.0  1.612471\n",
       "0.20          22.0   0.0   3.0  2.893993\n",
       "0.22          27.0  17.0   0.0  2.359051"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "methods = ['Symbolic', '0.20', '0.22']\n",
    "df = pd.DataFrame(np.zeros(shape=(len(methods), len(methods)+1)), columns = methods + ['loss'])\n",
    "df.index = methods\n",
    "\n",
    "# Calculate 'wins'\n",
    "performances = list(zip(methods, [symb_default_performances, old_default_performances, new_default_performances]))\n",
    "for (method, performance) in performances:\n",
    "    for (method2, performance2) in performances:\n",
    "        one_over_two = (performance.avg - performance2.avg) > 0\n",
    "        df.loc[method][method2] = sum(one_over_two)\n",
    "\n",
    "# Calculate loss        \n",
    "for (method, performance) in performances:\n",
    "    loss_sum = 0\n",
    "    for i, row in performance.iterrows():\n",
    "        best_score = svc_results[svc_results.task_id == row.name].predictive_accuracy.max()\n",
    "        loss = best_score - row.avg\n",
    "        if loss < 0:\n",
    "            print('{} outperformed best on task {} by {}'.format(method, row.name, loss))\n",
    "        loss_sum += loss\n",
    "    df.loc[method]['loss'] = loss_sum\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reads as *Symbolic* won over the *0.20* default 40 times, while the *0.20* default was better than *Symbolic* on 22 tasks. *Symbolic* obtained a loss of 1.612 over the best known result of each task.\n",
    "\n",
    "We see that *Symbolic* (i.e. `C=128, gamma=mkd/4`) as default outperforms either of the two scikit-learn ones, both in terms of tasks where it achieves higher predictive accuracy, and the loss in accuracy it occurs across tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**note**: Everything below is scratchpad and should be ignored\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
