{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of default configurations\n",
    "\n",
    "\n",
    "We have two answer to questions:\n",
    " 1. By which method can we find good Symbolic Defaults? \n",
    " 2. **Can we find good (i.e. better than currently known) symbolic defaults?**\n",
    " \n",
    "This notebook addresses the second question.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "general remarks:\n",
    " - does not currently factor in runtime, but this may be especially important for default values. I didn't explicitly measure runtime, but the 'symbolic defaults' in adaboost take much longer (because it significantly increases `n_estimators` and `max_depth`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from persistence import load_problem, load_results_for_problem\n",
    "from visualization.output_parser import get_performance_from_console_output, get_performance_from_csv\n",
    "\n",
    "def load_random_search_results(problem_name):\n",
    "    p = load_problem('problems.json', problem_name)\n",
    "    return load_results_for_problem(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1a. SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After determining good symbolic defaults, we ought to see how they compare to current (scikit-learn) defaults. To this end, we compare three different default configurations (in bold is the name by which they will be referenced henceforth):\n",
    "\n",
    " - The **symbolic_pre** defaults we found from evolutionary optimization, specifically: `C=128, gamma=(mkd / 4)`. \n",
    " This symbolic function uses metafeatures as calculated on the dataset *before* it is preprocessed.\n",
    " - The **symbolic_post** defaults we found from evolutionary optimization, specifically: `C=64, gamma=mkd`.\n",
    "     This symbolic function uses metafeatures as calculated on the dataset *after* it has been preprocessed.\n",
    " - The scikit-learn **0.20** defaults, specifically: `C=1., gamma=(1 / n_features)`\n",
    " - The scikit-learn >= **0.22** defaults, specifically: `C=1., gamma=(1 / (n_features * X.var()))`\n",
    " \n",
    "Note that actually all of these defaults are symbolic.\n",
    "\n",
    "A second important detail to note is that these settings are not tried by themselves.\n",
    "A (fairly standard) preprocessing pipeline is applied:\n",
    " - **Imputation**: using the mean for numeric features, and the most frequent value for categorical features.\n",
    " - **Transformation**: numeric features are scaled to N(0, 1), categorical features are one-hot encoded.\n",
    " - **Feature Selection**: all constant features are removed.\n",
    " \n",
    "After these steps, the SVC is invoked on the preprocessed data with the given values for `C` and `gamma`.\n",
    "\n",
    "Note: for the scikit-learn defaults, currently the metafeatures of the preprocessed data are used (e.g. `n_features` is determined after one-hot encoding, for instance). For the *symbolic* method `mkd` is determined on the original, (largely) unprocessed dataset (samples with NaN values are ignored)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a.1  Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The grid search result from Jan.\n",
    "svc_results = load_random_search_results('svc_rbf')\n",
    "\n",
    "# results currently still stored in log. should be aggregated to single file..\n",
    "# \"data/results/pipeline_c128mkd4.txt\"\n",
    "symb_default_performances = get_performance_from_csv(\"data/results/ppp_svc_rbf_C32_Gmkd.txt\")\n",
    "old_default_performances = get_performance_from_csv(\"data/results/ppp_svc_rbf_default.txt\")\n",
    "new_default_performances = get_performance_from_csv(\"data/results/ppp_svc_rbf_default_Gscale.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a.2 Comparing Results\n",
    "We compare results by number of times one's average cross-validation performance is better (first three columns) and by their loss as compared to the best found result in the original set of experiments ('loss' column).\n",
    "\n",
    "**note:** The 'loss' column is calculated over all executed tasks with the default, because of work-in-progress/earlier cut-off with time constaints, currently the amount of tasks evaluated per method differs, so you find the amount of tasks for which the defaults have been evaluated in the 'N' column. The total loss is *not* normalized for the amount of completed tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic outperformed best on task 29 by -0.0057970434782607105\n",
      "Symbolic outperformed best on task 3560 by -0.003849189873417702\n",
      "Symbolic outperformed best on task 34538 by -0.003703222222221969\n",
      "Symbolic outperformed best on task 23 by -0.006135132744989891\n",
      "Symbolic outperformed best on task 9956 by -0.0037779685534591323\n",
      "Symbolic outperformed best on task 20 by -0.0024999999999999467\n",
      "Symbolic outperformed best on task 14 by -0.000500000000000056\n",
      "Symbolic outperformed best on task 146607 by -0.0010720731730265998\n",
      "Symbolic outperformed best on task 14965 by -0.0003985369498339386\n",
      "Symbolic outperformed best on task 7592 by -0.003931128026509967\n",
      "0.20 outperformed best on task 34538 by -0.0027772962962961945\n",
      "0.20 outperformed best on task 23 by -0.006135132744989891\n",
      "0.22 outperformed best on task 125920 by -0.008000000000000007\n",
      "0.22 outperformed best on task 34538 by -0.0027772962962961945\n",
      "0.22 outperformed best on task 23 by -0.008185141937856244\n",
      "0.22 outperformed best on task 7592 by -0.002088418097822453\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbolic</th>\n",
       "      <th>0.20</th>\n",
       "      <th>0.22</th>\n",
       "      <th>loss</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Symbolic</th>\n",
       "      <td>0.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1.740567</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.20</th>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.965604</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.22</th>\n",
       "      <td>21.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.351373</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Symbolic  0.20  0.22      loss     N\n",
       "Symbolic       0.0  67.0  65.0  1.740567  92.0\n",
       "0.20          19.0   0.0   3.0  3.965604  92.0\n",
       "0.22          21.0  26.0   0.0  3.351373  92.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "methods = ['Symbolic', '0.20', '0.22']\n",
    "df = pd.DataFrame(np.zeros(shape=(len(methods), len(methods)+2)), columns = methods + ['loss', 'N'])\n",
    "df.index = methods\n",
    "\n",
    "# Calculate 'wins'\n",
    "performances = list(zip(methods, [symb_default_performances, old_default_performances, new_default_performances]))\n",
    "for (method, performance) in performances:\n",
    "    for (method2, performance2) in performances:\n",
    "        one_over_two = (performance.avg - performance2.avg) > 0\n",
    "        df.loc[method][method2] = sum(one_over_two)\n",
    "\n",
    "# Calculate loss        \n",
    "for (method, performance) in performances:\n",
    "    loss_sum = 0\n",
    "    for i, row in performance.iterrows():\n",
    "        best_score = svc_results[svc_results.task_id == row.name].predictive_accuracy.max()\n",
    "        loss = best_score - row.avg\n",
    "        if loss < 0:\n",
    "            print('{} outperformed best on task {} by {}'.format(method, row.name, loss))\n",
    "        loss_sum += loss\n",
    "    df.loc[method]['loss'] = loss_sum\n",
    "    df.loc[method]['N'] = len(performance)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reads as *Symbolic* won over the *0.20* default 67 times, while the *0.20* default was better than *Symbolic* on 19 tasks. *Symbolic* obtained a loss of 1.74 over the best known result of each task (or slightly under 0.02 accuracy, on average).\n",
    "\n",
    "We see that *Symbolic* (i.e. `C=128, gamma=mkd/4`) as default outperforms either of the two scikit-learn ones, both in terms of tasks where it achieves higher predictive accuracy, and the loss in accuracy it occurs across tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1b SVC Poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b.1 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The grid search result from Jan.\n",
    "svc_results = load_random_search_results('svc_poly')\n",
    "\n",
    "# results currently still stored in log. should be aggregated to single file..\n",
    "# \"data/results/pipeline_c128mkd4.txt\"\n",
    "symb_default_performances = get_performance_from_csv(\"data/results/ppp_svc_poly_D1_C6_G7.txt\")\n",
    "old_default_performances = get_performance_from_csv(\"data/results/ppp_svc_poly_default.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Comparing Results\n",
    "We compare results by number of times one's average cross-validation performance is better (first three columns) and by their loss as compared to the best found result in the original set of experiments ('loss' column).\n",
    "\n",
    "**note:** The 'loss' column is calculated over all executed tasks with the default, because of work-in-progress/earlier cut-off with time constaints, currently the amount of tasks evaluated per method differs, so you find the amount of tasks for which the defaults have been evaluated in the 'N' column. The total loss is *not* normalized for the amount of completed tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symbolic outperformed best on task 9976 by -0.01999992307692311\n",
      "Symbolic outperformed best on task 3904 by -0.0002752198725218813\n",
      "0.20 outperformed best on task 34538 by -0.004629148148147966\n",
      "0.20 outperformed best on task 9956 by -0.0012683144654086487\n",
      "0.20 outperformed best on task 9954 by -0.0031250000000000444\n",
      "0.20 outperformed best on task 9955 by -0.026249999999999996\n",
      "0.20 outperformed best on task 20 by -0.0014999999999999458\n",
      "0.20 outperformed best on task 3917 by -0.0009414299255247061\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbolic</th>\n",
       "      <th>0.20</th>\n",
       "      <th>loss</th>\n",
       "      <th>N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Symbolic</th>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.213985</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.20</th>\n",
       "      <td>77.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.493108</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Symbolic  0.20       loss     N\n",
       "Symbolic       0.0  12.0  11.213985  92.0\n",
       "0.20          77.0   0.0   3.493108  91.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "methods = ['Symbolic', '0.20']\n",
    "df = pd.DataFrame(np.zeros(shape=(len(methods), len(methods)+2)), columns = methods + ['loss', 'N'])\n",
    "df.index = methods\n",
    "\n",
    "# Calculate 'wins'\n",
    "performances = list(zip(methods, [symb_default_performances, old_default_performances, new_default_performances]))\n",
    "for (method, performance) in performances:\n",
    "    for (method2, performance2) in performances:\n",
    "        one_over_two = (performance.avg - performance2.avg) > 0\n",
    "        df.loc[method][method2] = sum(one_over_two)\n",
    "\n",
    "# Calculate loss        \n",
    "for (method, performance) in performances:\n",
    "    loss_sum = 0\n",
    "    for i, row in performance.iterrows():\n",
    "        best_score = svc_results[svc_results.task_id == row.name].predictive_accuracy.max()\n",
    "        loss = best_score - row.avg\n",
    "        if loss < 0:\n",
    "            print('{} outperformed best on task {} by {}'.format(method, row.name, loss))\n",
    "        loss_sum += loss\n",
    "    df.loc[method]['loss'] = loss_sum\n",
    "    df.loc[method]['N'] = len(performance)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the evolutionary optimization, we found that often recommended symbolic defaults:\n",
    " - **learning rate**: 0.75..1.0\n",
    " - **n_estimators**: n\n",
    " - **max_depth**: p\n",
    " \n",
    "However, as also noted due to the values of hyperparameters in the original experiments, it might as well read `n_estimators=500` and `max_depth=10`, the bounds of the original experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not aware of any proposed changes to default values for this algorithm, so the only comparison is to scikit-learn defaults as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The grid search result from Jan.\n",
    "adaboost_results = load_random_search_results('adaboost')\n",
    "\n",
    "# results currently still stored in log. should be aggregated to single file..\n",
    "# \"data/results/pipeline_c128mkd4.txt\"\n",
    "symb_default_performances = get_performance_from_console_output(\"data/results/pipeline_ada_75_500_10.txt\")\n",
    "sklearn_default_performances = get_performance_from_console_output(\"data/results/pipeline_ada_default.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Comparing Results\n",
    "We compare results by number of times one's average cross-validation performance is better (first three columns) and by their loss as compared to the best found result in the original set of experiments (last column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "methods = ['Symbolic', 'sklearn']\n",
    "df = pd.DataFrame(np.zeros(shape=(len(methods), len(methods)+2)), columns = methods + ['loss', 'N'])\n",
    "df.index = methods\n",
    "\n",
    "# Calculate 'wins'\n",
    "performances = list(zip(methods, [symb_default_performances, sklearn_default_performances]))\n",
    "for (method, performance) in performances:\n",
    "    for (method2, performance2) in performances:\n",
    "        one_over_two = (performance.avg - performance2.avg) > 0\n",
    "        df.loc[method][method2] = sum(one_over_two)\n",
    "\n",
    "# Calculate loss        \n",
    "for (method, performance) in performances:\n",
    "    loss_sum = 0\n",
    "    for i, row in performance.iterrows():\n",
    "        best_score = svc_results[svc_results.task_id == row.name].predictive_accuracy.max()\n",
    "        loss = best_score - row.avg\n",
    "        if loss < 0:\n",
    "            print('{} outperformed best on task {} by {}'.format(method, row.name, loss))\n",
    "        loss_sum += loss\n",
    "    df.loc[method]['loss'] = loss_sum\n",
    "    df.loc[method]['N'] = len(performance)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**note**: Everything below is scratchpad and should be ignored\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
