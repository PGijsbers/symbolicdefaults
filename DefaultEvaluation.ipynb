{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of default configurations\n",
    "\n",
    "\n",
    "We have two answer to questions:\n",
    " 1. By which method can we find good Symbolic Defaults? \n",
    " 2. **Can we find good (i.e. better than currently known) symbolic defaults?**\n",
    " \n",
    "This notebook addresses the second question.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After determining good symbolic defaults, we ought to see how they compare to current (scikit-learn) defaults. To this end, we compare three different default configurations (in bold is the name by which they will be referenced henceforth):\n",
    "\n",
    " - The **symbolic_pre** defaults we found from evolutionary optimization, specifically: `C=128, gamma=(mkd / 4)`. \n",
    " This symbolic function uses metafeatures as calculated on the dataset *before* it is preprocessed.\n",
    " - The **symbolic_post** defaults we found from evolutionary optimization, specifically: `C=64, gamma=mkd`.\n",
    "     This symbolic function uses metafeatures as calculated on the dataset *after* it has been preprocessed.\n",
    " - The scikit-learn **0.20** defaults, specifically: `C=1., gamma=(1 / n_features)`\n",
    " - The scikit-learn >= **0.22** defaults, specifically: `C=1., gamma=(1 / (n_features * X.var()))`\n",
    " \n",
    "Note that actually all of these defaults are symbolic.\n",
    "\n",
    "A second important detail to note is that these settings are not tried by themselves.\n",
    "A (fairly standard) preprocessing pipeline is applied:\n",
    " - **Imputation**: using the mean for numeric features, and the most frequent value for categorical features.\n",
    " - **Transformation**: numeric features are scaled to N(0, 1), categorical features are one-hot encoded.\n",
    " - **Feature Selection**: all constant features are removed.\n",
    " \n",
    "After these steps, the SVC is invoked on the preprocessed data with the given values for `C` and `gamma`.\n",
    "\n",
    "Note: for the scikit-learn defaults, currently the metafeatures of the preprocessed data are used (e.g. `n_features` is determined after one-hot encoding, for instance). For the *symbolic* method `mkd` is determined on the original, (largely) unprocessed dataset (samples with NaN values are ignored)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1  Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from persistence import load_problem, load_results_for_problem\n",
    "from visualization.output_parser import get_performance_from_console_output\n",
    "\n",
    "def load_random_search_results(problem_name):\n",
    "    p = load_problem('problems.json', problem_name)\n",
    "    return load_results_for_problem(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The grid search result from Jan.\n",
    "svc_results = load_random_search_results(p)\n",
    "\n",
    "# results currently still stored in log. should be aggregated to single file..\n",
    "# \"data/results/pipeline_c128mkd4.txt\"\n",
    "symb_default_performances = get_performance_from_console_output(\"data/results/pipeline_c128mkd4.txt\")\n",
    "old_default_performances = get_performance_from_console_output(\"data/results/pipeline_default.txt\")\n",
    "new_default_performances = get_performance_from_console_output(\"data/results/pipeline_scale.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Comparing Results\n",
    "We compare results by number of times one's average cross-validation performance is better (first three columns) and by their loss as compared to the best found result in the original set of experiments (last column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "methods = ['Symbolic', '0.20', '0.22']\n",
    "df = pd.DataFrame(np.zeros(shape=(len(methods), len(methods)+1)), columns = methods + ['loss'])\n",
    "df.index = methods\n",
    "\n",
    "# Calculate 'wins'\n",
    "performances = list(zip(methods, [symb_default_performances, old_default_performances, new_default_performances]))\n",
    "for (method, performance) in performances:\n",
    "    for (method2, performance2) in performances:\n",
    "        one_over_two = (performance.avg - performance2.avg) > 0\n",
    "        df.loc[method][method2] = sum(one_over_two)\n",
    "\n",
    "# Calculate loss        \n",
    "for (method, performance) in performances:\n",
    "    loss_sum = 0\n",
    "    for i, row in performance.iterrows():\n",
    "        best_score = svc_results[svc_results.task_id == row.name].predictive_accuracy.max()\n",
    "        loss = best_score - row.avg\n",
    "        if loss < 0:\n",
    "            print('{} outperformed best on task {} by {}'.format(method, task, loss))\n",
    "        loss_sum += loss\n",
    "    df.loc[method]['loss'] = loss_sum\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reads as *Symbolic* won over the *0.20* default 40 times, while the *0.20* default was better than *Symbolic* on 22 tasks. *Symbolic* obtained a loss of 1.612 over the best known result of each task.\n",
    "\n",
    "We see that *Symbolic* (i.e. `C=128, gamma=mkd/4`) as default outperforms either of the two scikit-learn ones, both in terms of tasks where it achieves higher predictive accuracy, and the loss in accuracy it occurs across tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**note**: Everything below is scratchpad and should be ignored\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
