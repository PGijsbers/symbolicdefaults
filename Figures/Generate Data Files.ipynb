{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook generates shared files required for the figures in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# surrogate_performance.csv\n",
    "Contains predicted performance of the generated solutions on the task that was left out during optimization and selection.\n",
    "\n",
    "Columns:\n",
    " - task: the task\n",
    " - learner: the learner the expression is found for (e.g. knn, svm)\n",
    " - expression: the expression for which the score is predicted\n",
    " - score: the normalized score as predicted by the surrogate model for the task\n",
    " - optimizer: the optimizer used for finding the expression, one of\n",
    "     - `Symbolic Default`: obtained with the $\\mu$ + $\\lambda$ symbolic regression including symbolic terminals.\n",
    "     - `Constant Default`: obtained with the $\\mu$ + $\\lambda$ symbolic regression without symbolic terminals.\n",
    "     - `Random Search X`: obtained with random search but otherwise same as `Symbolic Default`\n",
    "     - `Package Default`: the scikit-learn or mlr package default.\n",
    "     - `Optimistic Random Search X`: The best test score on the task among X randomly drawn experiments of **real data**.\n",
    "     \n",
    "Note on the difference of `Random Search` and `Optimistic Random Search`, the `Random Search` is an estimate where random search is employed as optimizer for symbolic expressions. The expression is optimized and selected based on tasks that are *not* the target task. By contrast, `Optimistic Random Search` directly optimizes the configuration on the test task. So `Random Search` finds a *default* whereas `Optimistic Random Search` simulates optimization on the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generated Default Surrogate Scores\n",
    "Results for optimizers `Symbolic Default`, `Constant Default`, and `Random Search X`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the generated defaults ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults_directory = \"../data/generated_defaults\"\n",
    "directory_map = dict(\n",
    "    # dirname = (optimizer, constants_only)\n",
    "    symbolic=(\"mupluslambda\", False), \n",
    "    constants=(\"mupluslambda\", True), \n",
    "    # symbolic=(\"mu_plus_lambda\", False), \n",
    ")\n",
    "\n",
    "generated_defaults = []\n",
    "for dirname, (optimizer, constants) in directory_map.items():\n",
    "    for defaults_file in os.listdir(os.path.join(defaults_directory, dirname)):\n",
    "        if not \"mean_rank\" in defaults_file:\n",
    "            continue\n",
    "\n",
    "        with open(os.path.join(defaults_directory, dirname, defaults_file), \"r\") as fh:\n",
    "            lines = fh.readlines()\n",
    "\n",
    "        for line in lines[1:]:\n",
    "            learner, task, expression = line[:-1].split(',', 2)\n",
    "            generated_defaults.append(dict(\n",
    "                task=task,\n",
    "                learner=learner,\n",
    "                optimizer=optimizer,\n",
    "                constants=constants,\n",
    "                expression=expression[1:-1],  # expression was exported with quotes\n",
    "            ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could recompile the expressions and query the surrogates to obtain the scores. However this is complicated to do for all algorithms in the same script due to some `DEAP` limitations. For that reason we simply look up the recorded test performance from the run files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_directory = \"../run\"\n",
    "run_directories = [\n",
    "    os.path.join(main_directory, subdir, rundir)\n",
    "    for subdir in os.listdir(main_directory) if os.path.isdir(os.path.join(main_directory, subdir))\n",
    "    for rundir in os.listdir(os.path.join(main_directory, subdir))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../run\\\\results_04_02_2021_evening.tar\\\\results_04_02_2021_evening\\\\metadata.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-b0597c4e11c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrun_directory\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrun_directories\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_directory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"metadata.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../run\\\\results_04_02_2021_evening.tar\\\\results_04_02_2021_evening\\\\metadata.csv'"
     ]
    }
   ],
   "source": [
    "runs = []\n",
    "\n",
    "for run_directory in run_directories:\n",
    "    with open(os.path.join(run_directory, \"metadata.csv\"), \"r\") as fh:\n",
    "        lines = fh.readlines()\n",
    "    metadata = dict(line[:-1].split(';') for line in lines[1:])\n",
    "    if metadata['aggregate'] != 'mean':\n",
    "        continue\n",
    "    \n",
    "    optimizer = metadata['algorithm']\n",
    "    constants = (metadata['constants_only'] == 'True')\n",
    "    learner = metadata['problem'][len('mlr_'):]\n",
    "    \n",
    "    for default in generated_defaults:\n",
    "        if 'surrogate_score' in default:\n",
    "            continue\n",
    "        \n",
    "        # run conditions don't matter for the score of the expression on the test set,\n",
    "        # but we can avoid loading a bunch of `final_pareto` files which likely don't have\n",
    "        # the expression we are looking for this way.\n",
    "        different_optimizer = default['optimizer'] != optimizer\n",
    "        different_constant_constraint = default['constants'] != constants\n",
    "        different_learner = default['learner'] != learner\n",
    "        if different_optimizer or different_constant_constraint or different_learner:\n",
    "            continue\n",
    "        \n",
    "        with open(os.path.join(run_directory, \"final_pareto.csv\"), \"r\") as fh:\n",
    "            for line in fh.readlines():\n",
    "                if default[\"expression\"] in line:\n",
    "                    _, _, task, score, *_ = line[:-1].split(';')\n",
    "                    if default[\"task\"] == task:\n",
    "                        default[\"surrogate_score\"] = score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_records = [d for d in generated_defaults if \"surrogate_score\" not in d]\n",
    "print(f\"Missing {len(missing_records)} surrogate performance estimates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_performance = pd.DataFrame.from_dict(generated_defaults, orient='columns')\n",
    "surrogate_performance.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Default Surrogate Scores\n",
    "Results for `Package Default` \"optimizer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "implementation_default_names = [\"sklearn_default\", \"mlr_default\"]\n",
    "implementation_defaults = []\n",
    "\n",
    "# we only need implementation performance for (task, learner) pairs which have a generated default\n",
    "for task, learner in set(zip(surrogate_performance.task, surrogate_performance.learner)):\n",
    "    for name in implementation_default_names:\n",
    "        # defaults only recorded for some problem; the glmnet default is ignored\n",
    "        if learner not in [\"svm\", \"xgboost\"] and name == \"sklearn_default\":\n",
    "            continue\n",
    "        if learner in [\"xgboost\"] and name == \"mlr_default\":\n",
    "            continue\n",
    "        implementation_defaults.append(dict(\n",
    "            task=task,\n",
    "            learner=learner,\n",
    "            optimizer=name,\n",
    "            constants=False,\n",
    "            expression=name,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_directory in run_directories:\n",
    "    with open(os.path.join(run_directory, \"metadata.csv\"), \"r\") as fh:\n",
    "        lines = fh.readlines()\n",
    "    metadata = dict(line[:-1].split(';') for line in lines[1:])\n",
    "    learner = metadata['problem'][len('mlr_'):]\n",
    "    if metadata['aggregate'] != 'mean':\n",
    "        continue\n",
    "    \n",
    "    for default in implementation_defaults:\n",
    "        if 'surrogate_score' in default:\n",
    "            continue\n",
    "        \n",
    "        # Since all runs evaluate defaults regardless of optimization,\n",
    "        # we don't need as strict filtering as above.\n",
    "        if default['learner'] != learner:\n",
    "            continue\n",
    "        \n",
    "        with open(os.path.join(run_directory, \"evaluations.csv\"), \"r\") as fh:\n",
    "            # implementation defaults are reported last\n",
    "            for line in fh.readlines()[-100:]:\n",
    "                if default[\"expression\"] in line:\n",
    "                    _, _, task, _, _, score, *_ = line[:-1].split(';')\n",
    "                    if default[\"task\"] == task:\n",
    "                        default[\"surrogate_score\"] = score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 0 surrogate performance estimates.\n"
     ]
    }
   ],
   "source": [
    "missing_records = [d for d in implementation_defaults if \"surrogate_score\" not in d]\n",
    "print(f\"Missing {len(missing_records)} surrogate performance estimates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>learner</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>constants</th>\n",
       "      <th>expression</th>\n",
       "      <th>surrogate_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>168331</td>\n",
       "      <td>glmnet</td>\n",
       "      <td>mlr_default</td>\n",
       "      <td>False</td>\n",
       "      <td>mlr_default</td>\n",
       "      <td>0.8440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3512</td>\n",
       "      <td>glmnet</td>\n",
       "      <td>mlr_default</td>\n",
       "      <td>False</td>\n",
       "      <td>mlr_default</td>\n",
       "      <td>0.9070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>168910</td>\n",
       "      <td>rpart</td>\n",
       "      <td>mlr_default</td>\n",
       "      <td>False</td>\n",
       "      <td>mlr_default</td>\n",
       "      <td>0.5292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3573</td>\n",
       "      <td>glmnet</td>\n",
       "      <td>mlr_default</td>\n",
       "      <td>False</td>\n",
       "      <td>mlr_default</td>\n",
       "      <td>0.9156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>9950</td>\n",
       "      <td>glmnet</td>\n",
       "      <td>mlr_default</td>\n",
       "      <td>False</td>\n",
       "      <td>mlr_default</td>\n",
       "      <td>0.9612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       task learner    optimizer  constants   expression surrogate_score\n",
       "214  168331  glmnet  mlr_default      False  mlr_default          0.8440\n",
       "97     3512  glmnet  mlr_default      False  mlr_default          0.9070\n",
       "77   168910   rpart  mlr_default      False  mlr_default          0.5292\n",
       "8      3573  glmnet  mlr_default      False  mlr_default          0.9156\n",
       "633    9950  glmnet  mlr_default      False  mlr_default          0.9612"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_performance = pd.DataFrame.from_dict(implementation_defaults, orient='columns')\n",
    "default_performance.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([default_performance, surrogate_performance]).to_csv(\"surrogate_performance.csv\", sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimistic Random Search\n",
    "\n",
    "In contrast to the previous two, these are drawn directly from the real experiment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading glmnet\n",
      "Loading knn\n",
      "Loading rf\n",
      "Loading rpart\n",
      "Loading svm\n",
      "Loading xgboost\n"
     ]
    }
   ],
   "source": [
    "import arff\n",
    "import json\n",
    "\n",
    "learners = surrogate_performance.learner.unique()\n",
    "task_learner_combinations = set(zip(surrogate_performance.task, surrogate_performance.learner))\n",
    "data_for_learner = dict()\n",
    "\n",
    "\n",
    "for learner in learners:\n",
    "    print(f\"Loading {learner}\")\n",
    "    with open(f\"../problems/mlr_{learner}.json\", \"r\") as json_file:\n",
    "        problem_definition = json.load(json_file)\n",
    "    experiment_file = problem_definition[\"experiment_data\"]\n",
    "    \n",
    "    with open(os.path.join(\"..\", experiment_file)) as arff_file:\n",
    "        d = arff.load(arff_file)\n",
    "    columns, dtypes = zip(*d[\"attributes\"])\n",
    "    experiment_data = pd.DataFrame(d[\"data\"], columns=columns)\n",
    "    data_for_learner[learner] = experiment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for task, learner in task_learner_combinations:\n",
    "    experiment_data = data_for_learner[learner]\n",
    "    learner_task_data = experiment_data[experiment_data.task_id == float(task)].copy()\n",
    "    # log loss is the target, and it is minimized\n",
    "    score_column = -learner_task_data[\"perf.logloss\"]\n",
    "    # perform min-max scaling to compare to surrogate predictions\n",
    "    learner_task_data[\"normalized_score\"] = (score_column - min(score_column)) / (max(score_column) - min(score_column))\n",
    "    \n",
    "    for x in [2**i for i in range(1, 6)]:\n",
    "        scores = []\n",
    "        for _ in range(100):\n",
    "            x_sample = learner_task_data.sample(x)\n",
    "            best_result = x_sample[\"normalized_score\"].idxmax()\n",
    "            scores.append(x_sample.loc[best_result][\"normalized_score\"])\n",
    "        \n",
    "        results.append(dict(\n",
    "            task=task,\n",
    "            learner=learner,\n",
    "            optimizer=f\"optimistic_random_search_{x}\",\n",
    "            constants=False,\n",
    "            expression=\"100 replications\",\n",
    "            surrogate_score=sum(scores)/len(scores),\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimistic_random_search = pd.DataFrame.from_dict(results, orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>learner</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>constants</th>\n",
       "      <th>expression</th>\n",
       "      <th>surrogate_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9960</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>optimistic_random_search_2</td>\n",
       "      <td>False</td>\n",
       "      <td>100 replications</td>\n",
       "      <td>0.876472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9960</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>optimistic_random_search_4</td>\n",
       "      <td>False</td>\n",
       "      <td>100 replications</td>\n",
       "      <td>0.941642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9960</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>optimistic_random_search_8</td>\n",
       "      <td>False</td>\n",
       "      <td>100 replications</td>\n",
       "      <td>0.979111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9960</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>optimistic_random_search_16</td>\n",
       "      <td>False</td>\n",
       "      <td>100 replications</td>\n",
       "      <td>0.992509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9960</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>optimistic_random_search_32</td>\n",
       "      <td>False</td>\n",
       "      <td>100 replications</td>\n",
       "      <td>0.996307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3290</th>\n",
       "      <td>168912</td>\n",
       "      <td>svm</td>\n",
       "      <td>optimistic_random_search_2</td>\n",
       "      <td>False</td>\n",
       "      <td>100 replications</td>\n",
       "      <td>0.674328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3291</th>\n",
       "      <td>168912</td>\n",
       "      <td>svm</td>\n",
       "      <td>optimistic_random_search_4</td>\n",
       "      <td>False</td>\n",
       "      <td>100 replications</td>\n",
       "      <td>0.874119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3292</th>\n",
       "      <td>168912</td>\n",
       "      <td>svm</td>\n",
       "      <td>optimistic_random_search_8</td>\n",
       "      <td>False</td>\n",
       "      <td>100 replications</td>\n",
       "      <td>0.939477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3293</th>\n",
       "      <td>168912</td>\n",
       "      <td>svm</td>\n",
       "      <td>optimistic_random_search_16</td>\n",
       "      <td>False</td>\n",
       "      <td>100 replications</td>\n",
       "      <td>0.949916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3294</th>\n",
       "      <td>168912</td>\n",
       "      <td>svm</td>\n",
       "      <td>optimistic_random_search_32</td>\n",
       "      <td>False</td>\n",
       "      <td>100 replications</td>\n",
       "      <td>0.973773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3295 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        task  learner                    optimizer  constants  \\\n",
       "0       9960  xgboost   optimistic_random_search_2      False   \n",
       "1       9960  xgboost   optimistic_random_search_4      False   \n",
       "2       9960  xgboost   optimistic_random_search_8      False   \n",
       "3       9960  xgboost  optimistic_random_search_16      False   \n",
       "4       9960  xgboost  optimistic_random_search_32      False   \n",
       "...      ...      ...                          ...        ...   \n",
       "3290  168912      svm   optimistic_random_search_2      False   \n",
       "3291  168912      svm   optimistic_random_search_4      False   \n",
       "3292  168912      svm   optimistic_random_search_8      False   \n",
       "3293  168912      svm  optimistic_random_search_16      False   \n",
       "3294  168912      svm  optimistic_random_search_32      False   \n",
       "\n",
       "            expression  surrogate_score  \n",
       "0     100 replications         0.876472  \n",
       "1     100 replications         0.941642  \n",
       "2     100 replications         0.979111  \n",
       "3     100 replications         0.992509  \n",
       "4     100 replications         0.996307  \n",
       "...                ...              ...  \n",
       "3290  100 replications         0.674328  \n",
       "3291  100 replications         0.874119  \n",
       "3292  100 replications         0.939477  \n",
       "3293  100 replications         0.949916  \n",
       "3294  100 replications         0.973773  \n",
       "\n",
       "[3295 rows x 6 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimistic_random_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimistic_random_search.to_csv(\"optimistic_random_search.csv\", sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
