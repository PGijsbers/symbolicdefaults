{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook generates shared files required for the figures in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# surrogate_performance.csv\n",
    "Contains predicted performance of the generated solutions on the task that was left out during optimization and selection.\n",
    "\n",
    "Columns:\n",
    " - task: the task\n",
    " - learner: the learner the expression is found for (e.g. knn, svm)\n",
    " - expression: the expression for which the score is predicted\n",
    " - score: the normalized score as predicted by the surrogate model for the task\n",
    " - optimizer: the optimizer used for finding the expression, one of\n",
    "     - `Symbolic Default`: obtained with the $\\mu$ + $\\lambda$ symbolic regression including symbolic terminals.\n",
    "     - `Constant Default`: obtained with the $\\mu$ + $\\lambda$ symbolic regression without symbolic terminals.\n",
    "     - `Random Search X`: obtained with random search but otherwise same as `Symbolic Default`\n",
    "     - `Package Default`: the scikit-learn or mlr package default.\n",
    "     - `Optimistic Random Search X`: The best test score on the task among X randomly drawn expressions.\n",
    "     \n",
    "Note on the difference of `Random Search` and `Optimistic Random Search`, the `Random Search` is an estimate where random search is employed as optimizer for symbolic expressions. The expression is optimized and selected based on tasks that are *not* the target task. By contrast, `Optimistic Random Search` directly optimizes the configuration on the test task. So `Random Search` finds a *default* whereas `Optimistic Random Search` simulates optimization on the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generated Default Surrogate Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the generated defaults ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults_directory = \"../data/generated_defaults\"\n",
    "directory_map = dict(\n",
    "    # dirname = (optimizer, constants_only)\n",
    "    symbolic=(\"mupluslambda\", False), \n",
    "    constants=(\"mupluslambda\", True), \n",
    "    # symbolic=(\"mu_plus_lambda\", False), \n",
    ")\n",
    "\n",
    "generated_defaults = []\n",
    "for dirname, (optimizer, constants) in directory_map.items():\n",
    "    for defaults_file in os.listdir(os.path.join(defaults_directory, dirname)):\n",
    "        if not \"mean_rank\" in defaults_file:\n",
    "            continue\n",
    "\n",
    "        with open(os.path.join(defaults_directory, dirname, defaults_file), \"r\") as fh:\n",
    "            lines = fh.readlines()\n",
    "\n",
    "        for line in lines[1:]:\n",
    "            learner, task, expression = line[:-1].split(',', 2)\n",
    "            generated_defaults.append(dict(\n",
    "                task=task,\n",
    "                learner=learner,\n",
    "                optimizer=optimizer,\n",
    "                constants=constants,\n",
    "                expression=expression[1:-1],  # expression was exported with quotes\n",
    "            ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could recompile the expressions and query the surrogates to obtain the scores. However this is complicated to do for all algorithms in the same script due to some `DEAP` limitations. For that reason we simply look up the recorded test performance from the run files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_directory = \"../run\"\n",
    "run_directories = [\n",
    "    os.path.join(main_directory, subdir, rundir)\n",
    "    for subdir in os.listdir(main_directory) if os.path.isdir(os.path.join(main_directory, subdir))\n",
    "    for rundir in os.listdir(os.path.join(main_directory, subdir))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = []\n",
    "\n",
    "for run_directory in run_directories:\n",
    "    with open(os.path.join(run_directory, \"metadata.csv\"), \"r\") as fh:\n",
    "        lines = fh.readlines()\n",
    "    metadata = dict(line[:-1].split(';') for line in lines[1:])\n",
    "    if metadata['aggregate'] != 'mean':\n",
    "        continue\n",
    "    \n",
    "    optimizer = metadata['algorithm']\n",
    "    constants = (metadata['constants_only'] == 'True')\n",
    "    learner = metadata['problem'][len('mlr_'):]\n",
    "    \n",
    "    for default in generated_defaults:\n",
    "        if 'surrogate_score' in default:\n",
    "            continue\n",
    "        \n",
    "        # run conditions don't matter for the score of the expression on the test set,\n",
    "        # but we can avoid loading a bunch of `final_pareto` files which likely don't have\n",
    "        # the expression we are looking for this way.\n",
    "        different_optimizer = default['optimizer'] != optimizer\n",
    "        different_constant_constraint = default['constants'] != constants\n",
    "        different_learner = default['learner'] != learner\n",
    "        if different_optimizer or different_constant_constraint or different_learner:\n",
    "            continue\n",
    "        \n",
    "        with open(os.path.join(run_directory, \"final_pareto.csv\"), \"r\") as fh:\n",
    "            for line in fh.readlines():\n",
    "                if default[\"expression\"] in line:\n",
    "                    _, _, task, score, *_ = line[:-1].split(';')\n",
    "                    if default[\"task\"] == task:\n",
    "                        default[\"surrogate_score\"] = score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 0 surrogate performance estimates.\n"
     ]
    }
   ],
   "source": [
    "missing_records = [d for d in generated_defaults if \"surrogate_score\" not in d]\n",
    "print(f\"Missing {len(missing_records)} surrogate performance estimates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>learner</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>constants</th>\n",
       "      <th>expression</th>\n",
       "      <th>surrogate_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>145681</td>\n",
       "      <td>rpart</td>\n",
       "      <td>mupluslambda</td>\n",
       "      <td>True</td>\n",
       "      <td>make_tuple(truediv(0.001027825260946386, 2), 9...</td>\n",
       "      <td>0.8858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>3021</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>mupluslambda</td>\n",
       "      <td>True</td>\n",
       "      <td>make_tuple(332, 0.19258991025238412, 7, 0.0021...</td>\n",
       "      <td>0.9988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>3917</td>\n",
       "      <td>knn</td>\n",
       "      <td>mupluslambda</td>\n",
       "      <td>True</td>\n",
       "      <td>make_tuple(add(35, 0.010976039052383521), 53, ...</td>\n",
       "      <td>0.9906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>9956</td>\n",
       "      <td>knn</td>\n",
       "      <td>mupluslambda</td>\n",
       "      <td>False</td>\n",
       "      <td>make_tuple(max(mul(if_gt(9, po, 8, mul(xvar, x...</td>\n",
       "      <td>0.9780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>168911</td>\n",
       "      <td>svm</td>\n",
       "      <td>mupluslambda</td>\n",
       "      <td>False</td>\n",
       "      <td>make_tuple(mul(if_gt(0.5109620985849945, mkd, ...</td>\n",
       "      <td>0.9446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        task  learner     optimizer  constants  \\\n",
       "896   145681    rpart  mupluslambda       True   \n",
       "1159    3021  xgboost  mupluslambda       True   \n",
       "809     3917      knn  mupluslambda       True   \n",
       "172     9956      knn  mupluslambda      False   \n",
       "458   168911      svm  mupluslambda      False   \n",
       "\n",
       "                                             expression surrogate_score  \n",
       "896   make_tuple(truediv(0.001027825260946386, 2), 9...          0.8858  \n",
       "1159  make_tuple(332, 0.19258991025238412, 7, 0.0021...          0.9988  \n",
       "809   make_tuple(add(35, 0.010976039052383521), 53, ...          0.9906  \n",
       "172   make_tuple(max(mul(if_gt(9, po, 8, mul(xvar, x...          0.9780  \n",
       "458   make_tuple(mul(if_gt(0.5109620985849945, mkd, ...          0.9446  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surrogate_performance = pd.DataFrame.from_dict(generated_defaults, orient='columns')\n",
    "surrogate_performance.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Default Surrogate Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "implementation_default_names = [\"sklearn_default\", \"mlr_default\"]\n",
    "implementation_defaults = []\n",
    "\n",
    "# we only need implementation performance for (task, learner) pairs which have a generated default\n",
    "for task, learner in set(zip(surrogate_performance.task, surrogate_performance.learner)):\n",
    "    for name in implementation_default_names:\n",
    "        # defaults only recorded for some problems\n",
    "        if learner not in [\"svm\", \"glmnet\", \"xgboost\"] and name == \"sklearn_default\":\n",
    "            continue\n",
    "        if learner in [\"xgboost\"] and name == \"mlr_default\":\n",
    "            continue\n",
    "        implementation_defaults.append(dict(\n",
    "            task=task,\n",
    "            learner=learner,\n",
    "            optimizer=name,\n",
    "            constants=False,\n",
    "            expression=name,\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run_directory in run_directories:\n",
    "    with open(os.path.join(run_directory, \"metadata.csv\"), \"r\") as fh:\n",
    "        lines = fh.readlines()\n",
    "    metadata = dict(line[:-1].split(';') for line in lines[1:])\n",
    "    learner = metadata['problem'][len('mlr_'):]\n",
    "    if metadata['aggregate'] != 'mean':\n",
    "        continue\n",
    "    \n",
    "    for default in implementation_defaults:\n",
    "        if 'surrogate_score' in default:\n",
    "            continue\n",
    "        \n",
    "        # Since all runs evaluate defaults regardless of optimization,\n",
    "        # we don't need as strict filtering as above.\n",
    "        if default['learner'] != learner:\n",
    "            continue\n",
    "        \n",
    "        with open(os.path.join(run_directory, \"evaluations.csv\"), \"r\") as fh:\n",
    "            # implementation defaults are reported last\n",
    "            for line in fh.readlines()[-100:]:\n",
    "                if default[\"expression\"] in line:\n",
    "                    _, _, task, _, _, score, *_ = line[:-1].split(';')\n",
    "                    if default[\"task\"] == task:\n",
    "                        default[\"surrogate_score\"] = score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing 0 surrogate performance estimates.\n"
     ]
    }
   ],
   "source": [
    "missing_records = [d for d in implementation_defaults if \"surrogate_score\" not in d]\n",
    "print(f\"Missing {len(missing_records)} surrogate performance estimates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task</th>\n",
       "      <th>learner</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>constants</th>\n",
       "      <th>expression</th>\n",
       "      <th>surrogate_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>14970</td>\n",
       "      <td>svm</td>\n",
       "      <td>sklearn_default</td>\n",
       "      <td>False</td>\n",
       "      <td>sklearn_default</td>\n",
       "      <td>0.9798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>189924</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>sklearn_default</td>\n",
       "      <td>False</td>\n",
       "      <td>sklearn_default</td>\n",
       "      <td>0.9919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>2074</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>sklearn_default</td>\n",
       "      <td>False</td>\n",
       "      <td>sklearn_default</td>\n",
       "      <td>0.9744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>167211</td>\n",
       "      <td>rf</td>\n",
       "      <td>mlr_default</td>\n",
       "      <td>False</td>\n",
       "      <td>mlr_default</td>\n",
       "      <td>0.9653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>168338</td>\n",
       "      <td>glmnet</td>\n",
       "      <td>mlr_default</td>\n",
       "      <td>False</td>\n",
       "      <td>mlr_default</td>\n",
       "      <td>0.4382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       task  learner        optimizer  constants       expression  \\\n",
       "810   14970      svm  sklearn_default      False  sklearn_default   \n",
       "379  189924  xgboost  sklearn_default      False  sklearn_default   \n",
       "177    2074  xgboost  sklearn_default      False  sklearn_default   \n",
       "753  167211       rf      mlr_default      False      mlr_default   \n",
       "100  168338   glmnet      mlr_default      False      mlr_default   \n",
       "\n",
       "    surrogate_score  \n",
       "810          0.9798  \n",
       "379          0.9919  \n",
       "177          0.9744  \n",
       "753          0.9653  \n",
       "100          0.4382  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_performance = pd.DataFrame.from_dict(implementation_defaults, orient='columns')\n",
    "default_performance.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([default_performance, surrogate_performance]).to_csv(\"surrogate_performance.csv\", sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
