{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolic Defaults by 'complexity' of expression\n",
    "In this notebook we take a look at the results of running the script at its default settings, this means:\n",
    " - evaluation across all tasks\n",
    " - recording the pareto front of symbolic defaults after each search\n",
    " - evaluating in-sample and out-of-sample performance of those dynamic defaults, as well as some pre-defined ones\n",
    " \n",
    "**note:** The console cut off results for the first few tasks, so I am rerunning those now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('runs/fullpareto.txt') as fh:\n",
    "    lines = fh.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predefined defaults are:\n",
      " * sklearn_scale := make_tuple(1., truediv(1., mul(p, xvar)))\n",
      " * symbolic_best := make_tuple(16., truediv(mkd, xvar))\n",
      " * symbolic_v2 := make_tuple(m, add(mkd, mkd)))\n",
      " * const := make_tuple(812.267350, 0.001361)\n"
     ]
    }
   ],
   "source": [
    "# saved because it's useful later, moreso than now.\n",
    "first_definition = [i for i, line in enumerate(lines) if ':=' in line][0]\n",
    "\n",
    "print(\"The predefined defaults are:\")\n",
    "for line in lines[first_definition:]:\n",
    "    if ':=' in line:\n",
    "        print(f\" * {line[len('INFO:root:'):-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = [\"const\", \"sklearn_scale\", \"symbolic_best\", \"symbolic_v2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_starts = [i for i, line in enumerate(lines) if \"INFO:root:START_TASK:\" in line]\n",
    "in_sample_starts = [i for i, line in enumerate(lines) if \"INFO:root:Evaluating in sample:\" in line]\n",
    "out_sample_starts = [i for i, line in enumerate(lines) if \"INFO:root:Evaluating out-of-sample:\" in line]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each task we will extract:\n",
    " - the number of generations optimization ran for (max=200)\n",
    " - max length expression\n",
    " - in and out of sample performance for length 1, 2 and 3 expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_evaluation_line(line) -> Tuple[str, int, float]:\n",
    "    \"\"\" Parse an evaluation line, returning the expression or name, its 'length' and the score.\n",
    "    \n",
    "    e.g. INFO:root:[make_tuple(p, mkd)|0.8893]\\n -> 'make_tuple(p, mkd)', 1, 0.8893 \n",
    "    Length is 0 for benchmark problems.\n",
    "    \"\"\"\n",
    "    start, pipe, end = line.find('['), line.find('|'), line.find(']')\n",
    "    expression = line[start + 1 : pipe]\n",
    "    expression_length = expression.count('(')\n",
    "    return expression, expression_length, float(line[pipe + 1 : end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each task save the benchmark results. We also save results for length 1, 2 and 3 solutions as well as the best one found (that may be longer). Specifically we record:\n",
    " - best in_sample performance at length 1, 2, 3\n",
    " - best in_sample performance for any length\n",
    " - average out_sample performance by length for length 1, 2, 3\n",
    " - average out_sample performance for the longest (i.e. best in-sample score) solution(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [int(line[:-1].split(\": \")[-1]) for line in lines if \"INFO:root:START_TASK:\" in line]\n",
    "idx = pd.MultiIndex.from_product([tasks, [\"in-sample\", \"out-sample\"]], names=['task', 'sample-type'])\n",
    "df = pd.DataFrame(index=idx, columns=[\"score-1\",\"score-2\", \"score-3\", \"score-best\", *benchmarks], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "expressions_by_length = defaultdict(list)\n",
    "generations_by_task = {}\n",
    "\n",
    "for task_start, in_start, out_start, next_task in zip(task_starts, in_sample_starts, out_sample_starts, task_starts[1:] + [first_definition]):\n",
    "    # start line looks like: INFO:root:START_TASK: 29\\n\n",
    "    task = int(lines[task_start][:-1].split(\": \")[-1])\n",
    "    \n",
    "    # Since the in-sample evaluation message follows directly after optimization is done, we use that to record\n",
    "    # the number of generations. We account for the early stopping message if it did not run to 200 generations.\n",
    "    ended_early = 0 if in_start - task_start == 201 else - 1\n",
    "    generations_by_task[task] = in_start - (task_start + 1) - ended_early\n",
    "    \n",
    "    # Following the \"INFO:root:Evaluating in sample:\" message, symbolic default performance are printed\n",
    "    # They are formatted as \"INFO:root:[make_tuple(p, mkd)|0.8893]\"\n",
    "    # First is any number of best solutions from the pareto front. The last four are benchmark solutions.\n",
    "    # It is possible that two equally good solutions are printed (i.e. same length and performance).    \n",
    "    expr_in_task = set()\n",
    "    \n",
    "#     in_sample_evaluation_lines = lines[in_start + 1 : out_start]\n",
    "#     in_sample_evaluations = [parse_evaluation_line(eval_) for eval_ in in_sample_evaluation_lines]\n",
    "#     max_len = max([len_ for _, len_, _ in in_sample_evaluations])\n",
    "#     expr_in_task = {expr for expr, _, _ in in_sample_evaluations}\n",
    "    max_length = 0\n",
    "    \n",
    "    for in_sample_evaluation in lines[in_start + 1 : out_start]:\n",
    "        expr, length, score = parse_evaluation_line(in_sample_evaluation)\n",
    "        # Pareto fronts may contain literal duplicates, so we filter those out manually.\n",
    "        if expr not in expr_in_task:\n",
    "            expressions_by_length[length].append(expr)\n",
    "            expr_in_task.add(expr)\n",
    "        \n",
    "        if length !=0:\n",
    "            if length < 4:\n",
    "                # Only report one out-of-sample solution for each length (and all benchmarks), so overwrite is OK.\n",
    "                df.loc[task, \"in-sample\"][f\"score-{length}\"] = score\n",
    "                \n",
    "            # Update best so far score and maximum length\n",
    "            df.loc[task, \"in-sample\"][f\"score-best\"] = np.nanmax([score, df.loc[task, \"in-sample\"][f\"score-best\"]])\n",
    "            max_length = max(max_length, length)\n",
    "        else:\n",
    "            df.loc[task, \"in-sample\"][expr] = score\n",
    "            \n",
    "        if length > max_length:\n",
    "            max_length = length  # To know for which length \"best\" should score out of sample\n",
    "    \n",
    "    # Because two equal solutions can be in the Pareto front, \n",
    "    # we note the average out of sample performance if multiple solutions were found.\n",
    "    # Naturally, the solutions with the best in-sample score were those with the highest length in the Pareto front.\n",
    "    \n",
    "    scores_by_length = defaultdict(list)\n",
    "    \n",
    "    for out_sample_evaluation in lines[out_start + 1 : next_task]:\n",
    "        expr, length, score = parse_evaluation_line(out_sample_evaluation)   \n",
    "        if length !=0:\n",
    "            scores_by_length[length].append(score)\n",
    "        else:\n",
    "            df.loc[task, \"out-sample\"][expr] = score\n",
    "            \n",
    "    for length, scores in scores_by_length.items():\n",
    "        if length < 4:\n",
    "            df.loc[task, \"out-sample\"][f\"score-{length}\"] = np.mean(scores)\n",
    "        if length == max_length:\n",
    "            df.loc[task, \"out-sample\"][f\"score-best\"] = np.mean(scores)\n",
    "        if np.mean(scores) == float(\"nan\"):\n",
    "            print('hi')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 424 expressions of length 0. Most frequent: sklearn_scale (106 times)\n",
      " Found 106 expressions of length 1. Most frequent: make_tuple(p, mkd) (106 times)\n",
      " Found 114 expressions of length 2. Most frequent: make_tuple(m, truediv(mkd, xvar)) (57 times)\n",
      " Found  68 expressions of length 3. Most frequent: make_tuple(truediv(m, mcp), truediv(mkd, xvar)) (6 times)\n",
      " Found  31 expressions of length 4. Most frequent: make_tuple(add(min(p, m), xvar), truediv(mkd, xvar)) (1 times)\n",
      " Found  17 expressions of length 5. Most frequent: make_tuple(mul(truediv(m, xvar), truediv(truediv(m, xvar), xvar)), mkd) (1 times)\n",
      " Found  12 expressions of length 6. Most frequent: make_tuple(sub(mul(truediv(m, xvar), truediv(truediv(m, xvar), xvar)), xvar), mkd) (1 times)\n",
      " Found   6 expressions of length 7. Most frequent: make_tuple(m, truediv(mkd, mul(xvar, min(max(mul(p, mkd), pow(0.125, xvar)), 2.0)))) (1 times)\n",
      " Found   1 expressions of length 8. Most frequent: make_tuple(truediv(truediv(if_gt(mkd, add(0.059241847598554886, rc), xvar, mul(0.5683767691931638, 32.0)), min(mcp, xvar)), pow(xvar, rc)), mkd) (1 times)\n",
      " Found   1 expressions of length 9. Most frequent: make_tuple(truediv(truediv(if_gt(mkd, add(0.059241847598554886, rc), xvar, mul(0.5683767691931638, 32.0)), min(mcp, xvar)), pow(min(xvar, 0.721305195389539), rc)), mkd) (1 times)\n"
     ]
    }
   ],
   "source": [
    "for length, expressions in sorted(expressions_by_length.items()):\n",
    "    m = max(set(expressions), key=expressions.count)\n",
    "    print(f\" Found {len(expressions):3d} expressions of length {length}. Most frequent: {m} ({expressions.count(m)} times)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Found `N` expressions of length `L`\" here means across all the tasks' pareto fronts `N` solutions have length `L`.\n",
    "Pareto fronts may contain duplicate entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1e88ab57588>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(generations_by_task, name=\"generations\").hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above plot shows the histogram of the number of generations across tasks (binsize=10).\n",
    "Note that if something ran for less than 200 generations, it found its optimum 20 generations earlier and early stopping terminated search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>score-1</th>\n",
       "      <th>score-2</th>\n",
       "      <th>score-3</th>\n",
       "      <th>score-best</th>\n",
       "      <th>const</th>\n",
       "      <th>sklearn_scale</th>\n",
       "      <th>symbolic_best</th>\n",
       "      <th>symbolic_v2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th>sample-type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>in-sample</th>\n",
       "      <td>0.8896</td>\n",
       "      <td>0.9018</td>\n",
       "      <td>0.9040</td>\n",
       "      <td>0.9040</td>\n",
       "      <td>0.8608</td>\n",
       "      <td>0.8870</td>\n",
       "      <td>0.8949</td>\n",
       "      <td>0.8855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.9641</td>\n",
       "      <td>0.9132</td>\n",
       "      <td>0.9146</td>\n",
       "      <td>0.9146</td>\n",
       "      <td>0.9420</td>\n",
       "      <td>0.9193</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.8774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">6</th>\n",
       "      <th>in-sample</th>\n",
       "      <td>0.8894</td>\n",
       "      <td>0.8998</td>\n",
       "      <td>0.9014</td>\n",
       "      <td>0.9014</td>\n",
       "      <td>0.8609</td>\n",
       "      <td>0.8865</td>\n",
       "      <td>0.8948</td>\n",
       "      <td>0.8843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.9893</td>\n",
       "      <td>0.9912</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>0.9355</td>\n",
       "      <td>0.9681</td>\n",
       "      <td>0.9893</td>\n",
       "      <td>0.9958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <th>in-sample</th>\n",
       "      <td>0.8902</td>\n",
       "      <td>0.9020</td>\n",
       "      <td>0.9046</td>\n",
       "      <td>0.9047</td>\n",
       "      <td>0.8612</td>\n",
       "      <td>0.8873</td>\n",
       "      <td>0.8951</td>\n",
       "      <td>0.8852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189928</th>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.9249</td>\n",
       "      <td>0.9397</td>\n",
       "      <td>0.9423</td>\n",
       "      <td>0.9423</td>\n",
       "      <td>0.8361</td>\n",
       "      <td>0.8853</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.8864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">190411</th>\n",
       "      <th>in-sample</th>\n",
       "      <td>0.8902</td>\n",
       "      <td>0.8903</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8903</td>\n",
       "      <td>0.8613</td>\n",
       "      <td>0.8874</td>\n",
       "      <td>0.8954</td>\n",
       "      <td>0.8854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.9041</td>\n",
       "      <td>0.9041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9041</td>\n",
       "      <td>0.8884</td>\n",
       "      <td>0.8788</td>\n",
       "      <td>0.9301</td>\n",
       "      <td>0.8811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">190412</th>\n",
       "      <th>in-sample</th>\n",
       "      <td>0.8915</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>0.8674</td>\n",
       "      <td>0.8914</td>\n",
       "      <td>0.8972</td>\n",
       "      <td>0.8883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.7625</td>\n",
       "      <td>0.7375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7375</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.4536</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>0.5789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    score-1  score-2  score-3  score-best   const  \\\n",
       "task   sample-type                                                  \n",
       "3      in-sample     0.8896   0.9018   0.9040      0.9040  0.8608   \n",
       "       out-sample    0.9641   0.9132   0.9146      0.9146  0.9420   \n",
       "6      in-sample     0.8894   0.8998   0.9014      0.9014  0.8609   \n",
       "       out-sample    0.9893   0.9912   0.9917      0.9917  0.9355   \n",
       "11     in-sample     0.8902   0.9020   0.9046      0.9047  0.8612   \n",
       "...                     ...      ...      ...         ...     ...   \n",
       "189928 out-sample    0.9249   0.9397   0.9423      0.9423  0.8361   \n",
       "190411 in-sample     0.8902   0.8903      NaN      0.8903  0.8613   \n",
       "       out-sample    0.9041   0.9041      NaN      0.9041  0.8884   \n",
       "190412 in-sample     0.8915   0.8929      NaN      0.8929  0.8674   \n",
       "       out-sample    0.7625   0.7375      NaN      0.7375  0.2500   \n",
       "\n",
       "                    sklearn_scale  symbolic_best  symbolic_v2  \n",
       "task   sample-type                                             \n",
       "3      in-sample           0.8870         0.8949       0.8855  \n",
       "       out-sample          0.9193         0.9816       0.8774  \n",
       "6      in-sample           0.8865         0.8948       0.8843  \n",
       "       out-sample          0.9681         0.9893       0.9958  \n",
       "11     in-sample           0.8873         0.8951       0.8852  \n",
       "...                           ...            ...          ...  \n",
       "189928 out-sample          0.8853         0.9782       0.8864  \n",
       "190411 in-sample           0.8874         0.8954       0.8854  \n",
       "       out-sample          0.8788         0.9301       0.8811  \n",
       "190412 in-sample           0.8914         0.8972       0.8883  \n",
       "       out-sample          0.4536         0.7339       0.5789  \n",
       "\n",
       "[212 rows x 8 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>score-1</th>\n",
       "      <th>score-2</th>\n",
       "      <th>score-3</th>\n",
       "      <th>score-best</th>\n",
       "      <th>const</th>\n",
       "      <th>sklearn_scale</th>\n",
       "      <th>symbolic_best</th>\n",
       "      <th>symbolic_v2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th>sample-type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.9641</td>\n",
       "      <td>0.9132</td>\n",
       "      <td>0.9146</td>\n",
       "      <td>0.9146</td>\n",
       "      <td>0.9420</td>\n",
       "      <td>0.9193</td>\n",
       "      <td>0.9816</td>\n",
       "      <td>0.8774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.9893</td>\n",
       "      <td>0.9912</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>0.9355</td>\n",
       "      <td>0.9681</td>\n",
       "      <td>0.9893</td>\n",
       "      <td>0.9958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.9035</td>\n",
       "      <td>0.8982</td>\n",
       "      <td>0.9025</td>\n",
       "      <td>0.9029</td>\n",
       "      <td>0.8968</td>\n",
       "      <td>0.8807</td>\n",
       "      <td>0.9531</td>\n",
       "      <td>0.9050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.9990</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>0.9983</td>\n",
       "      <td>0.9925</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.9958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.9647</td>\n",
       "      <td>0.9958</td>\n",
       "      <td>0.9867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189924</th>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.6658</td>\n",
       "      <td>0.6658</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.6658</td>\n",
       "      <td>0.5543</td>\n",
       "      <td>0.7663</td>\n",
       "      <td>0.9862</td>\n",
       "      <td>0.5274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189927</th>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.9457</td>\n",
       "      <td>0.9695</td>\n",
       "      <td>0.9478</td>\n",
       "      <td>0.9478</td>\n",
       "      <td>0.9355</td>\n",
       "      <td>0.9692</td>\n",
       "      <td>0.9695</td>\n",
       "      <td>0.8911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189928</th>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.9249</td>\n",
       "      <td>0.9397</td>\n",
       "      <td>0.9423</td>\n",
       "      <td>0.9423</td>\n",
       "      <td>0.8361</td>\n",
       "      <td>0.8853</td>\n",
       "      <td>0.9782</td>\n",
       "      <td>0.8864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190411</th>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.9041</td>\n",
       "      <td>0.9041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9041</td>\n",
       "      <td>0.8884</td>\n",
       "      <td>0.8788</td>\n",
       "      <td>0.9301</td>\n",
       "      <td>0.8811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190412</th>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.7625</td>\n",
       "      <td>0.7375</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7375</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.4536</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>0.5789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    score-1  score-2  score-3  score-best   const  \\\n",
       "task   sample-type                                                  \n",
       "3      out-sample    0.9641   0.9132   0.9146      0.9146  0.9420   \n",
       "6      out-sample    0.9893   0.9912   0.9917      0.9917  0.9355   \n",
       "11     out-sample    0.9035   0.8982   0.9025      0.9029  0.8968   \n",
       "12     out-sample    0.9990   0.9988      NaN      0.9988  0.9983   \n",
       "14     out-sample    0.9960   0.9944      NaN      0.9944  0.9926   \n",
       "...                     ...      ...      ...         ...     ...   \n",
       "189924 out-sample    0.6658   0.6658      NaN      0.6658  0.5543   \n",
       "189927 out-sample    0.9457   0.9695   0.9478      0.9478  0.9355   \n",
       "189928 out-sample    0.9249   0.9397   0.9423      0.9423  0.8361   \n",
       "190411 out-sample    0.9041   0.9041      NaN      0.9041  0.8884   \n",
       "190412 out-sample    0.7625   0.7375      NaN      0.7375  0.2500   \n",
       "\n",
       "                    sklearn_scale  symbolic_best  symbolic_v2  \n",
       "task   sample-type                                             \n",
       "3      out-sample          0.9193         0.9816       0.8774  \n",
       "6      out-sample          0.9681         0.9893       0.9958  \n",
       "11     out-sample          0.8807         0.9531       0.9050  \n",
       "12     out-sample          0.9925         0.9985       0.9958  \n",
       "14     out-sample          0.9647         0.9958       0.9867  \n",
       "...                           ...            ...          ...  \n",
       "189924 out-sample          0.7663         0.9862       0.5274  \n",
       "189927 out-sample          0.9692         0.9695       0.8911  \n",
       "189928 out-sample          0.8853         0.9782       0.8864  \n",
       "190411 out-sample          0.8788         0.9301       0.8811  \n",
       "190412 out-sample          0.4536         0.7339       0.5789  \n",
       "\n",
       "[106 rows x 8 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_sample = df.index.map(lambda idx: idx[1] == \"out-sample\")\n",
    "df.loc[out_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the scores and note which solution leads to the best out of sample score per task. A solution wins **alone** if all other solutions have worse performance. It wins **shared** if at least one other solution has the same score, but no solution has a better score. **either** is the sum of alone and shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alone = defaultdict(int)\n",
    "shared = defaultdict(int)\n",
    "for _, out in df.loc[out_sample].iterrows():\n",
    "    best = out[out == out.max()].index.values\n",
    "    if len(best) == 1:\n",
    "        alone[best[0]] += 1\n",
    "    else:\n",
    "        for winner in best:\n",
    "            shared[winner] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>score-1</th>\n",
       "      <th>score-2</th>\n",
       "      <th>score-3</th>\n",
       "      <th>score-best</th>\n",
       "      <th>sklearn_scale</th>\n",
       "      <th>symbolic_best</th>\n",
       "      <th>symbolic_v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alone</th>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shared</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>either</th>\n",
       "      <td>14</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        const  score-1  score-2  score-3  score-best  sklearn_scale  \\\n",
       "alone      11       22        1        1           1             14   \n",
       "shared      3        8       16       11          18              2   \n",
       "either     14       30       17       12          19             16   \n",
       "\n",
       "        symbolic_best  symbolic_v2  \n",
       "alone              16           16  \n",
       "shared              8            3  \n",
       "either             24           19  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alone = {k: alone[k] for k in sorted(alone)}\n",
    "shared = {k: shared[k] for k in sorted(shared)}\n",
    "either = {k: shared[k] + alone[k] for k in sorted(shared)}\n",
    "pd.DataFrame([alone, shared, either], index=['alone', 'shared', 'either'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But tallying wins does not say much about the robustness of the symbolic defaults. We can also compare the average or median distance from the top performer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df.loc[out_sample].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out['max'] = df_out.max(axis=1)\n",
    "for col in df_out:\n",
    "    df_out['d_'+col] = df_out['max'] - df_out[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d_score-1          0.043785\n",
       "d_score-2          0.042344\n",
       "d_score-3          0.029300\n",
       "d_score-best       0.042664\n",
       "d_const            0.072528\n",
       "d_sklearn_scale    0.046802\n",
       "d_symbolic_best    0.038395\n",
       "d_symbolic_v2      0.048700\n",
       "d_max              0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_cols = [c for c in df_out.columns if c.startswith('d_')]\n",
    "df_out[d_cols].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d_score-1          0.01520\n",
       "d_score-2          0.01325\n",
       "d_score-3          0.00850\n",
       "d_score-best       0.01155\n",
       "d_const            0.02750\n",
       "d_sklearn_scale    0.01770\n",
       "d_symbolic_best    0.00585\n",
       "d_symbolic_v2      0.01125\n",
       "d_max              0.00000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out[d_cols].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at in-sample performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score-2          37\n",
       "score-3          28\n",
       "score-best       26\n",
       "symbolic_best    15\n",
       "dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_sample = df.index.map(lambda idx: idx[1] == \"in-sample\")\n",
    "df.loc[in_sample].idxmax(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`idxmax` reports the first column that has a max score of the row. So we see that never is the \"score-1\" solution the best in-sample. It is divided somewhat evenly between score-2, score-3 and scores for greater lengths. In 15 of 106 cases, it does not find the solution \"symbolic best\", which would have had better in-sample performance for that task (for other tasks we don't know if it was considered)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reversing column order, we confirms there are no ties between any found solutions and benchmark ones (in-sample):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score-best       91\n",
       "symbolic_best    15\n",
       "dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[in_sample][reversed(df.columns)].idxmax(axis=1).value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
