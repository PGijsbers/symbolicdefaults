{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolic Defaults by 'complexity' of expression\n",
    "In this notebook we take a look at the results of running the script at its default settings, this means:\n",
    " - evaluation across all tasks\n",
    " - recording the pareto front of symbolic defaults after each search\n",
    " - evaluating in-sample and out-of-sample performance of those dynamic defaults, as well as some pre-defined ones\n",
    " \n",
    "**note:** The console cut off results for the first few tasks, so I am rerunning those now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('runs/knn.log') as fh:\n",
    "    lines = fh.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predefined defaults are:\n",
      " * t := make_tuple(1., 16., 10., 200.)\n"
     ]
    }
   ],
   "source": [
    "# saved because it's useful later, moreso than now.\n",
    "first_definition = [i for i, line in enumerate(lines) if ':=' in line][0]\n",
    "\n",
    "print(\"The predefined defaults are:\")\n",
    "for line in lines[first_definition:]:\n",
    "    if ':=' in line:\n",
    "        print(f\" * {line[len('INFO:root:'):-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = [\"const\", \"sklearn_scale\", \"symbolic_best\", \"symbolic_v2\"]\n",
    "benchmarks = [\"mlr_default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_starts = [i for i, line in enumerate(lines) if \"INFO:root:START_TASK:\" in line]\n",
    "in_sample_starts = [i for i, line in enumerate(lines) if \"INFO:root:Evaluating in sample:\" in line]\n",
    "out_sample_starts = [i for i, line in enumerate(lines) if \"INFO:root:Evaluating out-of-sample:\" in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_starts = [i for i, line in enumerate(lines) if \"START_TASK:\" in line]\n",
    "in_sample_starts = [i for i, line in enumerate(lines) if \"Evaluating in sample:\" in line]\n",
    "out_sample_starts = [i for i, line in enumerate(lines) if \"Evaluating out-of-sample:\" in line]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each task we will extract:\n",
    " - the number of generations optimization ran for (max=200)\n",
    " - max length expression\n",
    " - in and out of sample performance for length 1, 2 and 3 expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_evaluation_line(line) -> Tuple[str, int, float]:\n",
    "    \"\"\" Parse an evaluation line, returning the expression or name, its 'length' and the score.\n",
    "    \n",
    "    e.g. INFO:root:[make_tuple(p, mkd)|0.8893]\\n -> 'make_tuple(p, mkd)', 1, 0.8893 \n",
    "    Length is 0 for benchmark problems.\n",
    "    \"\"\"\n",
    "    start, pipe, end = line.find('['), line.find('|'), line.find(']')\n",
    "    expression = line[start + 1 : pipe]\n",
    "    expression_length = expression.count('(')\n",
    "    return expression, expression_length, float(line[pipe + 1 : end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each task save the benchmark results. We also save results for length 1, 2 and 3 solutions as well as the best one found (that may be longer). Specifically we record:\n",
    " - best in_sample performance at length 1, 2, 3\n",
    " - best in_sample performance for any length\n",
    " - average out_sample performance by length for length 1, 2, 3\n",
    " - average out_sample performance for the longest (i.e. best in-sample score) solution(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [int(line[:-1].split(\": \")[-1]) for line in lines if \"START_TASK:\" in line]\n",
    "idx = pd.MultiIndex.from_product([tasks, [\"in-sample\", \"out-sample\"]], names=['task', 'sample-type'])\n",
    "df = pd.DataFrame(index=idx, columns=[\"score-1\",\"score-2\", \"score-3\", \"score-best\", *benchmarks], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "expressions_by_length = defaultdict(list)\n",
    "generations_by_task = {}\n",
    "\n",
    "for task_start, in_start, out_start, next_task in zip(task_starts, in_sample_starts, out_sample_starts, task_starts[1:] + [first_definition]):\n",
    "    # start line looks like: INFO:root:START_TASK: 29\\n\n",
    "    task = int(lines[task_start][:-1].split(\": \")[-1])\n",
    "    \n",
    "    # Since the in-sample evaluation message follows directly after optimization is done, we use that to record\n",
    "    # the number of generations. We account for the early stopping message if it did not run to 200 generations.\n",
    "    ended_early = 0 if in_start - task_start == 201 else - 1\n",
    "    generations_by_task[task] = in_start - (task_start + 1) - ended_early\n",
    "    \n",
    "    # Following the \"INFO:root:Evaluating in sample:\" message, symbolic default performance are printed\n",
    "    # They are formatted as \"INFO:root:[make_tuple(p, mkd)|0.8893]\"\n",
    "    # First is any number of best solutions from the pareto front. The last four are benchmark solutions.\n",
    "    # It is possible that two equally good solutions are printed (i.e. same length and performance).    \n",
    "    expr_in_task = set()\n",
    "    \n",
    "#     in_sample_evaluation_lines = lines[in_start + 1 : out_start]\n",
    "#     in_sample_evaluations = [parse_evaluation_line(eval_) for eval_ in in_sample_evaluation_lines]\n",
    "#     max_len = max([len_ for _, len_, _ in in_sample_evaluations])\n",
    "#     expr_in_task = {expr for expr, _, _ in in_sample_evaluations}\n",
    "    max_length = 0\n",
    "    \n",
    "    for in_sample_evaluation in lines[in_start + 1 : out_start]:\n",
    "        expr, length, score = parse_evaluation_line(in_sample_evaluation)\n",
    "        # Pareto fronts may contain literal duplicates, so we filter those out manually.\n",
    "        if expr not in expr_in_task:\n",
    "            expressions_by_length[length].append(expr)\n",
    "            expr_in_task.add(expr)\n",
    "        \n",
    "        if length !=0:\n",
    "            if length < 4:\n",
    "                # Only report one out-of-sample solution for each length (and all benchmarks), so overwrite is OK.\n",
    "                df.loc[task, \"in-sample\"][f\"score-{length}\"] = score\n",
    "                \n",
    "            # Update best so far score and maximum length\n",
    "            df.loc[task, \"in-sample\"][f\"score-best\"] = np.nanmax([score, df.loc[task, \"in-sample\"][f\"score-best\"]])\n",
    "            max_length = max(max_length, length)\n",
    "        else:\n",
    "            df.loc[task, \"in-sample\"][expr] = score\n",
    "            \n",
    "        if length > max_length:\n",
    "            max_length = length  # To know for which length \"best\" should score out of sample\n",
    "    \n",
    "    # Because two equal solutions can be in the Pareto front, \n",
    "    # we note the average out of sample performance if multiple solutions were found.\n",
    "    # Naturally, the solutions with the best in-sample score were those with the highest length in the Pareto front.\n",
    "    \n",
    "    scores_by_length = defaultdict(list)\n",
    "    \n",
    "    for out_sample_evaluation in lines[out_start + 1 : next_task]:\n",
    "        expr, length, score = parse_evaluation_line(out_sample_evaluation)   \n",
    "        if length !=0:\n",
    "            scores_by_length[length].append(score)\n",
    "        else:\n",
    "            df.loc[task, \"out-sample\"][expr] = score\n",
    "            \n",
    "    for length, scores in scores_by_length.items():\n",
    "        if length < 4:\n",
    "            df.loc[task, \"out-sample\"][f\"score-{length}\"] = np.mean(scores)\n",
    "        if length == max_length:\n",
    "            df.loc[task, \"out-sample\"][f\"score-best\"] = np.mean(scores)\n",
    "        if np.mean(scores) == float(\"nan\"):\n",
    "            print('hi')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 116 expressions of length 0. Most frequent: mlr_default (116 times)\n",
      " Found 475 expressions of length 1. Most frequent: make_tuple(9.0, n, 256.0, 256.0) (83 times)\n",
      " Found 245 expressions of length 2. Most frequent: make_tuple(9.0, truediv(256.0, m), 256.0, 256.0) (19 times)\n",
      " Found 167 expressions of length 3. Most frequent: make_tuple(9.0, 512.0, sub(n, 512.0), truediv(256.0, xvar)) (3 times)\n",
      " Found 107 expressions of length 4. Most frequent: make_tuple(9.0, mul(mcp, 256.0), mul(0.015625, add(p, n)), 1024.0) (1 times)\n",
      " Found  74 expressions of length 5. Most frequent: make_tuple(9.0, truediv(1024.0, neg(sub(m, 5.0))), truediv(n, 9.0), 1024.0) (1 times)\n",
      " Found  56 expressions of length 6. Most frequent: make_tuple(9.0, 256.0, sub(n, 512.0), truediv(truediv(256.0, mcp), min(mcp, sub(n, 512.0)))) (1 times)\n",
      " Found  31 expressions of length 7. Most frequent: make_tuple(9.0, n, max(max(128.0, truediv(pow(p, xvar), mcp)), truediv(p, sub(mcp, xvar))), 1024.0) (1 times)\n",
      " Found  15 expressions of length 8. Most frequent: make_tuple(9.0, mul(mcp, 256.0), sub(n, min(add(256.0, truediv(256.0, xvar)), 1024.0)), truediv(truediv(256.0, xvar), xvar)) (1 times)\n",
      " Found   7 expressions of length 9. Most frequent: make_tuple(9.0, truediv(256.0, sub(m, xvar)), sub(add(m, truediv(256.0, xvar)), truediv(256.0, m)), truediv(truediv(256.0, 0.9183113644279483), xvar)) (1 times)\n",
      " Found   7 expressions of length 10. Most frequent: make_tuple(9.0, truediv(add(add(m, add(m, add(9.0, 256.0))), 0.892693948151636), m), 1024.0, add(9.0, add(9.0, add(9.0, add(9.0, 256.0))))) (1 times)\n",
      " Found  11 expressions of length 11. Most frequent: make_tuple(9.0, truediv(256.0, sub(m, pow(xvar, m))), sub(add(0.9183113644279483, add(m, truediv(256.0, xvar))), truediv(256.0, m)), truediv(if_gt(mcp, xvar, 1024.0, 256.0), 0.9183113644279483)) (1 times)\n",
      " Found   9 expressions of length 12. Most frequent: make_tuple(9.0, add(truediv(n, pow(32.0, truediv(xvar, 0.9082883832341695))), poly_gt(n, truediv(rc, sub(32.0, p)))), add(truediv(n, pow(32.0, truediv(xvar, 0.9082883832341695))), 0.9082883832341695), n) (1 times)\n"
     ]
    }
   ],
   "source": [
    "for length, expressions in sorted(expressions_by_length.items()):\n",
    "    m = max(set(expressions), key=expressions.count)\n",
    "    print(f\" Found {len(expressions):3d} expressions of length {length}. Most frequent: {m} ({expressions.count(m)} times)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Found `N` expressions of length `L`\" here means across all the tasks' pareto fronts `N` solutions have length `L`.\n",
    "Pareto fronts may contain duplicate entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f93c8c1f28>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(generations_by_task, name=\"generations\").hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above plot shows the histogram of the number of generations across tasks (binsize=10).\n",
    "Note that if something ran for less than 200 generations, it found its optimum 20 generations earlier and early stopping terminated search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>score-1</th>\n",
       "      <th>score-2</th>\n",
       "      <th>score-3</th>\n",
       "      <th>score-best</th>\n",
       "      <th>mlr_default</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th>sample-type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>in-sample</th>\n",
       "      <td>0.7703</td>\n",
       "      <td>0.7711</td>\n",
       "      <td>0.7716</td>\n",
       "      <td>0.7719</td>\n",
       "      <td>0.5021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.8705</td>\n",
       "      <td>0.8705</td>\n",
       "      <td>0.8731</td>\n",
       "      <td>0.8731</td>\n",
       "      <td>0.1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">6</th>\n",
       "      <th>in-sample</th>\n",
       "      <td>0.7706</td>\n",
       "      <td>0.7712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7712</td>\n",
       "      <td>0.4951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.8295</td>\n",
       "      <td>0.8293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8293</td>\n",
       "      <td>0.9370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <th>in-sample</th>\n",
       "      <td>0.7698</td>\n",
       "      <td>0.7708</td>\n",
       "      <td>0.7711</td>\n",
       "      <td>0.7715</td>\n",
       "      <td>0.5020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189927</th>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.8339</td>\n",
       "      <td>0.8414</td>\n",
       "      <td>0.8414</td>\n",
       "      <td>0.8414</td>\n",
       "      <td>0.0219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">189928</th>\n",
       "      <th>in-sample</th>\n",
       "      <td>0.7700</td>\n",
       "      <td>0.7744</td>\n",
       "      <td>0.7751</td>\n",
       "      <td>0.7756</td>\n",
       "      <td>0.4977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.9075</td>\n",
       "      <td>0.9075</td>\n",
       "      <td>0.9093</td>\n",
       "      <td>0.9100</td>\n",
       "      <td>0.6381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">190411</th>\n",
       "      <th>in-sample</th>\n",
       "      <td>0.7702</td>\n",
       "      <td>0.7714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7714</td>\n",
       "      <td>0.5021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.8110</td>\n",
       "      <td>0.8110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8110</td>\n",
       "      <td>0.1313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>232 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    score-1  score-2  score-3  score-best  mlr_default\n",
       "task   sample-type                                                    \n",
       "3      in-sample     0.7703   0.7711   0.7716      0.7719       0.5021\n",
       "       out-sample    0.8705   0.8705   0.8731      0.8731       0.1286\n",
       "6      in-sample     0.7706   0.7712      NaN      0.7712       0.4951\n",
       "       out-sample    0.8295   0.8293      NaN      0.8293       0.9370\n",
       "11     in-sample     0.7698   0.7708   0.7711      0.7715       0.5020\n",
       "...                     ...      ...      ...         ...          ...\n",
       "189927 out-sample    0.8339   0.8414   0.8414      0.8414       0.0219\n",
       "189928 in-sample     0.7700   0.7744   0.7751      0.7756       0.4977\n",
       "       out-sample    0.9075   0.9075   0.9093      0.9100       0.6381\n",
       "190411 in-sample     0.7702   0.7714      NaN      0.7714       0.5021\n",
       "       out-sample    0.8110   0.8110      NaN      0.8110       0.1313\n",
       "\n",
       "[232 rows x 5 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>score-1</th>\n",
       "      <th>score-2</th>\n",
       "      <th>score-3</th>\n",
       "      <th>score-best</th>\n",
       "      <th>mlr_default</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th>sample-type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.8705</td>\n",
       "      <td>0.8705</td>\n",
       "      <td>0.8731</td>\n",
       "      <td>0.8731</td>\n",
       "      <td>0.1286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.8295</td>\n",
       "      <td>0.8293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8293</td>\n",
       "      <td>0.9370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.9207</td>\n",
       "      <td>0.8974</td>\n",
       "      <td>0.9020</td>\n",
       "      <td>0.8895</td>\n",
       "      <td>0.1472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.8328</td>\n",
       "      <td>0.8467</td>\n",
       "      <td>0.8467</td>\n",
       "      <td>0.8467</td>\n",
       "      <td>0.8921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.8229</td>\n",
       "      <td>0.8471</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8471</td>\n",
       "      <td>0.3793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168912</th>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.8768</td>\n",
       "      <td>0.8768</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8768</td>\n",
       "      <td>0.5460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189924</th>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.7088</td>\n",
       "      <td>0.7002</td>\n",
       "      <td>0.6948</td>\n",
       "      <td>0.6948</td>\n",
       "      <td>0.0941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189927</th>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.8339</td>\n",
       "      <td>0.8414</td>\n",
       "      <td>0.8414</td>\n",
       "      <td>0.8414</td>\n",
       "      <td>0.0219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189928</th>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.9075</td>\n",
       "      <td>0.9075</td>\n",
       "      <td>0.9093</td>\n",
       "      <td>0.9100</td>\n",
       "      <td>0.6381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190411</th>\n",
       "      <th>out-sample</th>\n",
       "      <td>0.8110</td>\n",
       "      <td>0.8110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8110</td>\n",
       "      <td>0.1313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    score-1  score-2  score-3  score-best  mlr_default\n",
       "task   sample-type                                                    \n",
       "3      out-sample    0.8705   0.8705   0.8731      0.8731       0.1286\n",
       "6      out-sample    0.8295   0.8293      NaN      0.8293       0.9370\n",
       "11     out-sample    0.9207   0.8974   0.9020      0.8895       0.1472\n",
       "12     out-sample    0.8328   0.8467   0.8467      0.8467       0.8921\n",
       "14     out-sample    0.8229   0.8471      NaN      0.8471       0.3793\n",
       "...                     ...      ...      ...         ...          ...\n",
       "168912 out-sample    0.8768   0.8768      NaN      0.8768       0.5460\n",
       "189924 out-sample    0.7088   0.7002   0.6948      0.6948       0.0941\n",
       "189927 out-sample    0.8339   0.8414   0.8414      0.8414       0.0219\n",
       "189928 out-sample    0.9075   0.9075   0.9093      0.9100       0.6381\n",
       "190411 out-sample    0.8110   0.8110      NaN      0.8110       0.1313\n",
       "\n",
       "[116 rows x 5 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_sample = df.index.map(lambda idx: idx[1] == \"out-sample\")\n",
    "df.loc[out_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the scores and note which solution leads to the best out of sample score per task. A solution wins **alone** if all other solutions have worse performance. It wins **shared** if at least one other solution has the same score, but no solution has a better score. **either** is the sum of alone and shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alone = defaultdict(int)\n",
    "shared = defaultdict(int)\n",
    "for _, out in df.loc[out_sample].iterrows():\n",
    "    best = out[out == out.max()].index.values\n",
    "    if len(best) == 1:\n",
    "        alone[best[0]] += 1\n",
    "    else:\n",
    "        for winner in best:\n",
    "            shared[winner] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mlr_default</th>\n",
       "      <th>score-1</th>\n",
       "      <th>score-2</th>\n",
       "      <th>score-3</th>\n",
       "      <th>score-best</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alone</th>\n",
       "      <td>36.0</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shared</th>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "      <td>35</td>\n",
       "      <td>30</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>either</th>\n",
       "      <td>NaN</td>\n",
       "      <td>47</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mlr_default  score-1  score-2  score-3  score-best\n",
       "alone          36.0       19        5        3           6\n",
       "shared          NaN       28       35       30          39\n",
       "either          NaN       47       40       33          45"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alone = {k: alone[k] for k in sorted(alone)}\n",
    "shared = {k: shared[k] for k in sorted(shared)}\n",
    "either = {k: shared[k] + alone[k] for k in sorted(shared)}\n",
    "pd.DataFrame([alone, shared, either], index=['alone', 'shared', 'either'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But tallying wins does not say much about the robustness of the symbolic defaults. We can also compare the average or median distance from the top performer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df.loc[out_sample].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out['max'] = df_out.max(axis=1)\n",
    "for col in df_out:\n",
    "    df_out['d_'+col] = df_out['max'] - df_out[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d_score-1        0.083527\n",
       "d_score-2        0.080796\n",
       "d_score-3        0.079558\n",
       "d_score-best     0.081667\n",
       "d_mlr_default    0.349429\n",
       "d_max            0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_cols = [c for c in df_out.columns if c.startswith('d_')]\n",
    "df_out[d_cols].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "d_score-1        0.00190\n",
       "d_score-2        0.00150\n",
       "d_score-3        0.00180\n",
       "d_score-best     0.00115\n",
       "d_mlr_default    0.34560\n",
       "d_max            0.00000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out[d_cols].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at in-sample performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score-best    54\n",
       "score-2       34\n",
       "score-3       28\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_sample = df.index.map(lambda idx: idx[1] == \"in-sample\")\n",
    "df.loc[in_sample].idxmax(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`idxmax` reports the first column that has a max score of the row. So we see that never is the \"score-1\" solution the best in-sample. It is divided somewhat evenly between score-2, score-3 and scores for greater lengths. In 15 of 106 cases, it does not find the solution \"symbolic best\", which would have had better in-sample performance for that task (for other tasks we don't know if it was considered)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reversing column order, we confirms there are no ties between any found solutions and benchmark ones (in-sample):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score-best    116\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[in_sample][reversed(df.columns)].idxmax(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "task    sample-type\n",
       "3       in-sample      score-best\n",
       "6       in-sample      score-best\n",
       "11      in-sample      score-best\n",
       "12      in-sample      score-best\n",
       "14      in-sample      score-best\n",
       "                          ...    \n",
       "168912  in-sample      score-best\n",
       "189924  in-sample      score-best\n",
       "189927  in-sample      score-best\n",
       "189928  in-sample      score-best\n",
       "190411  in-sample      score-best\n",
       "Length: 116, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[in_sample][reversed(df.columns)].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
