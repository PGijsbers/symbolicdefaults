{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Surrogate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently we use `RandomForestRegressor(n_estimators=100)` to train a surrogate model mapping hyperparameters to performance.\n",
    "Before training the performance is normalized s.t. the worst recorded performances on the task is 0, and the best recorded performance is 1.\n",
    "For each task, we train a separate surrogate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Problem` object is created, which is a wrapper for a configuration that specifies which hyperparameters to model, and which experiment data or surrogate models to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./src/\")\n",
    "from problem import Problem\n",
    "\n",
    "problem = Problem(\"mlr_svm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create a grid with surrogate responses for many configurations of `cost` and `gamma`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "min_c = problem.data.groupby(by='task_id')\n",
    "\n",
    "costs = np.geomspace(problem.data.cost.min(), problem.data.cost.max(), 200)\n",
    "gammas = np.geomspace(problem.data.gamma.min(), problem.data.gamma.max(), 200)\n",
    "configurations = list(itertools.product(costs, gammas))\n",
    "surrogate_response = pd.DataFrame(list(configurations), columns=['cost','gamma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task, surrogate in problem.surrogates.items():\n",
    "    scores = surrogate.predict(configurations)\n",
    "    surrogate_response[task] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_response[\"mean\"] = surrogate_response.iloc[:, 2:].mean(axis=1)\n",
    "surrogate_response[\"median\"] = surrogate_response.iloc[:, 2:].median(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cost</th>\n",
       "      <th>gamma</th>\n",
       "      <th>3893</th>\n",
       "      <th>3902</th>\n",
       "      <th>3903</th>\n",
       "      <th>3904</th>\n",
       "      <th>3907</th>\n",
       "      <th>3913</th>\n",
       "      <th>3917</th>\n",
       "      <th>3918</th>\n",
       "      <th>...</th>\n",
       "      <th>45</th>\n",
       "      <th>3560</th>\n",
       "      <th>3561</th>\n",
       "      <th>49</th>\n",
       "      <th>53</th>\n",
       "      <th>6</th>\n",
       "      <th>58</th>\n",
       "      <th>14954</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.209920e-15</td>\n",
       "      <td>6.262440e-12</td>\n",
       "      <td>0.737437</td>\n",
       "      <td>0.636123</td>\n",
       "      <td>0.964960</td>\n",
       "      <td>0.374267</td>\n",
       "      <td>0.976864</td>\n",
       "      <td>0.380642</td>\n",
       "      <td>0.351622</td>\n",
       "      <td>0.445954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665893</td>\n",
       "      <td>0.510417</td>\n",
       "      <td>0.233432</td>\n",
       "      <td>0.054122</td>\n",
       "      <td>0.077754</td>\n",
       "      <td>0.250004</td>\n",
       "      <td>0.330905</td>\n",
       "      <td>0.213614</td>\n",
       "      <td>0.394369</td>\n",
       "      <td>0.338140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.209920e-15</td>\n",
       "      <td>8.082234e-12</td>\n",
       "      <td>0.737437</td>\n",
       "      <td>0.636123</td>\n",
       "      <td>0.964960</td>\n",
       "      <td>0.374267</td>\n",
       "      <td>0.976864</td>\n",
       "      <td>0.380642</td>\n",
       "      <td>0.351622</td>\n",
       "      <td>0.445954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665893</td>\n",
       "      <td>0.510417</td>\n",
       "      <td>0.233432</td>\n",
       "      <td>0.054122</td>\n",
       "      <td>0.077754</td>\n",
       "      <td>0.250004</td>\n",
       "      <td>0.330905</td>\n",
       "      <td>0.213614</td>\n",
       "      <td>0.394369</td>\n",
       "      <td>0.338140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.209920e-15</td>\n",
       "      <td>1.043084e-11</td>\n",
       "      <td>0.737437</td>\n",
       "      <td>0.636123</td>\n",
       "      <td>0.964960</td>\n",
       "      <td>0.374267</td>\n",
       "      <td>0.976864</td>\n",
       "      <td>0.380642</td>\n",
       "      <td>0.351622</td>\n",
       "      <td>0.445954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665893</td>\n",
       "      <td>0.510417</td>\n",
       "      <td>0.233432</td>\n",
       "      <td>0.054122</td>\n",
       "      <td>0.077754</td>\n",
       "      <td>0.250004</td>\n",
       "      <td>0.330905</td>\n",
       "      <td>0.213614</td>\n",
       "      <td>0.394369</td>\n",
       "      <td>0.338140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.209920e-15</td>\n",
       "      <td>1.346193e-11</td>\n",
       "      <td>0.737437</td>\n",
       "      <td>0.636123</td>\n",
       "      <td>0.964960</td>\n",
       "      <td>0.374267</td>\n",
       "      <td>0.976864</td>\n",
       "      <td>0.380642</td>\n",
       "      <td>0.351622</td>\n",
       "      <td>0.445954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665893</td>\n",
       "      <td>0.510417</td>\n",
       "      <td>0.233432</td>\n",
       "      <td>0.054122</td>\n",
       "      <td>0.077754</td>\n",
       "      <td>0.250004</td>\n",
       "      <td>0.330905</td>\n",
       "      <td>0.213614</td>\n",
       "      <td>0.394369</td>\n",
       "      <td>0.338140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.209920e-15</td>\n",
       "      <td>1.737381e-11</td>\n",
       "      <td>0.737437</td>\n",
       "      <td>0.636123</td>\n",
       "      <td>0.964960</td>\n",
       "      <td>0.374267</td>\n",
       "      <td>0.976864</td>\n",
       "      <td>0.380642</td>\n",
       "      <td>0.351622</td>\n",
       "      <td>0.445954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665893</td>\n",
       "      <td>0.510417</td>\n",
       "      <td>0.233432</td>\n",
       "      <td>0.054122</td>\n",
       "      <td>0.077754</td>\n",
       "      <td>0.250004</td>\n",
       "      <td>0.330905</td>\n",
       "      <td>0.213614</td>\n",
       "      <td>0.394369</td>\n",
       "      <td>0.338140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>1.129040e+10</td>\n",
       "      <td>2.514069e+10</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.636635</td>\n",
       "      <td>0.996669</td>\n",
       "      <td>0.506477</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>0.275781</td>\n",
       "      <td>0.387702</td>\n",
       "      <td>0.653483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201143</td>\n",
       "      <td>0.273411</td>\n",
       "      <td>0.222201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045392</td>\n",
       "      <td>0.108461</td>\n",
       "      <td>0.023628</td>\n",
       "      <td>0.120485</td>\n",
       "      <td>0.283484</td>\n",
       "      <td>0.128956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>1.129040e+10</td>\n",
       "      <td>3.244629e+10</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.636635</td>\n",
       "      <td>0.996669</td>\n",
       "      <td>0.506477</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>0.275781</td>\n",
       "      <td>0.387702</td>\n",
       "      <td>0.653483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201143</td>\n",
       "      <td>0.273411</td>\n",
       "      <td>0.222201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045392</td>\n",
       "      <td>0.108461</td>\n",
       "      <td>0.023628</td>\n",
       "      <td>0.120485</td>\n",
       "      <td>0.283484</td>\n",
       "      <td>0.128956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>1.129040e+10</td>\n",
       "      <td>4.187482e+10</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.636635</td>\n",
       "      <td>0.996669</td>\n",
       "      <td>0.506477</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>0.275781</td>\n",
       "      <td>0.387702</td>\n",
       "      <td>0.653483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201143</td>\n",
       "      <td>0.273411</td>\n",
       "      <td>0.222201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045392</td>\n",
       "      <td>0.108461</td>\n",
       "      <td>0.023628</td>\n",
       "      <td>0.120485</td>\n",
       "      <td>0.283484</td>\n",
       "      <td>0.128956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>1.129040e+10</td>\n",
       "      <td>5.404317e+10</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.636635</td>\n",
       "      <td>0.996669</td>\n",
       "      <td>0.506477</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>0.275781</td>\n",
       "      <td>0.387702</td>\n",
       "      <td>0.653483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201143</td>\n",
       "      <td>0.273411</td>\n",
       "      <td>0.222201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045392</td>\n",
       "      <td>0.108461</td>\n",
       "      <td>0.023628</td>\n",
       "      <td>0.120485</td>\n",
       "      <td>0.283484</td>\n",
       "      <td>0.128956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>1.129040e+10</td>\n",
       "      <td>6.974750e+10</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.636635</td>\n",
       "      <td>0.996669</td>\n",
       "      <td>0.506477</td>\n",
       "      <td>0.999974</td>\n",
       "      <td>0.275781</td>\n",
       "      <td>0.387702</td>\n",
       "      <td>0.653483</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201143</td>\n",
       "      <td>0.273411</td>\n",
       "      <td>0.222201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045392</td>\n",
       "      <td>0.108461</td>\n",
       "      <td>0.023628</td>\n",
       "      <td>0.120485</td>\n",
       "      <td>0.283484</td>\n",
       "      <td>0.128956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               cost         gamma      3893      3902      3903      3904  \\\n",
       "0      1.209920e-15  6.262440e-12  0.737437  0.636123  0.964960  0.374267   \n",
       "1      1.209920e-15  8.082234e-12  0.737437  0.636123  0.964960  0.374267   \n",
       "2      1.209920e-15  1.043084e-11  0.737437  0.636123  0.964960  0.374267   \n",
       "3      1.209920e-15  1.346193e-11  0.737437  0.636123  0.964960  0.374267   \n",
       "4      1.209920e-15  1.737381e-11  0.737437  0.636123  0.964960  0.374267   \n",
       "...             ...           ...       ...       ...       ...       ...   \n",
       "39995  1.129040e+10  2.514069e+10  0.000002  0.636635  0.996669  0.506477   \n",
       "39996  1.129040e+10  3.244629e+10  0.000002  0.636635  0.996669  0.506477   \n",
       "39997  1.129040e+10  4.187482e+10  0.000002  0.636635  0.996669  0.506477   \n",
       "39998  1.129040e+10  5.404317e+10  0.000002  0.636635  0.996669  0.506477   \n",
       "39999  1.129040e+10  6.974750e+10  0.000002  0.636635  0.996669  0.506477   \n",
       "\n",
       "           3907      3913      3917      3918  ...        45      3560  \\\n",
       "0      0.976864  0.380642  0.351622  0.445954  ...  0.665893  0.510417   \n",
       "1      0.976864  0.380642  0.351622  0.445954  ...  0.665893  0.510417   \n",
       "2      0.976864  0.380642  0.351622  0.445954  ...  0.665893  0.510417   \n",
       "3      0.976864  0.380642  0.351622  0.445954  ...  0.665893  0.510417   \n",
       "4      0.976864  0.380642  0.351622  0.445954  ...  0.665893  0.510417   \n",
       "...         ...       ...       ...       ...  ...       ...       ...   \n",
       "39995  0.999974  0.275781  0.387702  0.653483  ...  0.201143  0.273411   \n",
       "39996  0.999974  0.275781  0.387702  0.653483  ...  0.201143  0.273411   \n",
       "39997  0.999974  0.275781  0.387702  0.653483  ...  0.201143  0.273411   \n",
       "39998  0.999974  0.275781  0.387702  0.653483  ...  0.201143  0.273411   \n",
       "39999  0.999974  0.275781  0.387702  0.653483  ...  0.201143  0.273411   \n",
       "\n",
       "           3561        49        53         6        58     14954      mean  \\\n",
       "0      0.233432  0.054122  0.077754  0.250004  0.330905  0.213614  0.394369   \n",
       "1      0.233432  0.054122  0.077754  0.250004  0.330905  0.213614  0.394369   \n",
       "2      0.233432  0.054122  0.077754  0.250004  0.330905  0.213614  0.394369   \n",
       "3      0.233432  0.054122  0.077754  0.250004  0.330905  0.213614  0.394369   \n",
       "4      0.233432  0.054122  0.077754  0.250004  0.330905  0.213614  0.394369   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "39995  0.222201  0.000000  0.045392  0.108461  0.023628  0.120485  0.283484   \n",
       "39996  0.222201  0.000000  0.045392  0.108461  0.023628  0.120485  0.283484   \n",
       "39997  0.222201  0.000000  0.045392  0.108461  0.023628  0.120485  0.283484   \n",
       "39998  0.222201  0.000000  0.045392  0.108461  0.023628  0.120485  0.283484   \n",
       "39999  0.222201  0.000000  0.045392  0.108461  0.023628  0.120485  0.283484   \n",
       "\n",
       "         median  \n",
       "0      0.338140  \n",
       "1      0.338140  \n",
       "2      0.338140  \n",
       "3      0.338140  \n",
       "4      0.338140  \n",
       "...         ...  \n",
       "39995  0.128956  \n",
       "39996  0.128956  \n",
       "39997  0.128956  \n",
       "39998  0.128956  \n",
       "39999  0.128956  \n",
       "\n",
       "[40000 rows x 110 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surrogate_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note not all experiment tasks seem to have meta-data yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{168759.0, 168761.0, 168770.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(problem.data.task_id.unique()) - set(problem.metadata.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compare two configurations on their in-sample and out-of-sample performance, we would expect them to agree most of the times. We check this by:\n",
    " - drawing two configurations at random\n",
    " - compare the in-sample and out-of-sample configuration and note if they agree\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_scores(s, task):\n",
    "    in_sample = np.mean(s[2:-2][s[2:-2].index != task])\n",
    "    return s[task], in_sample\n",
    "    \n",
    "task_bool_pairs = list(itertools.product(problem.metadata.index,[True,  False]))\n",
    "mi = pd.MultiIndex.from_tuples(task_bool_pairs, names=('task', ' '))\n",
    "conf = pd.DataFrame(np.zeros((2*len(problem.metadata.index), 2)), columns=[True, False], index=mi)\n",
    "\n",
    "for _ in range(100):\n",
    "    samples = surrogate_response.sample(2)\n",
    "    for task in problem.metadata.index:\n",
    "        a_out, a_in = get_sample_scores(samples.iloc[0], task)\n",
    "        b_out, b_in = get_sample_scores(samples.iloc[1], task)\n",
    "\n",
    "        if a_out == b_out or a_in == b_in:\n",
    "            continue\n",
    "        conf.loc[(task, a_in > b_in)][a_out > b_out] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = pd.DataFrame([[0]] * len(problem.metadata.index), columns=[\"accuracy\"], index=problem.metadata.index)\n",
    "\n",
    "for task in problem.metadata.index:\n",
    "    c = conf.loc[task]\n",
    "    acc.loc[task] = (c.loc[True, True] + c.loc[False, False]) / c.sum().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x0000026F32470668>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAV2ElEQVR4nO3dfZAcd33n8fc39jmxvUY2EVkSYVgegnnQQgpNfAnccbv4SGxMMCSksBEPdpxsQhXEdSU4zKUSuEpRp0viy6XOl6N04DKEnDdgDOFsSHAZFlcIxkiOQH4iGBBGImcBBsE6TszC9/6YVrRe7Wp6u2dm+wfvV9WWtnu65/dRj/TZ3p7p7shMJEnl+ZGNDiBJasYCl6RCWeCSVCgLXJIKZYFLUqEscEkqlAUuSYWywCWpUBa4tIbo8/+IOst/nOq8iLg8Ir4QEd+JiDsj4iXLHvuNiLhr2WPPquafGRHXRcTXIuIbEXFlNf8tEfHuZetPRURGxInV9EJEvDUiPgH8I/CEiLhk2RhfjIjfXJHvgojYGxHfrnKeGxG/GhF7Viy3IyI+MLotpR82FrhK8AXg3wKbgP8MvDsifjIifhV4C/Aq4BHAi4BvRMQJwPXAl4EpYAswv47xXgnMAadVz3EIeGE1xiXAHy/7QXE28C7gDcDpwHOB/cAHgcdHxFOXPe8rgD9b199cOg4LXJ2Xme/NzK9m5vcz8y+AzwNnA78O/EFmfjr77snML1eP/RTwhsx8IDP/KTP/Zh1DXp2Zd2TmUmZ+NzNvyMwvVGN8HPgI/R8oAJcCV2XmjVW+g5l5d2b+M/AX9EubiHg6/R8m1w9hk0iABa4CRMSrqkMU34qIbwFbgc3AmfT3zlc6E/hyZi41HPIrK8Y/LyJuiYj7q/FfUI1/ZKzVMgC8E3h5RAT9vfr3VMUuDYUFrk6LiMcB/xt4LfDjmXk6cDsQ9Iv2iaus9hXgsUeOa6/wAHDKsulHr7LMv1yiMyJ+FHgf8EfAZDX+h6rxj4y1WgYy8xbgIfp76y/HwycaMgtcXXcq/UL9GkBEXEJ/Dxzg7cDrI2Jb9YmRJ1WFfyvwD8DOiDg1In4sIp5TrbMXeG5EPDYiNgFvGjD+ScCPVuMvRcR5wC8se/wdwCURcU5E/EhEbImIpyx7/F3AlcDSOg/jSANZ4Oq0zLwTuAL4JHAfMA18onrsvcBbgf8DfAf4APDIzPwe8EvAk4B7gQPAy6p1bqR/bPqzwB4GHJPOzO8Avw28B/gm/T3pDy57/FaqNzaBw8DHgccte4o/o/8Dx71vDV14QwdpdCLiZPqfYnlWZn5+o/PoB4t74NJovQb4tOWtUVjtTR5JQxAR++m/2fniDY6iH1AeQpGkQnkIRZIKNdZDKJs3b86pqanG6z/wwAOceuqpwws0AmZsr+v5oPsZu54Pup+xS/n27Nnz9cx81DEPZObYvrZt25ZtfOxjH2u1/jiYsb2u58vsfsau58vsfsYu5QN25yqd6iEUSSqUBS5JhbLAJalQFrgkFcoCl6RCWeCSVCgLXJIKZYFLUqEscEkqlFcjlFSUqctvaLzu/p3nDzHJxnMPXJIKZYFLUqEscEkqlAUuSYWywCWpUBa4JBXKApekQlngklSogQUeEVdFxKGIuH3F/NdFxOci4o6I+IPRRZQkrabOHvjVwLnLZ0TELHAB8IzMfDrwR8OPJkk6noEFnpk3A/evmP0aYGdm/nO1zKERZJMkHUf0b3g8YKGIKeD6zNxaTe8F/pL+nvk/Aa/PzE+vse4cMAcwOTm5bX5+vnHYxcVFJiYmGq8/DmZsr+v5oPsZu54Pmmfcd/Bw4zGnt2yqvWyXtuHs7OyezOytnN/0YlYnAmcAPwf8LPCeiHhCrvLTIDN3AbsAer1ezszMNBwSFhYWaLP+OJixva7ng+5n7Ho+aJ7x4jYXs9pef7wStmHTT6EcAK7LvluB7wObhxdLkjRI0wL/APA8gIh4MnAS8PVhhZIkDTbwEEpEXAPMAJsj4gDwZuAq4Krqo4UPAa9e7fCJJGl0BhZ4Zl60xkOvGHIWSdI6eCamJBXKApekQlngklQoC1ySCmWBS1KhLHBJKlTTU+kl/RCbanE6O8D+necPKckPN/fAJalQFrgkFcoCl6RCWeCSVCgLXJIKZYFLUqEscEkqlAUuSYUaWOARcVVEHKpu3rDysddHREaEt1OTpDGrswd+Nf27zz9MRJwJPB+4d8iZJEk1DCzwzLwZuH+Vh/4Y+I+At1KTpA0QdW5lGRFTwPWZubWafhFwTmZeFhH7gV5mrnpT44iYA+YAJicnt83PzzcOu7i4yMTEROP1x8GM7XU9H3Q/46jz7Tt4uNX601s2Nc7YZuzpLZtqL9ul13h2dnZPZvZWzl/3xawi4hTgd4BfqLN8Zu4CdgH0er2cmZlZ75D/YmFhgTbrj4MZ2+t6Puh+xlHnu7jtxay2zzTO2Gbs/dvrj9f11xiafQrlicDjgc9Ue9+PAW6LiEcPM5gk6fjWvQeemfuAnzgyPegQiiRpNOp8jPAa4JPAWRFxICIuHX0sSdIgA/fAM/OiAY9PDS2NJKk2z8SUpEJZ4JJUKAtckgplgUtSoSxwSSqUBS5JhbLAJalQFrgkFcoCl6RCWeCSVCgLXJIKZYFLUqEscEkqlAUuSYWywCWpUHVu6HBVRByKiNuXzfvDiLg7Ij4bEe+PiNNHG1OStFKdPfCrgXNXzLsR2JqZzwD+HnjTkHNJkgYYWOCZeTNw/4p5H8nMpWryFvo3NpYkjdEwjoH/GvDhITyPJGkdIjMHLxQxBVyfmVtXzP8doAf8cq7xRBExB8wBTE5Obpufn28cdnFxkYmJicbrj4MZ2+t6Puh+xlHn23fwcKv1p7dsapyxzdjTWzbVXrZLr/Hs7OyezOytnD/wpsZriYhXAy8EzlmrvAEycxewC6DX6+XMzEzTIVlYWKDN+uNgxva6ng+6n3HU+S6+/IZW6+/fPtM4Y5ux92+vP17XX2NoWOARcS7wRuDfZeY/DjeSJKmOOh8jvAb4JHBWRByIiEuBK4HTgBsjYm9EvG3EOSVJKwzcA8/Mi1aZ/Y4RZJEkrYNnYkpSoSxwSSqUBS5JhbLAJalQFrgkFcoCl6RCWeCSVKjGp9JLUmmm1nEa/o7ppYedtr9/5/mjiNSKe+CSVCgLXJIKZYFLUqEscEkqlAUuSYWywCWpUBa4JBXKApekQtW5I89VEXEoIm5fNu+REXFjRHy++vOM0caUJK1UZw/8auDcFfMuB27KzJ8GbqqmJUljNLDAM/Nm4P4Vsy8A3ll9/07gxUPOJUkaIDJz8EIRU8D1mbm1mv5WZp6+7PFvZuaqh1EiYg6YA5icnNw2Pz/fOOzi4iITExON1x8HM7bX9XxwbMZ9Bw83fq7pLZuGEelhRr0N2/x9of93bpqx7dh1TZ4M9z14dHoUr1Nds7OzezKzt3L+yC9mlZm7gF0AvV4vZ2ZmGj/XwsICbdYfBzO21/V8cGzGi9dxkaSV9m+fGbjMeo16G7b5+0L/79w0Y9ux69oxvcQV+45W5Chep7aafgrlvoj4SYDqz0PDiyRJqqNpgX8QeHX1/auBvxxOHElSXXU+RngN8EngrIg4EBGXAjuB50fE54HnV9OSpDEaeAw8My9a46FzhpxFkrQOnokpSYWywCWpUBa4JBXKApekQlngklQoC1ySCjXyU+klddPUmE5J1+i4By5JhbLAJalQFrgkFcoCl6RCWeCSVCgLXJIKZYFLUqEscEkqVKsCj4j/EBF3RMTtEXFNRPzYsIJJko6vcYFHxBbgt4Fedbf6E4ALhxVMknR8bQ+hnAicHBEnAqcAX20fSZJUR2Rm85UjLgPeCjwIfCQzt6+yzBwwBzA5Obltfn6+8XiLi4tMTEw0Xn8czNhe1/PBsRn3HTzc+Lmmt2waRqSHqbMN22Rua3rLpsav87hyT54M9z14dHoUr1Nds7OzezKzt3J+4wKPiDOA9wEvA74FvBe4NjPfvdY6vV4vd+/e3Wg8gIWFBWZmZhqvPw5mbK/r+eDYjG0uDLV/5/lDSPRwdbbhRl7Mav/O8xu/zuPKvWN6iSv2Hb3e3yhep7oiYtUCb3MI5d8DX8rMr2Xmd4HrgGe3eD5J0jq0KfB7gZ+LiFMiIujfpf6u4cSSJA3SuMAz81PAtcBtwL7quXYNKZckaYBWN3TIzDcDbx5SFknSOngmpiQVygKXpEJZ4JJUKAtckgplgUtSoSxwSSqUBS5JhWr1OXBJ7XXtOirjMHX5DeyYXuLiDbweyw8C98AlqVAWuCQVygKXpEJZ4JJUKAtckgplgUtSoSxwSSqUBS5JhWpV4BFxekRcGxF3R8RdEfHzwwomSTq+tmdi/gnwV5n50og4CThlCJkkSTU0LvCIeATwXOBigMx8CHhoOLEkSYNEZjZbMeJn6N/E+E7gmcAe4LLMfGDFcnPAHMDk5OS2+fn5xmEXFxeZmJhovP44mLG9rueDYzPuO3h4Q3JMb9m06vw623CjMh8xeTLc9+CGRjiulfnW2tbjMDs7uyczeyvntynwHnAL8JzM/FRE/Anw7cz83bXW6fV6uXv37kbjASwsLDAzM9N4/XEwY3tdzwfHZmxzQao21rqYVZ1tuFGZj9gxvcQV+7p7Pb2V+TbywmERsWqBt3kT8wBwIDM/VU1fCzyrxfNJktahcYFn5v8DvhIRZ1WzzqF/OEWSNAZtf395HfDn1SdQvghc0j6SJKmOVgWemXuBY47LSJJGzzMxJalQFrgkFcoCl6RCWeCSVCgLXJIKZYFLUqG6ex6rNEbrPa18x/QSF2/wqeiwdu6u5PtB0vbSA6M4Fd89cEkqlAUuSYWywCWpUBa4JBXKApekQlngklQoC1ySCmWBS1KhWhd4RJwQEX8XEdcPI5AkqZ5h7IFfBtw1hOeRJK1DqwKPiMcA5wNvH04cSVJdkZnNV464FvgvwGnA6zPzhassMwfMAUxOTm6bn59vPN7i4iITExON1x8HM7bXNN++g4dHkGZ1kyfDfQ+Obbh163o+6H7GYeeb3rKp8bqzs7N7MvOY21c2vphVRLwQOJSZeyJiZq3lMnMXsAug1+vlzMyaiw60sLBAm/XHwYztNc03zos37Zhe4op93b0WXNfzQfczDjvf/u0zQ3uuI9ocQnkO8KKI2A/MA8+LiHcPJZUkaaDGBZ6Zb8rMx2TmFHAh8NHMfMXQkkmSjsvPgUtSoYZygCczF4CFYTyXJKke98AlqVAWuCQVygKXpEJZ4JJUKAtckgplgUtSobp7HmuHTK3jFO0d00sPO6V7/87zRxFJktwDl6RSWeCSVCgLXJIKZYFLUqEscEkqlAUuSYWywCWpUBa4JBWqcYFHxJkR8bGIuCsi7oiIy4YZTJJ0fG3OxFwCdmTmbRFxGrAnIm7MzDuHlE2SdBxt7on5D5l5W/X9d4C7gC3DCiZJOr7IzPZPEjEF3Axszcxvr3hsDpgDmJyc3DY/P994nMXFRSYmJpoHbWjfwcO1l508Ge578Oj09JZNI0jUzkZtxzr2HTx8zDbsoq5n7Ho+6H7GYedr0wWzs7N7MrO3cn7rAo+ICeDjwFsz87rjLdvr9XL37t2Nx1pYWGBmZqbx+k2t92JWV+w7emSqixez2qjtWMfU5Tccsw27qOsZu54Pup9x2PnadEFErFrgrT6FEhH/Cngf8OeDyluSNFxtPoUSwDuAuzLzvw0vkiSpjjZ74M8BXgk8LyL2Vl8vGFIuSdIAjQ/wZObfADHELJKkdfBMTEkqlAUuSYWywCWpUBa4JBXKApekQlngklQoC1ySCtXdCxGscOQaGRev47okR3TxeiSjdrzrtzTdjnX9MG5vaSO4By5JhbLAJalQFrgkFcoCl6RCWeCSVCgLXJIKZYFLUqEscEkqVNt7Yp4bEZ+LiHsi4vJhhZIkDdbmnpgnAP8TOA94GnBRRDxtWMEkScfXZg/8bOCezPxiZj4EzAMXDCeWJGmQyMxmK0a8FDg3M3+9mn4l8K8z87UrlpsD5qrJs4DPNY/LZuDrLdYfBzO21/V80P2MXc8H3c/YpXyPy8xHrZzZ5mJWq93Q+JifBpm5C9jVYpyjA0bszszeMJ5rVMzYXtfzQfczdj0fdD9j1/NBu0MoB4Azl00/BvhquziSpLraFPingZ+OiMdHxEnAhcAHhxNLkjRI40MombkUEa8F/ho4AbgqM+8YWrLVDeVQzIiZsb2u54PuZ+x6Puh+xq7na/4mpiRpY3kmpiQVygKXpEJ1ssAHnaIfEdsj4rPV199GxDM7mPGCKt/eiNgdEf+mS/mWLfezEfG96nP9Y1VjG85ExOFqG+6NiN/rUr5lGfdGxB0R8fFx5quTMSLesGz73V691o/sUL5NEfF/I+Iz1Ta8ZFzZ1pHxjIh4f/X/+daI2DrujGvKzE590X9D9AvAE4CTgM8AT1uxzLOBM6rvzwM+1cGMExx9j+EZwN1dyrdsuY8CHwJe2sFtOANc3+F/h6cDdwKPraZ/omsZVyz/S8BHu5QP+E/Af62+fxRwP3BSxzL+IfDm6vunADdtxL/J1b66uAc+8BT9zPzbzPxmNXkL/c+gdy3jYlavOHAqq5zktJH5Kq8D3gccGmO2I7p+KYY6+V4OXJeZ9wJk5ri343q34UXANWNJ1lcnXwKnRUTQ3+m5H1jqWManATcBZObdwFRETI4x45q6WOBbgK8smz5QzVvLpcCHR5roWLUyRsRLIuJu4Abg18aUDWrki4gtwEuAt40x13J1X+efr369/nBEPH080YB6+Z4MnBERCxGxJyJeNbZ0fbX/r0TEKcC59H9gj0udfFcCT6V/EuA+4LLM/P544gH1Mn4G+GWAiDgbeBzj32lcVRcLvNYp+gARMUu/wN840kSrDL3KvNUuI/D+zHwK8GLg90ee6qg6+f478MbM/N4Y8qymTsbb6F8D4pnA/wA+MPJUR9XJdyKwDTgf+EXgdyPiyaMOtkzt/yv0D598IjPvH2Gelerk+0VgL/BTwM8AV0bEI0YdbJk6GXfS/0G9l/5vrX/HeH9LWFOba6GMSq1T9CPiGcDbgfMy8xtjynbEui4jkJk3R8QTI2JzZo7j4jh18vWA+f5vrmwGXhARS5k5rpIcmDEzv73s+w9FxJ92bBseAL6emQ8AD0TEzcAzgb8fQ74j49f9d3gh4z18AvXyXQLsrA433hMRX6J/nPnW8USs/e/wEoDqUM+Xqq+Nt9EH4Vd5U+FE4IvA4zn6psLTVyzzWOAe4Nkdzvgkjr6J+Szg4JHpLuRbsfzVjP9NzDrb8NHLtuHZwL1d2ob0f/W/qVr2FOB2YGuXtmG13Cb6x5ZP7eBr/L+At1TfT1b/TzZ3LOPpVG+sAr8BvGuc2/F4X53bA881TtGPiN+qHn8b8HvAjwN/Wu1BLuUYrxpWM+OvAK+KiO8CDwIvy+pfQEfybaiaGV8KvCYiluhvwwu7tA0z866I+Cvgs8D3gbdn5u3jyFc3Y7XoS4CPZP83hbGpme/3gasjYh/9wxlvzPH8hrWejE8F3hUR36P/qaNLx5VvEE+ll6RCdfFNTElSDRa4JBXKApekQlngklQoC1ySCmWBS1KhLHBJKtT/B1J8/oqe5LCDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.733, median: 0.770\n"
     ]
    }
   ],
   "source": [
    "print(f\"mean: {acc.mean().values[0]:.3f}, median: {acc.median().values[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To me, it seems that for static configurations the surrogate models behave as expected. Not perfect, but likely good enough. The big difference here is that we compared static configurations. We will now try this comparison with some dynamic configurations (takes more time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn(row):\n",
    "    return np.asarray([1., 1. / (row.p * row.xvar)]).reshape(1, -1)\n",
    "\n",
    "def symbolic1(row):\n",
    "    return np.asarray([16., row.mkd / row.xvar]).reshape(1, -1)\n",
    "\n",
    "def symbolic2(row):\n",
    "    return np.asarray([row.m, row.mkd + row.mkd]).reshape(1, -1)\n",
    "\n",
    "def constant(row):\n",
    "    return np.asarray([812.267350, 0.001361]).reshape(1, -1)\n",
    "\n",
    "dynamic_defaults = [sklearn, symbolic1, symbolic2, constant]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is un-optimized. This is deliberate, to make it easier to check for correctness.\n",
    "We create a dataframe with for each task a row, and for each dynamic default a column. \n",
    "In each cell we note the predicted performance of the dynamic configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_names = list(map(lambda x:x.__name__, dynamic_defaults))\n",
    "out_sample = pd.DataFrame([[0.] * 4] * len(problem.metadata.index), columns=dd_names, index=problem.metadata.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task, metadata in problem.metadata.iterrows():\n",
    "    for name, fn in zip(dd_names, dynamic_defaults):\n",
    "        out_sample.loc[task][name] = problem.surrogates[task].predict(fn(metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sklearn</th>\n",
       "      <th>symbolic1</th>\n",
       "      <th>symbolic2</th>\n",
       "      <th>constant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.921743</td>\n",
       "      <td>0.980686</td>\n",
       "      <td>0.876921</td>\n",
       "      <td>0.942129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.968598</td>\n",
       "      <td>0.988993</td>\n",
       "      <td>0.996004</td>\n",
       "      <td>0.926806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.883043</td>\n",
       "      <td>0.952532</td>\n",
       "      <td>0.904208</td>\n",
       "      <td>0.896596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.992578</td>\n",
       "      <td>0.998448</td>\n",
       "      <td>0.995730</td>\n",
       "      <td>0.998270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.964482</td>\n",
       "      <td>0.996041</td>\n",
       "      <td>0.987067</td>\n",
       "      <td>0.993019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189924</th>\n",
       "      <td>0.766128</td>\n",
       "      <td>0.986726</td>\n",
       "      <td>0.521526</td>\n",
       "      <td>0.557975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189927</th>\n",
       "      <td>0.971395</td>\n",
       "      <td>0.972945</td>\n",
       "      <td>0.887940</td>\n",
       "      <td>0.935110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189928</th>\n",
       "      <td>0.888723</td>\n",
       "      <td>0.977982</td>\n",
       "      <td>0.885039</td>\n",
       "      <td>0.836121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190411</th>\n",
       "      <td>0.871535</td>\n",
       "      <td>0.929714</td>\n",
       "      <td>0.885429</td>\n",
       "      <td>0.888776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190412</th>\n",
       "      <td>0.450556</td>\n",
       "      <td>0.755833</td>\n",
       "      <td>0.597778</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sklearn  symbolic1  symbolic2  constant\n",
       "3       0.921743   0.980686   0.876921  0.942129\n",
       "6       0.968598   0.988993   0.996004  0.926806\n",
       "11      0.883043   0.952532   0.904208  0.896596\n",
       "12      0.992578   0.998448   0.995730  0.998270\n",
       "14      0.964482   0.996041   0.987067  0.993019\n",
       "...          ...        ...        ...       ...\n",
       "189924  0.766128   0.986726   0.521526  0.557975\n",
       "189927  0.971395   0.972945   0.887940  0.935110\n",
       "189928  0.888723   0.977982   0.885039  0.836121\n",
       "190411  0.871535   0.929714   0.885429  0.888776\n",
       "190412  0.450556   0.755833   0.597778  0.250000\n",
       "\n",
       "[103 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting the best dynamic default for each task (based on the \"true\" performance predicted by the surrogates):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "symbolic1    44\n",
       "symbolic2    21\n",
       "constant     20\n",
       "sklearn      18\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_sample.idxmax(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the best overall dynamic default is `symbolic1`, though it certainly varies per task.\n",
    "Let's see what the in-sample surrogates would predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sample = pd.DataFrame([[0.] * 4] * len(problem.metadata.index), columns=dd_names, index=problem.metadata.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task, metadata in problem.metadata.iterrows():\n",
    "    for name, fn in zip(dd_names, dynamic_defaults):\n",
    "        # take the mean of the predicted performance on all tasks but 'task' for the given dynamic default:\n",
    "        in_sample.loc[task][name] = out_sample[out_sample.index != task][name].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sklearn</th>\n",
       "      <th>symbolic1</th>\n",
       "      <th>symbolic2</th>\n",
       "      <th>constant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.883735</td>\n",
       "      <td>0.894451</td>\n",
       "      <td>0.884096</td>\n",
       "      <td>0.857473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.883276</td>\n",
       "      <td>0.894370</td>\n",
       "      <td>0.882929</td>\n",
       "      <td>0.857623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.884115</td>\n",
       "      <td>0.894727</td>\n",
       "      <td>0.883829</td>\n",
       "      <td>0.857919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.883041</td>\n",
       "      <td>0.894277</td>\n",
       "      <td>0.882931</td>\n",
       "      <td>0.856922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.883316</td>\n",
       "      <td>0.894301</td>\n",
       "      <td>0.883016</td>\n",
       "      <td>0.856974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189924</th>\n",
       "      <td>0.885261</td>\n",
       "      <td>0.894392</td>\n",
       "      <td>0.887581</td>\n",
       "      <td>0.861239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189927</th>\n",
       "      <td>0.883248</td>\n",
       "      <td>0.894527</td>\n",
       "      <td>0.883988</td>\n",
       "      <td>0.857542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189928</th>\n",
       "      <td>0.884059</td>\n",
       "      <td>0.894478</td>\n",
       "      <td>0.884017</td>\n",
       "      <td>0.858512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190411</th>\n",
       "      <td>0.884227</td>\n",
       "      <td>0.894951</td>\n",
       "      <td>0.884013</td>\n",
       "      <td>0.857996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190412</th>\n",
       "      <td>0.888355</td>\n",
       "      <td>0.896656</td>\n",
       "      <td>0.886833</td>\n",
       "      <td>0.864258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sklearn  symbolic1  symbolic2  constant\n",
       "3       0.883735   0.894451   0.884096  0.857473\n",
       "6       0.883276   0.894370   0.882929  0.857623\n",
       "11      0.884115   0.894727   0.883829  0.857919\n",
       "12      0.883041   0.894277   0.882931  0.856922\n",
       "14      0.883316   0.894301   0.883016  0.856974\n",
       "...          ...        ...        ...       ...\n",
       "189924  0.885261   0.894392   0.887581  0.861239\n",
       "189927  0.883248   0.894527   0.883988  0.857542\n",
       "189928  0.884059   0.894478   0.884017  0.858512\n",
       "190411  0.884227   0.894951   0.884013  0.857996\n",
       "190412  0.888355   0.896656   0.886833  0.864258\n",
       "\n",
       "[103 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "symbolic1    103\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_sample.idxmax(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Symbolic1 completely dominates the in-sample ranking. It was picked based on the average response surface of surrogate models, so that's not really a surprise. On the other hand it is surprising since before we observed that for constant configurations in and out sample seem to largely agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = []\n",
    "for name1, name2 in itertools.combinations(dd_names, 2):\n",
    "    for task in problem.metadata.index:\n",
    "        a_in = in_sample.loc[task][name1]\n",
    "        b_in = in_sample.loc[task][name2]\n",
    "        b_out = out_sample.loc[task][name2]\n",
    "        a_out = out_sample.loc[task][name1]\n",
    "        correct.append((a_in > b_in and a_out > b_out) or (a_in <= b_in and a_out <= b_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.551779935275081"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(correct)/ len(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One difference is that for constants, we had many different samples drawn, so let's draw four constant configurations and compare them in the same way we did dynamic ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = surrogate_response.sample(4)\n",
    "out_sample_const = pd.DataFrame([[0.] * 4] * len(problem.metadata.index), columns=[0, 1, 2, 3], index=problem.metadata.index)\n",
    "in_sample_const = pd.DataFrame([[0.] * 4] * len(problem.metadata.index), columns=[0, 1, 2, 3], index=problem.metadata.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task, metadata in problem.metadata.iterrows():\n",
    "    for i, (idx, sample) in enumerate(samples.iterrows()):\n",
    "        out_sample_const.loc[task][i] = sample[task]\n",
    "        in_sample_const.loc[task][i] = sample.iloc[2:-2][sample.iloc[2:-2].index != task].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = []\n",
    "for c1, c2 in itertools.combinations(range(4), 2):\n",
    "    for task in problem.metadata.index:\n",
    "        a_in = in_sample_const.loc[task][c1]\n",
    "        b_in = in_sample_const.loc[task][c2]\n",
    "        b_out = out_sample_const.loc[task][c2]\n",
    "        a_out = out_sample_const.loc[task][c1]\n",
    "        correct.append((a_in > b_in and a_out > b_out) or (a_in <= b_in and a_out <= b_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8446601941747572"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(correct)/ len(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick to rerun, results vary from 0.58 (similar to dynamic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- \n",
    "**ignore below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate for each task gamma/cost based on a formula, and evaluate if evalution code is correct.\n",
    "Can also draw two random configurations and see if order holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_scores(s, task):\n",
    "    in_sample = np.mean(s[2:-2][s[2:-2].index != task])\n",
    "    return s[task], in_sample\n",
    "    \n",
    "task_bool_pairs = list(itertools.product(problem.metadata.index,[True,  False]))\n",
    "mi = pd.MultiIndex.from_tuples(task_bool_pairs, names=('task', ' '))\n",
    "conf = pd.DataFrame(np.zeros((2*len(problem.metadata.index), 2)), columns=[True, False], index=mi)\n",
    "\n",
    "for _ in range(1000):\n",
    "    samples = surrogate_response.sample(2)\n",
    "    for task in problem.metadata.index:\n",
    "        a_out, a_in = get_sample_scores(samples.iloc[0], task)\n",
    "        b_out, b_in = get_sample_scores(samples.iloc[1], task)\n",
    "\n",
    "        if a_out == b_out or a_in == b_in:\n",
    "            continue\n",
    "        conf.loc[(task, a_in > b_in)][a_out > b_out] += 1\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_scores(s, task):\n",
    "    in_sample = np.mean(s[2:-2][s[2:-2].index != task])\n",
    "    return s[task], in_sample\n",
    "    \n",
    "agreed=0\n",
    "for _ in range(1000):\n",
    "    samples = surrogate_response.sample(2)\n",
    "    a_in_outs = [get_sample_scores(samples.iloc[0], task) for task in problem.metadata.index]\n",
    "    b_in_outs = [get_sample_scores(samples.iloc[1], task) for task in problem.metadata.index]\n",
    "    a_outs, a_ins = list(zip(*a_in_outs))\n",
    "    b_outs, b_ins = list(zip(*b_in_outs))\n",
    "    a_in_wins = sum([a_in > b_in for (a_in, b_in) in zip(a_ins, b_ins)])\n",
    "    b_in_wins = sum([b_in > a_in for (a_in, b_in) in zip(a_ins, b_ins)])\n",
    "    a_out_wins = sum([a_in > b_in for (a_in, b_in) in zip(a_outs, b_outs)])\n",
    "    b_out_wins = sum([b_in > a_in for (a_in, b_in) in zip(a_outs, b_outs)])\n",
    "    if ((a_in_wins > b_in_wins and a_out_wins > b_out_wins)\n",
    "        or (b_in_wins > a_in_wins and b_out_wins > a_out_wins)):\n",
    "        agreed += 1\n",
    "agreed\n",
    "        \n",
    "    \n",
    "#     for task in problem.metadata.index:\n",
    "#         a_out, a_in = \n",
    "#         b_out, b_in = get_sample_scores(samples.iloc[1], task)\n",
    "\n",
    "#         if a_out == b_out or a_in == b_in:\n",
    "#             continue\n",
    "#         conf.loc[(task, a_in > b_in)][a_out > b_out] += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.loc[9971]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_acc = []\n",
    "for task in problem.metadata.index:\n",
    "    c = conf.loc[task]\n",
    "    task_acc.append(((c.loc[True, True] + c.loc[False, False]) / c.sum().sum(), task))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem.metadata.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(task_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_acc = []\n",
    "for task in problem.metadata.index:\n",
    "    c = conf.loc[task]\n",
    "    task_acc.append(((c.loc[True, True] + c.loc[False, False]) / c.sum().sum(), task))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = [1] * len(problem.metadata)\n",
    "gammas = 1. / (problem.metadata.p * problem.metadata.xvar)\n",
    "si = surrogate_response.set_index(['cost', 'gamma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.index.get_loc((costs[0], gammas.iloc[0]), method='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_pivot = surrogate_response.pivot(\"cost\", \"gamma\", \"mean\")\n",
    "sns.heatmap(surrogate_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nrows, ncolumns = 11, 10\n",
    "f, axes = plt.subplots(nrows, ncolumns, figsize=(100, 100))\n",
    "for i, task in enumerate(surrogate_response.columns[2:]):\n",
    "    ax = axes[i // ncolumns, i % ncolumns]\n",
    "    surrogate_pivot = surrogate_response.pivot(\"cost\", \"gamma\", task)\n",
    "    sns.heatmap(surrogate_pivot, ax=ax)\n",
    "    ax.set(title=task);\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.savefig(\"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoomed in:\n",
    "costs = np.geomspace(7e2, 1e3, 200)\n",
    "gammas = np.geomspace(1e-4, 1e-2, 200)\n",
    "configurations = list(itertools.product(costs, gammas))\n",
    "surrogate_scores = pd.DataFrame(list(configurations), columns=['cost','gamma'])\n",
    "\n",
    "for task, surrogate in problem.surrogates.items():\n",
    "    scores = surrogate.predict(configurations)\n",
    "    surrogate_scores[task] = scores\n",
    "    \n",
    "surrogate_scores[\"mean\"] = surrogate_scores.iloc[:, 2:].mean(axis=1)\n",
    "\n",
    "surrogate_pivot = surrogate_scores.pivot(\"cost\", \"gamma\", \"mean\")\n",
    "sns.heatmap(surrogate_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warp with metafeatures m, mkd\n",
    "costs =  np.geomspace(6e-2, 100, 200)\n",
    "gammas = np.geomspace(1e-1, 12, 200)\n",
    "configurations = list(itertools.product(costs, gammas))\n",
    "surrogate_scores = pd.DataFrame(list(configurations), columns=['cost','gamma'])\n",
    "\n",
    "\n",
    "for task, surrogate in problem.surrogates.items():\n",
    "    if (task in problem.metadata.index):\n",
    "        cfgs = list(itertools.product(costs * problem.metadata.loc[task, \"m\"], gammas * problem.metadata.loc[task,\"mkd\"]))\n",
    "        scores = surrogate.predict(cfgs)\n",
    "        surrogate_scores[task] = scores\n",
    "        \n",
    "surrogate_scores[\"mean\"] = surrogate_scores.iloc[:, 2:].mean(axis=1)\n",
    "\n",
    "surrogate_pivot = surrogate_scores.pivot(\"cost\", \"gamma\", \"mean\")\n",
    "sns.heatmap(surrogate_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Warp with metafeatures 1 / (p*xvar)\n",
    "costs =  np.geomspace(6e-2, 100, 200)\n",
    "gammas = np.geomspace(1e-1, 12, 200)\n",
    "configurations = list(itertools.product(costs, gammas))\n",
    "surrogate_scores = pd.DataFrame(list(configurations), columns=['cost','gamma'])\n",
    "\n",
    "\n",
    "for task, surrogate in problem.surrogates.items():\n",
    "    if (task in problem.metadata.index):\n",
    "        cfgs = list(itertools.product(costs, gammas * (1 / problem.metadata.loc[task,\"p\"] / problem.metadata.loc[task,\"xvar\"])))\n",
    "        scores = surrogate.predict(cfgs)\n",
    "        surrogate_scores[task] = scores\n",
    "        \n",
    "surrogate_scores[\"mean\"] = surrogate_scores.iloc[:, 2:].mean(axis=1)\n",
    "\n",
    "surrogate_pivot = surrogate_scores.pivot(\"cost\", \"gamma\", \"mean\")\n",
    "sns.heatmap(surrogate_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of settings:\n",
    "d0 = surrogate_scores.loc[surrogate_scores[\"mean\"] == max(surrogate_scores[\"mean\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = 0\n",
    "best = 0\n",
    "for idx, row in surrogate_scores.iterrows():\n",
    "    newmax = np.mean(np.maximum(row.iloc[2:-1].values, d0.iloc[:,2:-1].values))\n",
    "    if (newmax > best):\n",
    "        print(f\"Newmax found: {newmax}\")\n",
    "        best = newmax\n",
    "        best_idx = idx\n",
    "\n",
    "surrogate_scores.iloc[best_idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
