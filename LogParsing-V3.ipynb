{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runlog: \n",
    "    def __init__(self, problem: str, searches: List, benchmarks=None, ignore=None):\n",
    "        self.problem = problem\n",
    "        self.searches = searches\n",
    "        self.benchmarks = benchmarks\n",
    "        self.log_dirs= os.listdir(f\"runs/{algo}_{optimizer}/\")\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        df = read_run_logs(problem=self.problem, search=self.searches[0], target=\"new_evaluations\")\n",
    "        for search in self.searches[1:]:\n",
    "            df = df.append(read_run_logs(problem=self.problem, search=search, target=\"new_evaluations\"))\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def pick_relative(x, eps=0.01):\n",
    "        \"\"\"\n",
    "        Pick by relative improvement; Pick longer if better by 'eps', break if not\n",
    "        \"\"\"\n",
    "        x = x.copy()\n",
    "        x['relative_improvement'] = x['in'] - x['in'].shift(1, fill_value=0)\n",
    "        for ix, rw in x.iterrows():\n",
    "            if (rw['relative_improvement'] > eps):\n",
    "                use_ix = ix\n",
    "            else:\n",
    "                break\n",
    "        return use_ix\n",
    "\n",
    "    def pick_final_expression(self, method = \"best\"):\n",
    "        \"\"\"\n",
    "        Pick final expression\n",
    "        :method: either \"relative\", \"shortest\" or \"best\"\n",
    "        \"\"\"\n",
    "        df = self.data.copy()\n",
    "        df = df[df.groupby(\"run\")['gen'].transform(max) == df['gen']]\n",
    "        df = df.pivot_table(index=[\"run\", \"task\", \"gen\", \"length\", \"problem\", \"search\", \"expression\"],columns=\"inout\", values=\"score\")\n",
    "        df = df.reset_index()\n",
    "        if method == \"shortest\":\n",
    "            out = df[df.groupby(\"run\")['length'].transform(min) == df['length']]\n",
    "        elif method == \"relative\":\n",
    "            out = df.loc[[pick_relative(group) for name, group in fdf.groupby('run')]]\n",
    "        else:\n",
    "            out = df[df.groupby(\"run\")['in'].transform(max) == df['in']]\n",
    "        return out  \n",
    "        \n",
    "\n",
    "def read_run_logs(problem:str, search: str, target: str):\n",
    "    \"\"\"\n",
    "    Read all log-files for a given problem-search combination\n",
    "    \"\"\"\n",
    "    log_dir = f\"runs/{problem}_{search}/\"\n",
    "    dirs = [os.path.join(log_dir, f) for f in os.listdir(log_dir)]\n",
    "    df = pd.read_csv(f\"{dirs[0]}/{target}.csv\", sep=\";\",nrows=0)\n",
    "    for dir in dirs: \n",
    "        df = df.append(pd.read_csv(f\"{dir}/{target}.csv\", sep=\";\"))\n",
    "    df['problem'] = problem\n",
    "    df['search'] = search\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">run</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search</th>\n",
       "      <th>True</th>\n",
       "      <th>mupluslambda</th>\n",
       "      <th>random_search</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>task</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        run                           \n",
       "search True mupluslambda random_search\n",
       "task                                  \n",
       "3        19           20            19\n",
       "6        10           10            10\n",
       "11       10           10            10\n",
       "12        9           10            10\n",
       "14       10           10             9\n",
       "15       10           10            10\n",
       "16       10            9            10\n",
       "18       10           10            10"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmarks = [\"sklearn_default\", \"mlr_default\", \"symbolic_best\"]\n",
    "log = Runlog('svm',  [\"mupluslambda\", \"random_search\", \"True\"], benchmarks=benchmarks)\n",
    "\n",
    "import seaborn as sns\n",
    "df = log.pick_final_expression('shortest')\n",
    "df[[\"task\", \"search\", \"run\"]].pivot_table(index=\"task\",columns=\"search\", aggfunc=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f7bc0248bb0>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(data=df, x=\"task\", y=\"out\", hue = \"search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolic Defaults by 'complexity' of expression\n",
    "In this notebook we take a look at the results of running the script at its default settings, this means:\n",
    " - evaluation across all tasks\n",
    " - recording the pareto front of symbolic defaults after each search\n",
    " - evaluating in-sample and out-of-sample performance of those dynamic defaults, as well as some pre-defined ones\n",
    " \n",
    "**note:** The console cut off results for the first few tasks, so I am rerunning those now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each task we will extract:\n",
    " - the number of generations optimization ran for (max=200)\n",
    " - max length expression\n",
    " - in and out of sample performance for length 1, 2 and 3 expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each task save the benchmark results. We also save results for length 1, 2 and 3 solutions as well as the best one found (that may be longer). Specifically we record:\n",
    " - best in_sample performance at length 1, 2, 3\n",
    " - best in_sample performance for any length\n",
    " - average out_sample performance by length for length 1, 2, 3\n",
    " - average out_sample performance for the longest (i.e. best in-sample score) solution(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We have experiment data for a set of algorithms and meta-data for the datasets on which the experiments took place.\n",
    "We use symbolic regression to find an expression for symbolic default values that give good performance across tasks.\n",
    "Symbolic regression is performed with leave-one-task-out, which means for each algorithm we have multiple searches for a symbolic default, and their performance is recorded for both in-sample (the optimization surface of all-but-one tasks) and out-of-sample (the left out task) performance. Performance here is solely based on surrogate model predictions, no additional experiments have been performed (yet).\n",
    "\n",
    "In our search, we use NSGA-II selection to perform multi-objective optimization: find the expression with the best performance, while using the fewest number of operators (e.g. `divide`, `multiply`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms\n",
    "**Length** of an expression denotes the number of operators in it. A symbolic value is *not* considered an operation.\n",
    "Consider the following SVM defaults for cost and gamma:\n",
    " - `make_tuple`(m, mkd) is length 1.\n",
    " - `make_tuple`(m, `truediv`(mkd, xvar)) is length 2.\n",
    " - `make_tuple`(16., `truediv`(mkd, xvar)) is length 2.\n",
    "\n",
    "The **final** solution refers to the symbolic default with the highest in-sample score for a task (regardless of its length). This means for each task there is *at least* one final solution, but there may be more and they are not of a specific length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **baseline** solutions are typically the default hyperparameter settings of mlr, scikit-learn, or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read all the logs, because some logs are incomplete we have to explicitly give the name of the baselines (this will be fixed for future runs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we analyze the random forest\n",
    "alg = 'rf'\n",
    "\n",
    "import os\n",
    "baselines = dict(\n",
    "    glmnet=[\"mlr_default\", \"sklearn_default\"],\n",
    "    kerasff=[\"initial_values\"],\n",
    "    knn=[\"mlr_default\"],\n",
    "    rf=[\"mlr_default\"],\n",
    "    rpart=[\"mlr_default\"],\n",
    "    svm=[\"sklearn_scale\", \"symbolic_best\", \"skearn_default\", \"mlr_default\" , \"const\"],\n",
    ")\n",
    "dir_ = \"runs/running\"\n",
    "for file in os.listdir(dir_):\n",
    "    if file.endswith('.log') and alg in file and ('_0.log' in file or '_1.log' in file):\n",
    "        print(file)\n",
    "        baseline = []\n",
    "        for method, bls in baselines.items():\n",
    "            if method in file:\n",
    "                baseline = bls\n",
    "        traces[file[:-4]] = Trace(os.path.join(dir_, file), benchmarks=baseline, ignore=[\"const\", \"symbolic_v2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "As described before, for each problem we find a symbolic default leaving one task out.\n",
    "We are interested to see how fast the symbolic regression converges across tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Median number of generations across tasks by problem:\")\n",
    "for log, trace in traces.items():\n",
    "    print(f\"{log: <15} {trace.generations_by_task.median().astype(int):3d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = {k :v for (k,v) in traces.items() if k not in [\"mlr_glmnet_lisa_ints_0\", \"mlr_glmnet_lisa_ints_1\", \"mlr_glmnet_lisa_ints_2\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "fig, axes = plt.subplots(math.ceil(len(traces) / 4), 4, sharey=True, figsize=(16, 9))\n",
    "for ax, (log, trace) in zip(axes.flatten(), traces.items()):\n",
    "    traces[log].generations_by_task.hist(bins=20, ax=ax)\n",
    "    ax.set_title(f\"{log} ({len(trace.generations_by_task)} tasks)\")\n",
    "    ax.set_ylabel('count')\n",
    "    ax.set_xlabel('generations')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows a histogram counting the number of generations until stopping. These results were obtained with default setting of early stopping if no improvement was made after 20 generations, with a 200 generation maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing optimization traces\n",
    "The traces contain the full optimization traces inside the trace's **progdf** trace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.lineplot(x='gen', y='max', units='task', estimator=None, data=traces[f'{alg}_gauss24_1eph_d1_2'].progdf)\n",
    "# sns.lineplot(x='gen', y='max', units='task', estimator=None, data=traces[f'{alg}_gauss24_1eph_d1_2i'].progdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Expressions\n",
    "For a given problem, we have a Pareto front of solutions for search (=each left out task).\n",
    "This Pareto front may contain \"twins\", multiple solutions which performance equally well and have the same length.\n",
    "Given that the response surface does not differ *that* much when leaving any particular task out, we hope that the symbolic expressions we find are reasonably consistent across searches.\n",
    "To have some indication of how consistent the results are, for each problem we find the most frequent solutions of length 1, 2 and 3. We also note the number of hyperparameters for which we aim to find a symbolic default, as we expect this to be correlated to how consistent the solutions are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_count = pd.DataFrame(np.zeros((5, len(traces))), columns=list(traces), index=[1, 2, 3, \"#tasks\", \"params\"])\n",
    "for log, trace in traces.items():  \n",
    "    for length, expressions in sorted(trace.expressions.items()):\n",
    "        if 0 < length < 4:\n",
    "            m = max(set(expressions), key=expressions.count)\n",
    "            expr_count.loc[length][log] = expressions.count(m)\n",
    "            # print(f\" Found {len(expressions):3d} expressions of length {length}. Most frequent: {m} ({expressions.count(m)} times)\")\n",
    "            if length == 1:\n",
    "                expr_count.loc[\"#tasks\"][log] = len(trace.scores) / 2\n",
    "                expr_count.loc[\"params\"][log] = m.count(',') + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the found expressions per problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = f'{alg}' # run_one #f\"mlr_knn_lisa_gaussian\" # run_one\n",
    "for log, trace in traces.items():\n",
    "    print(log)\n",
    "    for length, expressions in sorted(trace.expressions.items()):\n",
    "        if 0 < length < 4:\n",
    "            m = max(set(expressions), key=expressions.count)\n",
    "            expr_count.loc[length][log] = expressions.count(m)\n",
    "            # print(f\" Found {len(expressions):3d} expressions of length {length}. Most frequent: {m} ({expressions.count(m)} times)\")\n",
    "            print(f\"Most frequent length {length} solution in Pareto front ({expressions.count(m)} times in {len(trace.scores) // 2} tasks):\\n     {m}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expression Quality\n",
    "The expressions we find also need to be good.\n",
    "Here we compare the following 'strategies':\n",
    " - length-*n*: always pick the best expression of length *n*\n",
    " - *final*: always pick the best expression, regardless of length\n",
    " - *baseline(s)*: compare it to baselines we defined\n",
    " \n",
    "We want to know (all based on out-of-sample performance):\n",
    " - which strategy gives the best solution most often?\n",
    " - which strategy experiences the least mean regret?\n",
    " - which strategy experiences the least median regret?\n",
    " \n",
    "As mentioned before, there can be \"twins\" in the Pareto front, which means multiple solutions with equal length have equal in-sample performance.\n",
    "In this case we average the out-of-sample score of those twins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### number of wins:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table records the number of times a strategy led to the symbolic expression with the best out-of-sample performance (multiple strategies can be the best each task):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_comparisons = pd.DataFrame()\n",
    "for log, trace in traces.items():\n",
    "    out_comparisons = out_comparisons.append(trace.comparison.loc['either'].rename(log))\n",
    "out_comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regret\n",
    "Here we look at the regret for a method compared to the best known performance on the dataset from the random search experiments. Per definition the best score in random search is 1 (normalized score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### median regret:\n",
    "The following table records the median regret for a specific strategy compared to picking the best in hindsight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only evaluate logs where all tasks in subset have completed\n",
    "\n",
    "full_tasks = [x[0] for x in traces[f\"mlr_{alg}_mupluslambda_0\"].scores.index]\n",
    "subset = set(full_tasks[:20])\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = {}\n",
    "for log, trace in traces.items():\n",
    "    if (subset.issubset(set([x[0] for x in trace.scores.index]))):\n",
    "        idx = trace.scores.index.map(lambda idx: idx[1] == \"out-sample\" and idx[0] in subset)\n",
    "        medians[log] = [(1 - trace.scores[idx].final).median()]\n",
    "        \n",
    "for bl in baselines[alg]:\n",
    "    medians[bl] = [(1 - traces[f\"mlr_{alg}_mupluslambda_0\"].scores[idx][bl]).median()]\n",
    "pd.DataFrame.from_dict(medians,orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = {}\n",
    "for log, trace in traces.items():\n",
    "    if (subset.issubset(set([x[0] for x in trace.scores.index]))):\n",
    "        idx = trace.scores.index.map(lambda idx: idx[1] == \"out-sample\" and idx[0] in subset)\n",
    "        means[log] = [(1 - trace.scores[idx].final).mean()]\n",
    "    \n",
    "for bl in baselines[alg]:\n",
    "    means[bl] = [(1 - traces[f\"mlr_{alg}_mupluslambda_0\"].scores[idx][bl]).median()]\n",
    "pd.DataFrame.from_dict(means,orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results averaged over replications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# means\n",
    "pd.DataFrame.from_dict(means,orient='index').groupby(lambda x: ''.join(i for i in x if not i.isdigit())).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medians\n",
    "pd.DataFrame.from_dict(medians,orient='index').groupby(lambda x: ''.join(i for i in x if not i.isdigit())).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checks\n",
    "Sometimes out-of-sample performance of a baseline may still be better than that of our solution.\n",
    "However, in-sample performance of our own solutions should always be better than any baseline.\n",
    "If that is not the case, this would indicate our search does not explore the space well enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_sample_comparisons = pd.DataFrame()\n",
    "for log, trace in traces.items():\n",
    "    in_sample_comparisons = in_sample_comparisons.append(trace.in_comparison.loc['either'].rename(log))\n",
    "in_sample_comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = traces['mlr_rf_mupluslambda_2']\n",
    "rs = traces['mlr_rf_random_search_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to random search \n",
    "The following provides an overview over scores for different iterations of random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsdf = pd.read_csv(\"data/mlr_\"+alg+\"_baselines.csv\", index_col=0)\n",
    "rsdf.iloc[:,1:].apply(np.mean,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "End of notebook - just sketchpad below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trsdf = rsdf.transpose()\n",
    "trsdf.index.name = \"task\"\n",
    "trsdf.index = pd.Index([int(float(x)) for x in trsdf.index.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df = ml.scores.loc[ml.scores.index.map(lambda idx: idx[1] == \"out-sample\")]\n",
    "df.index = pd.Index(df.index.map(lambda idx: idx[0]))\n",
    "df = df.join(trsdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:,df.median().sort_values().index]\n",
    "p = sns.boxplot(data=df.melt(), y = \"value\", x = \"variable\")\n",
    "p.set_xticklabels(p.get_xticklabels(), rotation=45)\n",
    "plt.show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ END OF RELEVANT PARTS #################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = \"rf\"\n",
    "run_one = f\"mlr_{alg}_mupluslambda_0\"\n",
    "minimum = dict(knn=100, svm=100, glmnet=100, rpart=60, rf=25)\n",
    "final_scores = pd.DataFrame()\n",
    "for log, trace in traces.items():\n",
    "    # Filter out runs with >100 tasks completed:\n",
    "    if len(trace.scores) / 2 > minimum[alg]:\n",
    "        out_sampfirst_n_tasksle = trace.scores.index.map(lambda idx: idx[1] == \"out-sample\")\n",
    "        log_oos = trace.scores.loc[out_sample].final.rename(log)\n",
    "        final_scores = final_scores.append(log_oos)\n",
    "        if log == run_one:\n",
    "            # contains benchmark scores\n",
    "            for b in trace.baseline:\n",
    "                baseline_score = trace.scores.loc[out_sample][b].rename(b)\n",
    "                final_scores = final_scores.append(baseline_score)\n",
    "final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out incomplete tasks:\n",
    "final = final_scores.loc[:, ~final_scores.isna().any()]\n",
    "df = final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_one=\"mlr_svm_lisa\"\n",
    "run_two=\"svm_warm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[[run_one, run_two]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alone = {k: 0 for k in df.index.values}\n",
    "shared = {k: 0 for k in df.index.values}\n",
    "\n",
    "for _, out in df.T.iterrows():\n",
    "    best = out[out == out.max()].index.values\n",
    "    if len(best) == 1:\n",
    "        alone[best[0]] += 1\n",
    "    else:\n",
    "        for winner in best:\n",
    "            shared[winner] += 1\n",
    "\n",
    "alone = {k: alone[k] for k in sorted(alone)}\n",
    "shared = {k: shared[k] for k in sorted(shared)}\n",
    "either = {k: shared[k] + alone[k] for k in sorted({**alone, **shared})}\n",
    "comparison = pd.DataFrame([alone, shared, either], index=['alone', 'shared', 'either'])\n",
    "comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df.T.copy()\n",
    "df_out['max'] = df_out.max(axis=1)\n",
    "for col in df_out:\n",
    "    df_out['d_' + col] = df_out['max'] - df_out[col]\n",
    "d_cols = [c for c in df_out.columns if c.startswith('d_') and 'max' not in c]\n",
    "df_out[d_cols].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out[d_cols].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (df.loc[run_one] - df.loc[run_two]).hist(bins=[(f / 40 - 1) for f in range(81)])\n",
    "ax.set_title(f\"Symbolic - Constant | median: {(df.loc[run_one] - df.loc[run_two]).median():.3f}, mean: {(df.loc[run_one] - df.loc[run_two]).mean():.3f}, {df.shape[1]} tasks\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_xlabel(\"Difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum((df.loc[\"mlr_svm_lisa\"] - df.loc[\"svm_cst\"]) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.loc[\"mlr_svm_lisa\"] - df.loc[\"svm_cst\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsdf.columns = [(round(float(x)), 'out-sample')  for x in rsdf.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rsdf.append(final_scores)\n",
    "df.iloc[:,1:].apply(np.mean,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.transpose().melt(var_name=\"method\", value_name=\"performance\")\n",
    "ax = sns.boxplot(x='method', y='performance', data = pdf)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mean regret:\n",
    "The following table records the mean regret for a specific strategy compared to picking the best in hindsight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = pd.DataFrame([])\n",
    "for log, trace in traces.items():\n",
    "    m = trace.d_scores.mean().rename(log)\n",
    "    means = means.append(m)\n",
    "means[[c for c in medians.columns if 'd_' in c and c != 'd_max']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for problem in [\"svm\"]:\n",
    "    for search in [\"mupluslambda\", \"random_search\", \"True\"]:\n",
    "        problem_search_dir = os.path.join(\"runs\", f\"{problem}_{search}\")   \n",
    "        for rundir in os.listdir(problem_search_dir):\n",
    "            eval_file = os.path.join(problem_search_dir, rundir, \"evaluations.csv\")\n",
    "            eval_file_new = os.path.join(problem_search_dir, rundir, \"new_evaluations.csv\")\n",
    "            with open(eval_file) as old, open(eval_file_new, 'w') as new:\n",
    "                for line in old.readlines():\n",
    "                    content = line.split(',')\n",
    "                    cs, expr_parts = content[:7], content[7:]\n",
    "                    newsep = ';'.join(cs + [','.join(expr_parts)])\n",
    "                    new.write(newsep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
