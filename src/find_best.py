import argparse
import functools
import logging
import os
import time
import uuid
import pathlib

import numpy as np

from deap import tools
import pandas as pd

from evolution import setup_toolbox
from evolution.operations import mass_evaluate, mass_evaluate_2, n_primitives_in, insert_fixed, approx_eq
from evolution.algorithms import one_plus_lambda, eaMuPlusLambda, random_search

from deap import gp, creator
from operator import attrgetter

from problem import Problem
from utils import str2bool


def cli_parser():
    description = "Use Symbolic Regression to find symbolic hyperparameter defaults."
    parser = argparse.ArgumentParser(description=description)
    parser.add_argument('problem', type=str,
                        help="Problem to optimize. Must match one of 'name' fields in the configuration file.")
    parser.add_argument('-m',
                        help=("mu for the mu+lambda algorithm. "
                              "Specifies the number of individuals that can create offspring."),
                        dest='mu', type=int, default=20)
    parser.add_argument('-l',
                        help=("lambda for the mu+lambda algorithm. "
                              "Specifies the number of offspring created at each iteration."
                              "Also used to determine the size of starting population."),
                        dest='lambda_', type=int, default=100)
    parser.add_argument('-ngen',
                        help="Maximum number of generations (default=300)",
                        dest='ngen', type=int, default=300)
    parser.add_argument('-a',
                        help="Algorithm. {mupluslambda, onepluslambda, random_search}",
                        dest='algorithm', type=str, default='mupluslambda')
    parser.add_argument('-esn',
                        help="Early Stopping N. Stop optimization if there is no improvement in n generations.",
                        dest='early_stop_n', type=int, default=20)
    parser.add_argument('-o',
                        help="Output directory."
                             "Write log, evaluations and progress to files in this dir.",
                        dest='output', type=str, default=None)
    parser.add_argument('-mno',
                        help="Max Number of Operators",
                        dest='max_number_operators', type=int, default=None)
    parser.add_argument('-oc',
                        help=(
                            "Optimize Constants. Instead of evaluating an individual with specific constants"
                            "evaluate based it on 50 random instantiation of constants instead."),
                        dest='optimize_constants', type=bool, default=False)
    parser.add_argument('-t',
                        help="Perform search and evaluation for this task only.",
                        dest='task', type=int, default=None)
    parser.add_argument('-warm',
                        help=(
                            "Warm-start optimization by including the 'benchmark' solutions in the "
                            "initial population."),
                        dest='warm_start', type=str2bool, default=False)
    parser.add_argument('-cst',
                    help=("Search only constant formulas?"),
                    dest='constants_only', type=str2bool, default=False)
    parser.add_argument('-age',
                help=("Regularize age by killing of older population members every nth generation."
                      "Defaults to a 1e5 (every 1e5 generations)."),
                dest='age_regularization', type=float, default=1e5)
    parser.add_argument('-cxpb',
                help=("Probability a new candidate is generated by crossover,"
                      "otherwise it is generated through mutation."),
                dest='cxpb', type=float, default=0.25)
    parser.add_argument('-mss',
                help=("Set the Max Start Size: the maximum depth per subtree for each"
                      "hyperparameter. (default=3). Note: for random search, "
                      "all candidates are constrained this way."),
                dest='max_start_size', type=int, default=2)
    return parser.parse_args()


def configure_logging(output_file: str = None):
    """ Configure INFO logging to console and optionally DEBUG to an output file. """
    logging.basicConfig()
    logging.getLogger().setLevel(logging.INFO)

    if output_file is not None:
        log_file_handle = logging.FileHandler(output_file)
        log_file_handle.setLevel(logging.DEBUG)
        logging.getLogger().addHandler(log_file_handle)


def main():
    run_id = str(uuid.uuid4())
    # Numpy must raise all warnings, otherwise overflows may go undetected.
    np.seterr(all='raise')
    args = cli_parser()
    if args.output:
        if not os.path.exists(args.output):
            pathlib.Path(args.output).mkdir(parents=True, exist_ok=True)
        run_dir = os.path.join(args.output, run_id)
        os.mkdir(run_dir)
        log_file = os.path.join(run_dir, 'output.log')
        configure_logging(log_file)
        with open(os.path.join(run_dir, "evaluations.csv"), 'a') as fh:
            fh.write(f"run;task;gen;inout;score;length;endresult;expression\n")
        with open(os.path.join(run_dir, "progress.csv"), 'a') as fh:
            fh.write(f"run;task;generation;score_min;score_avg;score_max\n")
    else:
        configure_logging()

    time_start = time.time()

    problem = Problem(args.problem)

    logging.info(f"Starting problem: {args.problem}")
    for parameter, value in args._get_kwargs():
        logging.info(f"param:{parameter}:{value}")
    logging.info(f"runid:{run_id}")

    logging.info(f"Benchmark problems: {args.problem}")
    if not args.constants_only:
        for check_name, check_individual in problem.benchmarks.items():
            logging.info(f"{check_name} := {check_individual}")

    if (args.optimize_constants):
        mass_eval_fun = mass_evaluate_2
    else:
        mass_eval_fun = mass_evaluate

    # The 'toolbox' defines all operations, and the primitive set defines the grammar.
    toolbox, pset = setup_toolbox(problem, args)
    # return toolbox, pset

    tasks = list(problem.metadata.index)
    if args.task is not None:
        if args.task not in tasks:
            logging.error("Requested task not in metadata.")
            quit(-1)
        else:
            tasks = [args.task]

    # ================================================
    # Start evolutionary optimization
    # ================================================
    in_sample_mean = {}
    for task in tasks:
        logging.info(f"START_TASK: {task}")
        # 'task' experiment data is used as validation set, so we must not use
        # it during our symbolic regression search.
        loo_metadataset = problem.metadata[problem.metadata.index != task]

        # 'map' will be called within the optimization algorithm for batch evaluation.
        # All evaluation variables are fixed, except for the individuals themselves.
        toolbox.register(
            "map",
            functools.partial(
                mass_eval_fun, pset=pset, metadataset=loo_metadataset,
                surrogates=problem.surrogates,
                toolbox=toolbox, optimize_constants=args.optimize_constants,
                problem=problem
            )
        )

        # Seed population with configurations from problem.benchmark // fully "Symc" config
        pop = []
        if args.optimize_constants:
            pop = [*pop, *toolbox.population_symc(problem)]
        if args.warm_start:
            pop = [*pop, *toolbox.population_benchmark(problem)]
        pop = [*pop, *toolbox.population(n=400_000)]

        print(len(pop))
        logging.info("Evaluating in sample:")
        scale_result_x_length = list(toolbox.map(toolbox.evaluate, pop))
        r = max(zip(scale_result_x_length, pop))
        print(r[0], str(r[1]))

        # logging.info("Evaluating out-of-sample:")
        # fn_ = gp.compile(ind, pset)
        # mf_values = problem.metadata.loc[task]
        # hp_values = insert_fixed(toolbox.evaluate(fn_, mf_values), problem)
        # score = problem.surrogates[task].predict(np.asarray(hp_values).reshape(1, -1))
        # logging.info(f"[GEN_{i}|{ind}|{score[0]:.4f}]")
        # if args.output:
        #     with open(os.path.join(run_dir, "evaluations.csv"), 'a') as fh:
        #         fh.write(f"{run_id};{task};{i};out;{score[0]:.4f};{n_primitives_in(ind)};{stop};{ind}\n")

    time_end = time.time()
    logging.info("Finished problem {} in {} seconds!".format(args.problem, round(time_end - time_start)))


if __name__ == '__main__':
    main()
