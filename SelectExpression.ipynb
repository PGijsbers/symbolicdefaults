{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook determine a symbolic default for each (problem, task) combination from random search results by concatenating the pareto fronts and picking the one with median in-sample performance estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Expected file structure.\n",
    "The working directory of the notebook server should contain:\n",
    " - `figures/`\n",
    " - `/run/results2/` in which all the `{problem}_{search}` folders are contained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Global settings\n",
    "sns.set_style('ticks')\n",
    "memory = joblib.Memory(\"data/r\", verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runlog:\n",
    "    \"\"\"\n",
    "    Read logs for a given problem across several searches\n",
    "    \"\"\"\n",
    "    def __init__(self, problem: str, searches: List, logdir: str):\n",
    "        self.problem = problem\n",
    "        self.searches = searches\n",
    "        self.logdir = logdir\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        df = pd.DataFrame([])\n",
    "        for search in self.searches:\n",
    "            df = df.append(read_run_logs(problem=self.problem, search=search, target=\"evaluations\", dir=self.logdir))\n",
    "        return df\n",
    "    \n",
    "    @property\n",
    "    def trace_data(self):\n",
    "        df = pd.DataFrame([])\n",
    "        for search in self.searches:\n",
    "            df = df.append(read_trace_logs(problem=self.problem, search=search, dir=self.logdir))\n",
    "        return df\n",
    "    \n",
    "    def pick_final_expression(self, method = \"best\", **kwargs):\n",
    "        \"\"\"\n",
    "        Pick final expression on \"in\" data\n",
    "        :method: either \"relative\", \"shortest\" or \"best\"\n",
    "        \"\"\"\n",
    "        df = self.data.copy()\n",
    "        df = df[df['expression'].str.contains(',')]\n",
    "        df = get_final_paretofront(df)\n",
    "        df = df.pivot_table(index=[\"run\", \"task\", \"gen\", \"length\", \"problem\", \"search\", \"expression\"], columns=\"inout\", values=\"score\")\n",
    "        df = df.reset_index()\n",
    "        if method == \"shortest\":\n",
    "            out = df[df.groupby(['run', 'search'])['length'].transform(min) == df['length']]\n",
    "        elif method == \"relative\":\n",
    "            out = df.loc[[pick_relative(group, **kwargs) for name, group in df.groupby(['run', 'search'])]]\n",
    "        elif method == \"cheat\":\n",
    "            out = df[df.groupby(['run', 'search'])['out'].transform(max) == df['out']]\n",
    "        elif method == \"scalarize\":\n",
    "            out = df.loc[[pick_scalarize(group, **kwargs) for name, group in df.groupby(['run', 'search'])]]\n",
    "        elif method == \"shortest_top_n\":\n",
    "            out = df.loc[[pick_shortest_top_n(group, **kwargs) for name, group in df.groupby(['run', 'search'])]]\n",
    "        else:\n",
    "            out = df[df.groupby(['run', 'search'])['in'].transform(max) == df['in']]\n",
    "        return out\n",
    "    \n",
    "    def get_benchmark_performances(self):\n",
    "        \"\"\"\n",
    "        Load benchmark performances\n",
    "        \"\"\"\n",
    "        df = self.data[~self.data['expression'].str.contains(',')]\n",
    "        df = df.pivot_table(index=[\"run\", \"task\",\"problem\", \"search\", \"expression\"], columns=\"inout\", values=\"score\")\n",
    "        df = df.reset_index()\n",
    "        df = df[[\"task\", \"problem\", \"expression\", \"in\", \"out\"]]\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        return(df)\n",
    "\n",
    "### Pick Strategies\n",
    "\n",
    "def pick_relative(x, eps=0.01, max_steps=1):\n",
    "    \"\"\"\n",
    "    Pick by relative improvement; \n",
    "    Consider only at most `max_steps` longer, if better by 'eps', break if not\n",
    "    \"\"\"\n",
    "    x = x.copy()\n",
    "    if len(x) == 1:\n",
    "        return(x.index.values[0])\n",
    "    \n",
    "    use_ix, length, score = None, 0, 0\n",
    "    for ix, rw in x.iterrows():\n",
    "        if rw['length'] - length > max_steps:\n",
    "            break  # candidates are too big\n",
    "        if rw['in'] - score < eps:\n",
    "            continue  # not enough increase\n",
    "        use_ix, length, score = ix, rw['length'], rw['in']        \n",
    "    return use_ix\n",
    "\n",
    "def pick_scalarize(x, b: float):\n",
    "    \"\"\"\n",
    "    Pick by scalarizing length and fitness; \n",
    "    Consider only at most `max_steps` longer, if better by 'eps', break if not\n",
    "    \"\"\"\n",
    "    x = x.copy()\n",
    "    x[\"scalar_score\"] = x[\"in\"] - b * (x[\"length\"] - 1)\n",
    "    return x[\"scalar_score\"].idxmax()\n",
    "    \n",
    "def pick_shortest_top_n(x, n: float):\n",
    "    \"\"\"\n",
    "    Pick by scalarizing length and fitness; \n",
    "    Consider only at most `max_steps` longer, if better by 'eps', break if not\n",
    "    \"\"\"\n",
    "    df = x.sort_values(\"in\").idx[:2,]\n",
    "    return df[\"length\"].idxmin()\n",
    "\n",
    "\n",
    "@memory.cache()\n",
    "def read_run_logs(problem:str, search: str, target: str, dir: str):\n",
    "    \"\"\"\n",
    "    Read all log-files for a given problem x search combination\n",
    "    \"\"\"\n",
    "    log_dir = f\"{dir}/{problem}_{search}/\"\n",
    "    if os.path.isdir(log_dir):\n",
    "        dirs = [os.path.join(log_dir, f) for f in os.listdir(log_dir)]\n",
    "        df = pd.DataFrame([])\n",
    "        for dir in dirs:\n",
    "            file = f\"{dir}/{target}.csv\"\n",
    "            if os.path.isfile(file):\n",
    "                tmpdf = pd.read_csv(file, sep=\";\")\n",
    "                tmpdf['search'] = search\n",
    "                # Pivot random search, rename constants only\n",
    "                if search == \"random_search\":\n",
    "                    tmpdf['search'] = tmpdf.apply(lambda x: x['search']+'_'+str((x['gen'] + 1)*100), axis=1)  \n",
    "                    tmpdf['endresult'] = True\n",
    "                elif search == \"True\":\n",
    "                    tmpdf['search'] = \"constants_only\"\n",
    "                df = df.append(tmpdf)\n",
    "        # Rename rf and add problem columns\n",
    "        if problem == \"rf\": \n",
    "            df['problem'] = \"random forest\"\n",
    "        else:\n",
    "            if problem == \"glmnet\": \n",
    "                df = df[df[\"expression\"] != \"sklearn_default\"] # ElasticNet sklearn/glmnet implementations don't match\n",
    "            df['problem'] = problem\n",
    "        return df\n",
    "    else:\n",
    "        print(log_dir,'is not path')\n",
    "        \n",
    "\n",
    "\n",
    "def get_final_paretofront(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Filter pareto front\n",
    "    \"\"\"\n",
    "    return df[df.endresult]\n",
    "    \n",
    "def read_trace_logs(problem:str, search: str, dir: str):\n",
    "    \"\"\"\n",
    "    Read \"progress\" log-files for a given problem x search combination (Optimization Traces)\n",
    "    \"\"\"\n",
    "    log_dir = f\"{dir}/{problem}_{search}/\"\n",
    "    if os.path.isdir(log_dir):\n",
    "        dirs = [os.path.join(log_dir, f) for f in os.listdir(log_dir)]\n",
    "        df = pd.DataFrame([])\n",
    "        for dir in dirs:\n",
    "            file = f\"{dir}/progress.csv\"\n",
    "            if os.path.isfile(file):\n",
    "                tmpdf = pd.read_csv(file, sep=\";\")\n",
    "                tmpdf['search'] = search\n",
    "                if search == \"True\":\n",
    "                    tmpdf['search'] = \"constants_only\"\n",
    "                elif search == \"random_search\":\n",
    "                    tmpdf['generation'] = (tmpdf['generation'] + 1)*100\n",
    "                df = df.append(tmpdf)\n",
    "\n",
    "        # Rename and add problem\n",
    "        if problem == \"rf\": \n",
    "            df['problem'] = \"random forest\"\n",
    "        else: \n",
    "            df['problem'] = problem\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Collection:\n",
    "**log** is a dict of all logs in the **logdir**(here `/runs/results2`) folder. \n",
    "\n",
    "Most of the time we are only interested in a single individual from the pareto-front per log.\n",
    "We can obtain this via `pick_final_expression(<strategy>)` from each log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect logs for all problems\n",
    "problems = ['svm', 'glmnet', 'rf', 'rpart', 'knn', 'xgboost']\n",
    "search_strategies = [\"random_search\"]\n",
    "log = {}\n",
    "for problem in problems:\n",
    "    log[problem] = Runlog(problem, search_strategies, logdir=\"runs/results2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At Random of Best:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for problem in problems:\n",
    "    f = log[problem].data\n",
    "    f = f[f.search == \"random_search_300\"]\n",
    "    f = f[f.inout == \"in\"]\n",
    "    f = f[f[\"expression\"].str.contains(',')]\n",
    "    f = f[f.groupby(\"run\").score.transform(max) == f.score]\n",
    "\n",
    "    # sample is for a shuffle\n",
    "    r = f.sample(frac=1).groupby('task').head(1)\n",
    "\n",
    "    r[[\"task\", \"expression\"]].to_csv(f\"{problem}_defaults.csv\", sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best of Best "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no data\n"
     ]
    }
   ],
   "source": [
    "random_search_3000 = pd.DataFrame()\n",
    "\n",
    "for problem in problems:\n",
    "    if problem == \"xgboost\":\n",
    "        print(\"no data yet\")\n",
    "        continue\n",
    "    # Filter on only random_search_300 expressions first\n",
    "    s = log[problem].data\n",
    "    s = s[s.search == \"random_search_300\"]\n",
    "    s = s[s[\"expression\"].str.contains(',')] \n",
    "    \n",
    "    # We want to select the best based on in-sample evaluations\n",
    "    f = s[s.inout == \"in\"].copy()\n",
    "    # Best by task (i.e. across runs)\n",
    "    f = f[f.groupby(\"task\").score.transform(max) == f.score]\n",
    "\n",
    "    # Multiple runs may have solutions with equivalent performance (expressions may differ)\n",
    "    # Shuffle the data, then select one at random:\n",
    "    r = f.sample(frac=1).groupby('task').head(1)\n",
    "    \n",
    "    assert len(r) == f.task.nunique(), \"You need as many expressions as tasks\"\n",
    "    \n",
    "    # Save the list of expressions to evaluate on real data\n",
    "    r[[\"task\", \"expression\"]].to_csv(f\"{problem}_defaults.csv\", sep=';', index=False)\n",
    "    \n",
    "    # Append to a file with both the in-sample and out-of-sample scores for the picked expression\n",
    "    # We do this to compare the results to other searches.\n",
    "    newdf = pd.DataFrame([])\n",
    "    for task, expression in r[[\"task\", \"expression\"]].values:\n",
    "        for idx, row in s.iterrows():\n",
    "            # We don't verify it's from the same run - but that shouldn't matter\n",
    "            # The performances should be the same so long as its calculated with the same task (held out)\n",
    "            if row.task == task and row.expression == expression:\n",
    "                newdf = newdf.append(row)\n",
    "    # Again because multiple runs on the same task may lead to the same expression,\n",
    "    # we filter out duplicates. This time no need to shuffle as they are duplicates.\n",
    "    best_per_task = newdf.groupby([\"task\", \"expression\", \"inout\"]).head(1)\n",
    "    random_search_3000 = random_search_3000.append(best_per_task)\n",
    "    \n",
    "random_search_3000.replace(\"random_search_300\", \"random_search_3000\", inplace=True)\n",
    "random_search_3000.to_csv(f\"results/random_search_30k.csv\", index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 make_tuple(min(n, 18), truediv(1, po))\n",
      "14965 make_tuple(if_gt(if_gt(p, rc, m, m), expit(n), max(50, p), min(mkd, xvar)), truediv(add(xvar, rc), if_gt(m, m, mkd, po)))\n",
      "3481 make_tuple(if_gt(add(p, p), add(po, xvar), min(po, m), max(248, rc)), truediv(pow(p, 0.11854840792412273), max(po, po)))\n",
      "3945 make_tuple(if_gt(rc, 0.4075700691169183, n, po), if_gt(p, mkd, mkd, 0.0160577656570737))\n",
      "167119 make_tuple(mul(n, 0.0021945151062239816), truediv(mkd, xvar))\n",
      "168909 make_tuple(if_gt(mkd, mcp, po, p), truediv(xvar, po))\n",
      "3561 make_tuple(max(truediv(p, xvar), expit(mkd)), truediv(expit(182), max(xvar, po)))\n",
      "9985 make_tuple(max(p, mkd), truediv(xvar, po))\n",
      "168868 make_tuple(add(p, xvar), truediv(xvar, po))\n",
      "189927 make_tuple(sub(m, rc), truediv(mkd, xvar))\n",
      "9970 make_tuple(if_gt(5, xvar, p, mkd), truediv(xvar, po))\n",
      "167125 make_tuple(min(n, p), truediv(0.7008622223175714, p))\n",
      "3512 make_tuple(add(183, p), truediv(0.2479426895779738, po))\n",
      "9956 make_tuple(add(mkd, p), truediv(xvar, po))\n",
      "9952 make_tuple(truediv(0.4142806569786889, mkd), truediv(mkd, xvar))\n",
      "3485 make_tuple(add(mcp, p), truediv(mcp, po))\n",
      "14954 make_tuple(min(749, p), truediv(1, po))\n",
      "3493 make_tuple(add(mcp, p), truediv(xvar, po))\n",
      "3893 make_tuple(add(xvar, 8), truediv(mkd, xvar))\n",
      "2073 make_tuple(if_gt(rc, po, p, po), truediv(0.8648305777085384, po))\n",
      "167211 make_tuple(if_gt(p, mcp, p, mcp), truediv(xvar, po))\n",
      "7592 make_tuple(add(max(mkd, po), pow(xvar, po)), truediv(truediv(0.4810653419409069, p), mul(1, xvar)))\n",
      "168908 make_tuple(truediv(po, xvar), truediv(mcp, po))\n",
      "6 make_tuple(max(mcp, p), truediv(xvar, po))\n",
      "9986 make_tuple(add(p, mcp), truediv(xvar, po))\n",
      "10090 make_tuple(max(m, p), truediv(xvar, po))\n",
      "9976 make_tuple(if_gt(rc, po, rc, p), truediv(mkd, xvar))\n",
      "189928 make_tuple(add(po, 0.048796933660157096), truediv(mkd, xvar))\n",
      "146195 make_tuple(add(po, 4), truediv(mkd, xvar))\n",
      "23 make_tuple(add(p, mcp), truediv(mcp, po))\n",
      "9950 make_tuple(truediv(m, xvar), truediv(mkd, xvar))\n",
      "22 make_tuple(add(m, 38), truediv(mkd, xvar))\n",
      "3560 make_tuple(truediv(m, xvar), truediv(xvar, po))\n",
      "9978 make_tuple(sub(m, xvar), truediv(mkd, xvar))\n",
      "49 make_tuple(add(p, p), truediv(mcp, po))\n",
      "3913 make_tuple(max(p, po), truediv(xvar, po))\n",
      "9964 make_tuple(max(32, p), truediv(mcp, po))\n",
      "167140 make_tuple(add(p, rc), truediv(xvar, po))\n",
      "14970 make_tuple(if_gt(0.0023861234090157453, rc, m, n), min(p, mkd))\n",
      "9977 make_tuple(truediv(po, xvar), truediv(mcp, po))\n",
      "9960 make_tuple(if_gt(mkd, m, mkd, po), truediv(mkd, xvar))\n",
      "146607 make_tuple(if_gt(p, p, po, po), truediv(mkd, xvar))\n",
      "41 make_tuple(add(po, mcp), truediv(mkd, xvar))\n",
      "168911 make_tuple(if_gt(xvar, rc, m, p), truediv(mkd, xvar))\n",
      "10093 make_tuple(max(po, p), truediv(mkd, xvar))\n",
      "32 make_tuple(add(p, mcp), truediv(xvar, po))\n",
      "2079 make_tuple(add(m, m), truediv(mcp, po))\n",
      "168912 make_tuple(add(p, rc), truediv(mcp, po))\n",
      "2074 make_tuple(min(po, po), truediv(1, po))\n",
      "37 make_tuple(max(mkd, p), truediv(xvar, po))\n",
      "14971 make_tuple(add(m, p), truediv(mkd, xvar))\n",
      "11 make_tuple(max(mkd, p), truediv(xvar, po))\n",
      "12 make_tuple(add(p, xvar), truediv(xvar, po))\n",
      "146820 make_tuple(add(rc, 13), truediv(xvar, po))\n",
      "14969 make_tuple(max(p, rc), truediv(mcp, po))\n",
      "9981 make_tuple(max(rc, po), truediv(mkd, xvar))\n",
      "3022 make_tuple(min(n, p), truediv(xvar, po))\n",
      "219 make_tuple(if_gt(mkd, p, 5, p), truediv(xvar, po))\n",
      "146819 make_tuple(max(min(xvar, mcp), add(rc, p)), truediv(expit(mcp), add(po, m)))\n",
      "125921 make_tuple(add(m, p), truediv(xvar, po))\n",
      "145681 make_tuple(min(122, po), truediv(mkd, xvar))\n",
      "146822 make_tuple(if_gt(n, 1, p, n), truediv(xvar, po))\n",
      "3021 make_tuple(add(xvar, 6), truediv(mkd, xvar))\n",
      "10106 make_tuple(pow(m, xvar), truediv(0.9525167259607216, po))\n",
      "9957 make_tuple(max(mcp, po), truediv(mkd, xvar))\n",
      "9946 make_tuple(min(n, p), truediv(xvar, po))\n",
      "168336 make_tuple(if_gt(m, p, xvar, m), truediv(mkd, xvar))\n",
      "16 make_tuple(sub(max(xvar, mcp), neg(80)), truediv(expit(mcp), add(3, po)))\n",
      "14952 make_tuple(pow(p, xvar), truediv(mkd, xvar))\n",
      "3918 make_tuple(neg(neg(p)), if_gt(sub(n, m), sub(0.055741297533198134, xvar), truediv(0.5396072514812273, po), if_gt(n, rc, p, mkd)))\n",
      "3510 make_tuple(sub(p, mkd), truediv(xvar, po))\n",
      "125920 make_tuple(max(p, p), truediv(xvar, po))\n",
      "3 make_tuple(max(add(p, mkd), expit(mkd)), truediv(pow(mkd, mkd), sub(p, mkd)))\n",
      "3903 make_tuple(if_gt(0.002409077505455971, p, p, po), truediv(mkd, xvar))\n",
      "9910 make_tuple(max(po, m), truediv(mkd, xvar))\n",
      "168910 make_tuple(if_gt(23, rc, p, mcp), truediv(xvar, po))\n",
      "3902 make_tuple(if_gt(mcp, 0.0010275806762833452, p, mcp), truediv(xvar, po))\n",
      "190411 make_tuple(add(m, po), truediv(mkd, xvar))\n",
      "28 make_tuple(add(p, xvar), truediv(mcp, po))\n",
      "24 make_tuple(min(po, po), truediv(mkd, xvar))\n",
      "189924 make_tuple(truediv(sub(p, mcp), truediv(po, po)), truediv(expit(rc), max(po, mcp)))\n",
      "18 make_tuple(min(n, p), truediv(xvar, po))\n",
      "3904 make_tuple(sub(po, 0.022526112079507925), truediv(mkd, xvar))\n",
      "3549 make_tuple(truediv(max(mcp, p), pow(xvar, rc)), min(if_gt(rc, mkd, xvar, n), truediv(xvar, po)))\n",
      "10101 make_tuple(neg(sub(xvar, p)), truediv(min(n, xvar), mul(p, xvar)))\n",
      "146212 make_tuple(mul(276, 0.1894345306285911), truediv(1, po))\n",
      "146818 make_tuple(mul(p, xvar), truediv(1, po))\n",
      "146817 make_tuple(add(mkd, p), truediv(0.7096847846572717, p))\n",
      "15 make_tuple(add(if_gt(mkd, mkd, xvar, 0.0026015886485645555), pow(po, 1)), truediv(expit(mcp), if_gt(p, rc, po, p)))\n",
      "43 make_tuple(if_gt(p, xvar, p, m), truediv(xvar, po))\n",
      "146821 make_tuple(add(po, 26), truediv(mkd, xvar))\n",
      "45 make_tuple(sub(max(p, m), truediv(rc, mcp)), truediv(if_gt(n, xvar, mcp, mcp), sub(po, mcp)))\n",
      "167141 make_tuple(min(n, p), truediv(xvar, po))\n",
      "190412 make_tuple(mul(po, 0.6216322308547769), truediv(mkd, xvar))\n",
      "146824 make_tuple(add(0.0011224745765569721, m), truediv(0.6013719043011699, po))\n",
      "125922 make_tuple(max(add(p, mcp), min(361, 6)), max(truediv(mkd, xvar), neg(rc)))\n",
      "58 make_tuple(max(p, 0.4169555358803517), truediv(mcp, po))\n",
      "53 make_tuple(add(mkd, 5), truediv(xvar, po))\n",
      "34539 make_tuple(sub(p, 0.36622303467594797), truediv(1, p))\n",
      "3907 make_tuple(if_gt(n, m, m, mcp), truediv(0.5802991241240293, po))\n",
      "3917 make_tuple(if_gt(mcp, po, mcp, 22), truediv(mkd, xvar))\n",
      "9971 make_tuple(pow(mul(xvar, p), min(xvar, xvar)), truediv(expit(po), sub(po, mcp)))\n",
      "29 make_tuple(add(m, m), truediv(mcp, po))\n",
      "14 make_tuple(truediv(add(46, mcp), max(rc, xvar)), min(truediv(mkd, xvar), pow(po, mkd)))\n",
      "146800 make_tuple(if_gt(n, 6, 13, po), truediv(mkd, xvar))\n",
      "168329 make_tuple(add(xvar, p), truediv(mcp, po))\n"
     ]
    }
   ],
   "source": [
    "# not sure how to do this elegantly due to median tie-breakers, so opted to break it down completely:\n",
    "f = log[\"svm\"].data\n",
    "f = f[f.search == \"random_search_300\"]\n",
    "f = f[f.inout == \"in\"]\n",
    "f = f[f[\"expression\"].str.contains(',')]\n",
    "for task in f.task.unique():\n",
    "    t = f[f.task == task]\n",
    "    m = t[t.score == t.score.median()]\n",
    "    \n",
    "    if len(m) == 0:\n",
    "        # Must be an even number of elements in consideration.\n",
    "        m = t.sort_values(by=\"score\").iloc[int(len(t)/2)-1:int(len(t)/2)+1]\n",
    "    \n",
    "    # select one from m\n",
    "    print(task, m.sample(1).expression.values[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
